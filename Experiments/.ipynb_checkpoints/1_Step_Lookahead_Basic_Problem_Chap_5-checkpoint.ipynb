{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2z471Fnwo6e",
    "outputId": "629d131e-99f9-48be-c4ef-2fa94484651b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Empirical Experiment for k-step lookahead Policy over Multiple Seeds ---\n",
      "Number of seeds: 100\n",
      "Number of evaluation episodes per seed: 100\n",
      "Lookahead depth (k): 1\n",
      "Rollout simulations: 100\n",
      "--------------------\n",
      "\n",
      "--- Running with Seed: 0 ---\n",
      "--- Seed 0 Results ---\n",
      "Success Rate: 0.6400 (64.00%)\n",
      "Average Detection Time (Successful Episodes): 5.23\n",
      "Average Episode Reward: -0.5240\n",
      "Average Episode Length: 6.95\n",
      "Time taken for this seed: 153.70 seconds\n",
      "\n",
      "--- Running with Seed: 1 ---\n",
      "--- Seed 1 Results ---\n",
      "Success Rate: 0.6300 (63.00%)\n",
      "Average Detection Time (Successful Episodes): 5.05\n",
      "Average Episode Reward: -0.5240\n",
      "Average Episode Length: 6.88\n",
      "Time taken for this seed: 150.78 seconds\n",
      "\n",
      "--- Running with Seed: 2 ---\n",
      "--- Seed 2 Results ---\n",
      "Success Rate: 0.6100 (61.00%)\n",
      "Average Detection Time (Successful Episodes): 5.39\n",
      "Average Episode Reward: -0.5690\n",
      "Average Episode Length: 7.19\n",
      "Time taken for this seed: 175.72 seconds\n",
      "\n",
      "--- Running with Seed: 3 ---\n",
      "--- Seed 3 Results ---\n",
      "Success Rate: 0.6000 (60.00%)\n",
      "Average Detection Time (Successful Episodes): 5.22\n",
      "Average Episode Reward: -0.5925\n",
      "Average Episode Length: 7.13\n",
      "Time taken for this seed: 157.30 seconds\n",
      "\n",
      "--- Running with Seed: 4 ---\n",
      "--- Seed 4 Results ---\n",
      "Success Rate: 0.6700 (67.00%)\n",
      "Average Detection Time (Successful Episodes): 5.31\n",
      "Average Episode Reward: -0.4380\n",
      "Average Episode Length: 6.86\n",
      "Time taken for this seed: 165.67 seconds\n",
      "\n",
      "--- Running with Seed: 5 ---\n",
      "--- Seed 5 Results ---\n",
      "Success Rate: 0.5800 (58.00%)\n",
      "Average Detection Time (Successful Episodes): 4.50\n",
      "Average Episode Reward: -0.5160\n",
      "Average Episode Length: 6.81\n",
      "Time taken for this seed: 165.02 seconds\n",
      "\n",
      "--- Running with Seed: 6 ---\n",
      "--- Seed 6 Results ---\n",
      "Success Rate: 0.6800 (68.00%)\n",
      "Average Detection Time (Successful Episodes): 4.50\n",
      "Average Episode Reward: -0.4185\n",
      "Average Episode Length: 6.26\n",
      "Time taken for this seed: 138.50 seconds\n",
      "\n",
      "--- Running with Seed: 7 ---\n",
      "--- Seed 7 Results ---\n",
      "Success Rate: 0.5300 (53.00%)\n",
      "Average Detection Time (Successful Episodes): 6.23\n",
      "Average Episode Reward: -0.7520\n",
      "Average Episode Length: 8.00\n",
      "Time taken for this seed: 187.58 seconds\n",
      "\n",
      "--- Running with Seed: 8 ---\n",
      "--- Seed 8 Results ---\n",
      "Success Rate: 0.5400 (54.00%)\n",
      "Average Detection Time (Successful Episodes): 4.96\n",
      "Average Episode Reward: -0.5865\n",
      "Average Episode Length: 7.28\n",
      "Time taken for this seed: 164.39 seconds\n",
      "\n",
      "--- Running with Seed: 9 ---\n",
      "--- Seed 9 Results ---\n",
      "Success Rate: 0.6400 (64.00%)\n",
      "Average Detection Time (Successful Episodes): 4.48\n",
      "Average Episode Reward: -0.4250\n",
      "Average Episode Length: 6.47\n",
      "Time taken for this seed: 149.22 seconds\n",
      "\n",
      "--- Running with Seed: 10 ---\n",
      "--- Seed 10 Results ---\n",
      "Success Rate: 0.6000 (60.00%)\n",
      "Average Detection Time (Successful Episodes): 5.32\n",
      "Average Episode Reward: -0.5145\n",
      "Average Episode Length: 7.19\n",
      "Time taken for this seed: 179.46 seconds\n",
      "\n",
      "--- Running with Seed: 11 ---\n",
      "--- Seed 11 Results ---\n",
      "Success Rate: 0.5600 (56.00%)\n",
      "Average Detection Time (Successful Episodes): 5.86\n",
      "Average Episode Reward: -0.6710\n",
      "Average Episode Length: 7.68\n",
      "Time taken for this seed: 171.39 seconds\n",
      "\n",
      "--- Running with Seed: 12 ---\n",
      "--- Seed 12 Results ---\n",
      "Success Rate: 0.5900 (59.00%)\n",
      "Average Detection Time (Successful Episodes): 4.80\n",
      "Average Episode Reward: -0.5525\n",
      "Average Episode Length: 6.93\n",
      "Time taken for this seed: 147.26 seconds\n",
      "\n",
      "--- Running with Seed: 13 ---\n",
      "--- Seed 13 Results ---\n",
      "Success Rate: 0.7200 (72.00%)\n",
      "Average Detection Time (Successful Episodes): 5.67\n",
      "Average Episode Reward: -0.3795\n",
      "Average Episode Length: 6.88\n",
      "Time taken for this seed: 161.13 seconds\n",
      "\n",
      "--- Running with Seed: 14 ---\n",
      "--- Seed 14 Results ---\n",
      "Success Rate: 0.7100 (71.00%)\n",
      "Average Detection Time (Successful Episodes): 5.51\n",
      "Average Episode Reward: -0.4240\n",
      "Average Episode Length: 6.81\n",
      "Time taken for this seed: 155.83 seconds\n",
      "\n",
      "--- Running with Seed: 15 ---\n",
      "--- Seed 15 Results ---\n",
      "Success Rate: 0.6600 (66.00%)\n",
      "Average Detection Time (Successful Episodes): 6.35\n",
      "Average Episode Reward: -0.4750\n",
      "Average Episode Length: 7.59\n",
      "Time taken for this seed: 201.23 seconds\n",
      "\n",
      "--- Running with Seed: 16 ---\n",
      "--- Seed 16 Results ---\n",
      "Success Rate: 0.5000 (50.00%)\n",
      "Average Detection Time (Successful Episodes): 5.90\n",
      "Average Episode Reward: -0.7295\n",
      "Average Episode Length: 7.95\n",
      "Time taken for this seed: 182.83 seconds\n",
      "\n",
      "--- Running with Seed: 17 ---\n",
      "--- Seed 17 Results ---\n",
      "Success Rate: 0.6400 (64.00%)\n",
      "Average Detection Time (Successful Episodes): 5.12\n",
      "Average Episode Reward: -0.5425\n",
      "Average Episode Length: 6.88\n",
      "Time taken for this seed: 157.52 seconds\n",
      "\n",
      "--- Running with Seed: 18 ---\n",
      "--- Seed 18 Results ---\n",
      "Success Rate: 0.6000 (60.00%)\n",
      "Average Detection Time (Successful Episodes): 5.65\n",
      "Average Episode Reward: -0.5625\n",
      "Average Episode Length: 7.39\n",
      "Time taken for this seed: 164.04 seconds\n",
      "\n",
      "--- Running with Seed: 19 ---\n",
      "--- Seed 19 Results ---\n",
      "Success Rate: 0.5800 (58.00%)\n",
      "Average Detection Time (Successful Episodes): 6.28\n",
      "Average Episode Reward: -0.5550\n",
      "Average Episode Length: 7.84\n",
      "Time taken for this seed: 194.59 seconds\n",
      "\n",
      "--- Running with Seed: 20 ---\n",
      "--- Seed 20 Results ---\n",
      "Success Rate: 0.6400 (64.00%)\n",
      "Average Detection Time (Successful Episodes): 5.58\n",
      "Average Episode Reward: -0.5535\n",
      "Average Episode Length: 7.17\n",
      "Time taken for this seed: 171.81 seconds\n",
      "\n",
      "--- Running with Seed: 21 ---\n",
      "--- Seed 21 Results ---\n",
      "Success Rate: 0.6000 (60.00%)\n",
      "Average Detection Time (Successful Episodes): 5.57\n",
      "Average Episode Reward: -0.4930\n",
      "Average Episode Length: 7.34\n",
      "Time taken for this seed: 192.71 seconds\n",
      "\n",
      "--- Running with Seed: 22 ---\n",
      "--- Seed 22 Results ---\n",
      "Success Rate: 0.5200 (52.00%)\n",
      "Average Detection Time (Successful Episodes): 6.40\n",
      "Average Episode Reward: -0.7200\n",
      "Average Episode Length: 8.13\n",
      "Time taken for this seed: 209.00 seconds\n",
      "\n",
      "--- Running with Seed: 23 ---\n",
      "--- Seed 23 Results ---\n",
      "Success Rate: 0.5700 (57.00%)\n",
      "Average Detection Time (Successful Episodes): 4.75\n",
      "Average Episode Reward: -0.5570\n",
      "Average Episode Length: 7.01\n",
      "Time taken for this seed: 168.36 seconds\n",
      "\n",
      "--- Running with Seed: 24 ---\n",
      "--- Seed 24 Results ---\n",
      "Success Rate: 0.5700 (57.00%)\n",
      "Average Detection Time (Successful Episodes): 5.82\n",
      "Average Episode Reward: -0.5770\n",
      "Average Episode Length: 7.62\n",
      "Time taken for this seed: 168.31 seconds\n",
      "\n",
      "--- Running with Seed: 25 ---\n",
      "--- Seed 25 Results ---\n",
      "Success Rate: 0.5100 (51.00%)\n",
      "Average Detection Time (Successful Episodes): 4.80\n",
      "Average Episode Reward: -0.7525\n",
      "Average Episode Length: 7.35\n",
      "Time taken for this seed: 180.31 seconds\n",
      "\n",
      "--- Running with Seed: 26 ---\n",
      "--- Seed 26 Results ---\n",
      "Success Rate: 0.6400 (64.00%)\n",
      "Average Detection Time (Successful Episodes): 5.67\n",
      "Average Episode Reward: -0.4925\n",
      "Average Episode Length: 7.23\n",
      "Time taken for this seed: 176.04 seconds\n",
      "\n",
      "--- Running with Seed: 27 ---\n",
      "--- Seed 27 Results ---\n",
      "Success Rate: 0.6600 (66.00%)\n",
      "Average Detection Time (Successful Episodes): 5.76\n",
      "Average Episode Reward: -0.4170\n",
      "Average Episode Length: 7.20\n",
      "Time taken for this seed: 172.20 seconds\n",
      "\n",
      "--- Running with Seed: 28 ---\n",
      "--- Seed 28 Results ---\n",
      "Success Rate: 0.6600 (66.00%)\n",
      "Average Detection Time (Successful Episodes): 5.39\n",
      "Average Episode Reward: -0.5560\n",
      "Average Episode Length: 6.96\n",
      "Time taken for this seed: 145.38 seconds\n",
      "\n",
      "--- Running with Seed: 29 ---\n",
      "--- Seed 29 Results ---\n",
      "Success Rate: 0.6000 (60.00%)\n",
      "Average Detection Time (Successful Episodes): 6.18\n",
      "Average Episode Reward: -0.5940\n",
      "Average Episode Length: 7.71\n",
      "Time taken for this seed: 178.00 seconds\n",
      "\n",
      "--- Running with Seed: 30 ---\n",
      "--- Seed 30 Results ---\n",
      "Success Rate: 0.6300 (63.00%)\n",
      "Average Detection Time (Successful Episodes): 5.48\n",
      "Average Episode Reward: -0.6135\n",
      "Average Episode Length: 7.15\n",
      "Time taken for this seed: 158.14 seconds\n",
      "\n",
      "--- Running with Seed: 31 ---\n",
      "--- Seed 31 Results ---\n",
      "Success Rate: 0.6200 (62.00%)\n",
      "Average Detection Time (Successful Episodes): 5.56\n",
      "Average Episode Reward: -0.5295\n",
      "Average Episode Length: 7.25\n",
      "Time taken for this seed: 163.83 seconds\n",
      "\n",
      "--- Running with Seed: 32 ---\n",
      "--- Seed 32 Results ---\n",
      "Success Rate: 0.5500 (55.00%)\n",
      "Average Detection Time (Successful Episodes): 4.51\n",
      "Average Episode Reward: -0.5515\n",
      "Average Episode Length: 6.98\n",
      "Time taken for this seed: 153.96 seconds\n",
      "\n",
      "--- Running with Seed: 33 ---\n",
      "--- Seed 33 Results ---\n",
      "Success Rate: 0.6600 (66.00%)\n",
      "Average Detection Time (Successful Episodes): 6.08\n",
      "Average Episode Reward: -0.5570\n",
      "Average Episode Length: 7.41\n",
      "Time taken for this seed: 191.57 seconds\n",
      "\n",
      "--- Running with Seed: 34 ---\n",
      "--- Seed 34 Results ---\n",
      "Success Rate: 0.6300 (63.00%)\n",
      "Average Detection Time (Successful Episodes): 4.57\n",
      "Average Episode Reward: -0.4445\n",
      "Average Episode Length: 6.58\n",
      "Time taken for this seed: 156.55 seconds\n",
      "\n",
      "--- Running with Seed: 35 ---\n",
      "--- Seed 35 Results ---\n",
      "Success Rate: 0.6800 (68.00%)\n",
      "Average Detection Time (Successful Episodes): 5.79\n",
      "Average Episode Reward: -0.4360\n",
      "Average Episode Length: 7.14\n",
      "Time taken for this seed: 176.31 seconds\n",
      "\n",
      "--- Running with Seed: 36 ---\n",
      "--- Seed 36 Results ---\n",
      "Success Rate: 0.6100 (61.00%)\n",
      "Average Detection Time (Successful Episodes): 5.34\n",
      "Average Episode Reward: -0.5700\n",
      "Average Episode Length: 7.16\n",
      "Time taken for this seed: 160.42 seconds\n",
      "\n",
      "--- Running with Seed: 37 ---\n",
      "--- Seed 37 Results ---\n",
      "Success Rate: 0.5100 (51.00%)\n",
      "Average Detection Time (Successful Episodes): 5.63\n",
      "Average Episode Reward: -0.6085\n",
      "Average Episode Length: 7.77\n",
      "Time taken for this seed: 195.44 seconds\n",
      "\n",
      "--- Running with Seed: 38 ---\n",
      "--- Seed 38 Results ---\n",
      "Success Rate: 0.5600 (56.00%)\n",
      "Average Detection Time (Successful Episodes): 5.88\n",
      "Average Episode Reward: -0.5945\n",
      "Average Episode Length: 7.69\n",
      "Time taken for this seed: 184.09 seconds\n",
      "\n",
      "--- Running with Seed: 39 ---\n",
      "--- Seed 39 Results ---\n",
      "Success Rate: 0.6400 (64.00%)\n",
      "Average Detection Time (Successful Episodes): 4.94\n",
      "Average Episode Reward: -0.4775\n",
      "Average Episode Length: 6.76\n",
      "Time taken for this seed: 141.42 seconds\n",
      "\n",
      "--- Running with Seed: 40 ---\n",
      "--- Seed 40 Results ---\n",
      "Success Rate: 0.4200 (42.00%)\n",
      "Average Detection Time (Successful Episodes): 6.62\n",
      "Average Episode Reward: -0.8420\n",
      "Average Episode Length: 8.58\n",
      "Time taken for this seed: 203.46 seconds\n",
      "\n",
      "--- Running with Seed: 41 ---\n",
      "--- Seed 41 Results ---\n",
      "Success Rate: 0.6300 (63.00%)\n",
      "Average Detection Time (Successful Episodes): 5.00\n",
      "Average Episode Reward: -0.5570\n",
      "Average Episode Length: 6.85\n",
      "Time taken for this seed: 171.66 seconds\n",
      "\n",
      "--- Running with Seed: 42 ---\n",
      "--- Seed 42 Results ---\n",
      "Success Rate: 0.5200 (52.00%)\n",
      "Average Detection Time (Successful Episodes): 6.33\n",
      "Average Episode Reward: -0.7280\n",
      "Average Episode Length: 8.09\n",
      "Time taken for this seed: 192.17 seconds\n",
      "\n",
      "--- Running with Seed: 43 ---\n",
      "--- Seed 43 Results ---\n",
      "Success Rate: 0.5200 (52.00%)\n",
      "Average Detection Time (Successful Episodes): 5.33\n",
      "Average Episode Reward: -0.7245\n",
      "Average Episode Length: 7.57\n",
      "Time taken for this seed: 151.74 seconds\n",
      "\n",
      "--- Running with Seed: 44 ---\n",
      "--- Seed 44 Results ---\n",
      "Success Rate: 0.5200 (52.00%)\n",
      "Average Detection Time (Successful Episodes): 4.96\n",
      "Average Episode Reward: -0.6350\n",
      "Average Episode Length: 7.38\n",
      "Time taken for this seed: 171.80 seconds\n",
      "\n",
      "--- Running with Seed: 45 ---\n",
      "--- Seed 45 Results ---\n",
      "Success Rate: 0.6800 (68.00%)\n",
      "Average Detection Time (Successful Episodes): 4.65\n",
      "Average Episode Reward: -0.4170\n",
      "Average Episode Length: 6.36\n",
      "Time taken for this seed: 149.81 seconds\n",
      "\n",
      "--- Running with Seed: 46 ---\n",
      "--- Seed 46 Results ---\n",
      "Success Rate: 0.5700 (57.00%)\n",
      "Average Detection Time (Successful Episodes): 6.11\n",
      "Average Episode Reward: -0.6270\n",
      "Average Episode Length: 7.78\n",
      "Time taken for this seed: 182.00 seconds\n",
      "\n",
      "--- Running with Seed: 47 ---\n",
      "--- Seed 47 Results ---\n",
      "Success Rate: 0.6200 (62.00%)\n",
      "Average Detection Time (Successful Episodes): 5.03\n",
      "Average Episode Reward: -0.5250\n",
      "Average Episode Length: 6.92\n",
      "Time taken for this seed: 171.67 seconds\n",
      "\n",
      "--- Running with Seed: 48 ---\n",
      "--- Seed 48 Results ---\n",
      "Success Rate: 0.5700 (57.00%)\n",
      "Average Detection Time (Successful Episodes): 5.28\n",
      "Average Episode Reward: -0.5945\n",
      "Average Episode Length: 7.31\n",
      "Time taken for this seed: 156.37 seconds\n",
      "\n",
      "--- Running with Seed: 49 ---\n",
      "--- Seed 49 Results ---\n",
      "Success Rate: 0.5700 (57.00%)\n",
      "Average Detection Time (Successful Episodes): 5.09\n",
      "Average Episode Reward: -0.6365\n",
      "Average Episode Length: 7.20\n",
      "Time taken for this seed: 154.62 seconds\n",
      "\n",
      "--- Running with Seed: 50 ---\n",
      "--- Seed 50 Results ---\n",
      "Success Rate: 0.6100 (61.00%)\n",
      "Average Detection Time (Successful Episodes): 5.64\n",
      "Average Episode Reward: -0.6315\n",
      "Average Episode Length: 7.34\n",
      "Time taken for this seed: 194.13 seconds\n",
      "\n",
      "--- Running with Seed: 51 ---\n",
      "--- Seed 51 Results ---\n",
      "Success Rate: 0.5600 (56.00%)\n",
      "Average Detection Time (Successful Episodes): 5.54\n",
      "Average Episode Reward: -0.6085\n",
      "Average Episode Length: 7.50\n",
      "Time taken for this seed: 182.07 seconds\n",
      "\n",
      "--- Running with Seed: 52 ---\n",
      "--- Seed 52 Results ---\n",
      "Success Rate: 0.6300 (63.00%)\n",
      "Average Detection Time (Successful Episodes): 4.98\n",
      "Average Episode Reward: -0.5150\n",
      "Average Episode Length: 6.84\n",
      "Time taken for this seed: 172.15 seconds\n",
      "\n",
      "--- Running with Seed: 53 ---\n",
      "--- Seed 53 Results ---\n",
      "Success Rate: 0.5200 (52.00%)\n",
      "Average Detection Time (Successful Episodes): 6.25\n",
      "Average Episode Reward: -0.7050\n",
      "Average Episode Length: 8.05\n",
      "Time taken for this seed: 197.96 seconds\n",
      "\n",
      "--- Running with Seed: 54 ---\n",
      "--- Seed 54 Results ---\n",
      "Success Rate: 0.5100 (51.00%)\n",
      "Average Detection Time (Successful Episodes): 5.39\n",
      "Average Episode Reward: -0.7215\n",
      "Average Episode Length: 7.65\n",
      "Time taken for this seed: 185.61 seconds\n",
      "\n",
      "--- Running with Seed: 55 ---\n",
      "--- Seed 55 Results ---\n",
      "Success Rate: 0.5900 (59.00%)\n",
      "Average Detection Time (Successful Episodes): 5.69\n",
      "Average Episode Reward: -0.5355\n",
      "Average Episode Length: 7.46\n",
      "Time taken for this seed: 185.67 seconds\n",
      "\n",
      "--- Running with Seed: 56 ---\n",
      "--- Seed 56 Results ---\n",
      "Success Rate: 0.6400 (64.00%)\n",
      "Average Detection Time (Successful Episodes): 4.95\n",
      "Average Episode Reward: -0.3775\n",
      "Average Episode Length: 6.77\n",
      "Time taken for this seed: 166.87 seconds\n",
      "\n",
      "--- Running with Seed: 57 ---\n",
      "--- Seed 57 Results ---\n",
      "Success Rate: 0.5400 (54.00%)\n",
      "Average Detection Time (Successful Episodes): 5.98\n",
      "Average Episode Reward: -0.5825\n",
      "Average Episode Length: 7.83\n",
      "Time taken for this seed: 196.79 seconds\n",
      "\n",
      "--- Running with Seed: 58 ---\n",
      "--- Seed 58 Results ---\n",
      "Success Rate: 0.6900 (69.00%)\n",
      "Average Detection Time (Successful Episodes): 5.28\n",
      "Average Episode Reward: -0.4395\n",
      "Average Episode Length: 6.74\n",
      "Time taken for this seed: 172.22 seconds\n",
      "\n",
      "--- Running with Seed: 59 ---\n",
      "--- Seed 59 Results ---\n",
      "Success Rate: 0.5800 (58.00%)\n",
      "Average Detection Time (Successful Episodes): 6.33\n",
      "Average Episode Reward: -0.5930\n",
      "Average Episode Length: 7.87\n",
      "Time taken for this seed: 191.50 seconds\n",
      "\n",
      "--- Running with Seed: 60 ---\n",
      "--- Seed 60 Results ---\n",
      "Success Rate: 0.5700 (57.00%)\n",
      "Average Detection Time (Successful Episodes): 4.68\n",
      "Average Episode Reward: -0.6635\n",
      "Average Episode Length: 6.97\n",
      "Time taken for this seed: 160.57 seconds\n",
      "\n",
      "--- Running with Seed: 61 ---\n",
      "--- Seed 61 Results ---\n",
      "Success Rate: 0.6500 (65.00%)\n",
      "Average Detection Time (Successful Episodes): 4.51\n",
      "Average Episode Reward: -0.4825\n",
      "Average Episode Length: 6.43\n",
      "Time taken for this seed: 156.25 seconds\n",
      "\n",
      "--- Running with Seed: 62 ---\n",
      "--- Seed 62 Results ---\n",
      "Success Rate: 0.6100 (61.00%)\n",
      "Average Detection Time (Successful Episodes): 4.59\n",
      "Average Episode Reward: -0.5055\n",
      "Average Episode Length: 6.70\n",
      "Time taken for this seed: 150.20 seconds\n",
      "\n",
      "--- Running with Seed: 63 ---\n",
      "--- Seed 63 Results ---\n",
      "Success Rate: 0.6100 (61.00%)\n",
      "Average Detection Time (Successful Episodes): 5.08\n",
      "Average Episode Reward: -0.4670\n",
      "Average Episode Length: 7.00\n",
      "Time taken for this seed: 152.07 seconds\n",
      "\n",
      "--- Running with Seed: 64 ---\n",
      "--- Seed 64 Results ---\n",
      "Success Rate: 0.5400 (54.00%)\n",
      "Average Detection Time (Successful Episodes): 4.69\n",
      "Average Episode Reward: -0.5875\n",
      "Average Episode Length: 7.13\n",
      "Time taken for this seed: 159.45 seconds\n",
      "\n",
      "--- Running with Seed: 65 ---\n",
      "--- Seed 65 Results ---\n",
      "Success Rate: 0.5600 (56.00%)\n",
      "Average Detection Time (Successful Episodes): 6.11\n",
      "Average Episode Reward: -0.7045\n",
      "Average Episode Length: 7.82\n",
      "Time taken for this seed: 185.28 seconds\n",
      "\n",
      "--- Running with Seed: 66 ---\n",
      "--- Seed 66 Results ---\n",
      "Success Rate: 0.6200 (62.00%)\n",
      "Average Detection Time (Successful Episodes): 5.58\n",
      "Average Episode Reward: -0.4825\n",
      "Average Episode Length: 7.26\n",
      "Time taken for this seed: 172.31 seconds\n",
      "\n",
      "--- Running with Seed: 67 ---\n",
      "--- Seed 67 Results ---\n",
      "Success Rate: 0.5900 (59.00%)\n",
      "Average Detection Time (Successful Episodes): 5.39\n",
      "Average Episode Reward: -0.5850\n",
      "Average Episode Length: 7.28\n",
      "Time taken for this seed: 170.40 seconds\n",
      "\n",
      "--- Running with Seed: 68 ---\n",
      "--- Seed 68 Results ---\n",
      "Success Rate: 0.5600 (56.00%)\n",
      "Average Detection Time (Successful Episodes): 5.29\n",
      "Average Episode Reward: -0.6240\n",
      "Average Episode Length: 7.36\n",
      "Time taken for this seed: 176.00 seconds\n",
      "\n",
      "--- Running with Seed: 69 ---\n",
      "--- Seed 69 Results ---\n",
      "Success Rate: 0.6300 (63.00%)\n",
      "Average Detection Time (Successful Episodes): 5.81\n",
      "Average Episode Reward: -0.5180\n",
      "Average Episode Length: 7.36\n",
      "Time taken for this seed: 185.00 seconds\n",
      "\n",
      "--- Running with Seed: 70 ---\n",
      "--- Seed 70 Results ---\n",
      "Success Rate: 0.5600 (56.00%)\n",
      "Average Detection Time (Successful Episodes): 5.50\n",
      "Average Episode Reward: -0.6570\n",
      "Average Episode Length: 7.48\n",
      "Time taken for this seed: 165.15 seconds\n",
      "\n",
      "--- Running with Seed: 71 ---\n",
      "--- Seed 71 Results ---\n",
      "Success Rate: 0.5100 (51.00%)\n",
      "Average Detection Time (Successful Episodes): 4.69\n",
      "Average Episode Reward: -0.6940\n",
      "Average Episode Length: 7.29\n",
      "Time taken for this seed: 166.93 seconds\n",
      "\n",
      "--- Running with Seed: 72 ---\n",
      "--- Seed 72 Results ---\n",
      "Success Rate: 0.6200 (62.00%)\n",
      "Average Detection Time (Successful Episodes): 5.63\n",
      "Average Episode Reward: -0.5025\n",
      "Average Episode Length: 7.29\n",
      "Time taken for this seed: 178.39 seconds\n",
      "\n",
      "--- Running with Seed: 73 ---\n",
      "--- Seed 73 Results ---\n",
      "Success Rate: 0.5900 (59.00%)\n",
      "Average Detection Time (Successful Episodes): 5.49\n",
      "Average Episode Reward: -0.6485\n",
      "Average Episode Length: 7.34\n",
      "Time taken for this seed: 170.72 seconds\n",
      "\n",
      "--- Running with Seed: 74 ---\n",
      "--- Seed 74 Results ---\n",
      "Success Rate: 0.5900 (59.00%)\n",
      "Average Detection Time (Successful Episodes): 5.83\n",
      "Average Episode Reward: -0.5960\n",
      "Average Episode Length: 7.54\n",
      "Time taken for this seed: 175.37 seconds\n",
      "\n",
      "--- Running with Seed: 75 ---\n",
      "--- Seed 75 Results ---\n",
      "Success Rate: 0.7600 (76.00%)\n",
      "Average Detection Time (Successful Episodes): 5.57\n",
      "Average Episode Reward: -0.3955\n",
      "Average Episode Length: 6.63\n",
      "Time taken for this seed: 159.19 seconds\n",
      "\n",
      "--- Running with Seed: 76 ---\n",
      "--- Seed 76 Results ---\n",
      "Success Rate: 0.6000 (60.00%)\n",
      "Average Detection Time (Successful Episodes): 5.07\n",
      "Average Episode Reward: -0.6255\n",
      "Average Episode Length: 7.04\n",
      "Time taken for this seed: 150.86 seconds\n",
      "\n",
      "--- Running with Seed: 77 ---\n",
      "--- Seed 77 Results ---\n",
      "Success Rate: 0.6500 (65.00%)\n",
      "Average Detection Time (Successful Episodes): 4.94\n",
      "Average Episode Reward: -0.5245\n",
      "Average Episode Length: 6.71\n",
      "Time taken for this seed: 161.96 seconds\n",
      "\n",
      "--- Running with Seed: 78 ---\n",
      "--- Seed 78 Results ---\n",
      "Success Rate: 0.6600 (66.00%)\n",
      "Average Detection Time (Successful Episodes): 5.44\n",
      "Average Episode Reward: -0.5795\n",
      "Average Episode Length: 6.99\n",
      "Time taken for this seed: 153.81 seconds\n",
      "\n",
      "--- Running with Seed: 79 ---\n",
      "--- Seed 79 Results ---\n",
      "Success Rate: 0.5700 (57.00%)\n",
      "Average Detection Time (Successful Episodes): 5.02\n",
      "Average Episode Reward: -0.6430\n",
      "Average Episode Length: 7.16\n",
      "Time taken for this seed: 160.62 seconds\n",
      "\n",
      "--- Running with Seed: 80 ---\n",
      "--- Seed 80 Results ---\n",
      "Success Rate: 0.6000 (60.00%)\n",
      "Average Detection Time (Successful Episodes): 4.80\n",
      "Average Episode Reward: -0.4755\n",
      "Average Episode Length: 6.88\n",
      "Time taken for this seed: 150.09 seconds\n",
      "\n",
      "--- Running with Seed: 81 ---\n",
      "--- Seed 81 Results ---\n",
      "Success Rate: 0.4800 (48.00%)\n",
      "Average Detection Time (Successful Episodes): 5.65\n",
      "Average Episode Reward: -0.7305\n",
      "Average Episode Length: 7.91\n",
      "Time taken for this seed: 181.53 seconds\n",
      "\n",
      "--- Running with Seed: 82 ---\n",
      "--- Seed 82 Results ---\n",
      "Success Rate: 0.6300 (63.00%)\n",
      "Average Detection Time (Successful Episodes): 6.41\n",
      "Average Episode Reward: -0.5890\n",
      "Average Episode Length: 7.74\n",
      "Time taken for this seed: 208.81 seconds\n",
      "\n",
      "--- Running with Seed: 83 ---\n",
      "--- Seed 83 Results ---\n",
      "Success Rate: 0.6100 (61.00%)\n",
      "Average Detection Time (Successful Episodes): 4.21\n",
      "Average Episode Reward: -0.5400\n",
      "Average Episode Length: 6.47\n",
      "Time taken for this seed: 135.83 seconds\n",
      "\n",
      "--- Running with Seed: 84 ---\n",
      "--- Seed 84 Results ---\n",
      "Success Rate: 0.5400 (54.00%)\n",
      "Average Detection Time (Successful Episodes): 4.43\n",
      "Average Episode Reward: -0.7075\n",
      "Average Episode Length: 6.99\n",
      "Time taken for this seed: 152.40 seconds\n",
      "\n",
      "--- Running with Seed: 85 ---\n",
      "--- Seed 85 Results ---\n",
      "Success Rate: 0.5700 (57.00%)\n",
      "Average Detection Time (Successful Episodes): 5.44\n",
      "Average Episode Reward: -0.5765\n",
      "Average Episode Length: 7.40\n",
      "Time taken for this seed: 182.14 seconds\n",
      "\n",
      "--- Running with Seed: 86 ---\n",
      "--- Seed 86 Results ---\n",
      "Success Rate: 0.5500 (55.00%)\n",
      "Average Detection Time (Successful Episodes): 6.38\n",
      "Average Episode Reward: -0.6915\n",
      "Average Episode Length: 8.01\n",
      "Time taken for this seed: 175.34 seconds\n",
      "\n",
      "--- Running with Seed: 87 ---\n",
      "--- Seed 87 Results ---\n",
      "Success Rate: 0.5900 (59.00%)\n",
      "Average Detection Time (Successful Episodes): 4.71\n",
      "Average Episode Reward: -0.6935\n",
      "Average Episode Length: 6.88\n",
      "Time taken for this seed: 149.74 seconds\n",
      "\n",
      "--- Running with Seed: 88 ---\n",
      "--- Seed 88 Results ---\n",
      "Success Rate: 0.6000 (60.00%)\n",
      "Average Detection Time (Successful Episodes): 5.48\n",
      "Average Episode Reward: -0.7070\n",
      "Average Episode Length: 7.29\n",
      "Time taken for this seed: 154.17 seconds\n",
      "\n",
      "--- Running with Seed: 89 ---\n",
      "--- Seed 89 Results ---\n",
      "Success Rate: 0.6900 (69.00%)\n",
      "Average Detection Time (Successful Episodes): 5.09\n",
      "Average Episode Reward: -0.4390\n",
      "Average Episode Length: 6.61\n",
      "Time taken for this seed: 151.06 seconds\n",
      "\n",
      "--- Running with Seed: 90 ---\n",
      "--- Seed 90 Results ---\n",
      "Success Rate: 0.5200 (52.00%)\n",
      "Average Detection Time (Successful Episodes): 4.69\n",
      "Average Episode Reward: -0.7170\n",
      "Average Episode Length: 7.24\n",
      "Time taken for this seed: 161.81 seconds\n",
      "\n",
      "--- Running with Seed: 91 ---\n",
      "--- Seed 91 Results ---\n",
      "Success Rate: 0.6700 (67.00%)\n",
      "Average Detection Time (Successful Episodes): 4.78\n",
      "Average Episode Reward: -0.4445\n",
      "Average Episode Length: 6.50\n",
      "Time taken for this seed: 147.35 seconds\n",
      "\n",
      "--- Running with Seed: 92 ---\n",
      "--- Seed 92 Results ---\n",
      "Success Rate: 0.7800 (78.00%)\n",
      "Average Detection Time (Successful Episodes): 4.36\n",
      "Average Episode Reward: -0.3185\n",
      "Average Episode Length: 5.60\n",
      "Time taken for this seed: 144.29 seconds\n",
      "\n",
      "--- Running with Seed: 93 ---\n",
      "--- Seed 93 Results ---\n",
      "Success Rate: 0.5300 (53.00%)\n",
      "Average Detection Time (Successful Episodes): 6.11\n",
      "Average Episode Reward: -0.6225\n",
      "Average Episode Length: 7.94\n",
      "Time taken for this seed: 213.05 seconds\n",
      "\n",
      "--- Running with Seed: 94 ---\n",
      "--- Seed 94 Results ---\n",
      "Success Rate: 0.6300 (63.00%)\n",
      "Average Detection Time (Successful Episodes): 4.49\n",
      "Average Episode Reward: -0.4465\n",
      "Average Episode Length: 6.53\n",
      "Time taken for this seed: 149.08 seconds\n",
      "\n",
      "--- Running with Seed: 95 ---\n",
      "--- Seed 95 Results ---\n",
      "Success Rate: 0.5100 (51.00%)\n",
      "Average Detection Time (Successful Episodes): 6.33\n",
      "Average Episode Reward: -0.6225\n",
      "Average Episode Length: 8.13\n",
      "Time taken for this seed: 203.58 seconds\n",
      "\n",
      "--- Running with Seed: 96 ---\n",
      "--- Seed 96 Results ---\n",
      "Success Rate: 0.6300 (63.00%)\n",
      "Average Detection Time (Successful Episodes): 5.41\n",
      "Average Episode Reward: -0.5130\n",
      "Average Episode Length: 7.11\n",
      "Time taken for this seed: 179.96 seconds\n",
      "\n",
      "--- Running with Seed: 97 ---\n",
      "--- Seed 97 Results ---\n",
      "Success Rate: 0.5900 (59.00%)\n",
      "Average Detection Time (Successful Episodes): 5.53\n",
      "Average Episode Reward: -0.5840\n",
      "Average Episode Length: 7.36\n",
      "Time taken for this seed: 171.09 seconds\n",
      "\n",
      "--- Running with Seed: 98 ---\n",
      "--- Seed 98 Results ---\n",
      "Success Rate: 0.6400 (64.00%)\n",
      "Average Detection Time (Successful Episodes): 4.42\n",
      "Average Episode Reward: -0.4975\n",
      "Average Episode Length: 6.43\n",
      "Time taken for this seed: 147.02 seconds\n",
      "\n",
      "--- Running with Seed: 99 ---\n",
      "--- Seed 99 Results ---\n",
      "Success Rate: 0.6300 (63.00%)\n",
      "Average Detection Time (Successful Episodes): 5.44\n",
      "Average Episode Reward: -0.5795\n",
      "Average Episode Length: 7.13\n",
      "Time taken for this seed: 154.90 seconds\n",
      "\n",
      "--- Overall 1-step Lookahead Policy Results ---\n",
      "Total experiment time across 100 seeds: 16941.06 seconds\n",
      "Average Success Rate over 100 seeds and 100 episodes each: 0.5987 (59.87%)\n",
      "Standard Deviation of Success Rate: 0.0600\n",
      "Average Detection Time (Successful Episodes) across all seeds: 5.35\n",
      "Standard Deviation of Detection Time: 2.67\n",
      "Average Episode Reward across all seeds: -0.5682\n",
      "Standard Deviation of Reward: 0.9172\n",
      "Average Episode Length across all seeds: 7.22\n",
      "Standard Deviation of Episode Length: 3.08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import time\n",
    "\n",
    "# --- Start of SingleObjectDPSolver (copied from txTdQaQVhkrA) ---\n",
    "class SingleObjectDPSolver:\n",
    "    \"\"\"\n",
    "    An exact Dynamic Programming solver for a single-object search problem.\n",
    "\n",
    "    This class uses backward recursion to find the optimal policy based on a\n",
    "    marginal prior distribution for a single target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n: int, T: int, p0_marginal: np.ndarray, gamma1: np.ndarray, c: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initializes the solver with the single-object problem parameters.\n",
    "\n",
    "        Args:\n",
    "            n (int): The number of cells.\n",
    "            T (int): The time horizon.\n",
    "            p0_marginal (np.ndarray): The n-element marginal prior probability vector for O1.\n",
    "            gamma1 (np.ndarray): Miss-detection rates for O1.\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.T = T\n",
    "        self.p0_marginal = p0_marginal\n",
    "        self.gamma1 = gamma1\n",
    "        self.c = c\n",
    "\n",
    "        # Data structures to store the results\n",
    "        self.J_values: Dict[int, Dict[Tuple[int, ...], float]] = {}  # Value function J_t(z_t)\n",
    "        self.Policy: Dict[int, Dict[Tuple[int, ...], int]] = {}      # Policy mu_t(z_t)\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None) # Memoization for performance\n",
    "    def _generate_z_vectors(t: int, n: int) -> List[Tuple[int, ...]]:\n",
    "        \"\"\"Recursively generates all state vectors z_t where sum(z_i) = t.\"\"\"\n",
    "        if n == 1:\n",
    "            return [(t,)]\n",
    "\n",
    "        vectors = []\n",
    "        for i in range(t + 1):\n",
    "            for sub_vector in SingleObjectDPSolver._generate_z_vectors(t - i, n - 1):\n",
    "                vectors.append((i,) + sub_vector)\n",
    "        return vectors\n",
    "\n",
    "    def _calculate_posterior(self, z_vector: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculates the posterior belief p(O1=i | z).\"\"\"\n",
    "        g1_z = np.power(self.gamma1, z_vector)\n",
    "        numerator = self.p0_marginal * g1_z\n",
    "        norm = np.sum(numerator)\n",
    "        return numerator / norm if norm > 0 else numerator\n",
    "\n",
    "    def solve(self):\n",
    "        \"\"\"\n",
    "        Executes the backward recursion to solve the DP problem.\n",
    "        \"\"\"\n",
    "        # print(\"Starting Single-Object DP solver...\")\n",
    "        # --- Initialization at T ---\n",
    "        # print(f\"Initializing for T={self.T}...\")\n",
    "        self.J_values[self.T] = {}\n",
    "        z_vectors_T = self._generate_z_vectors(self.T, self.n)\n",
    "        for z_T in z_vectors_T:\n",
    "            self.J_values[self.T][z_T] = 0.0\n",
    "\n",
    "        # --- Backward Recursion ---\n",
    "        for t in range(self.T - 1, -1, -1):\n",
    "            #start_time = time.time()\n",
    "            J_t = {}\n",
    "            policy_t = {}\n",
    "            z_vectors_t = self._generate_z_vectors(t, self.n)\n",
    "\n",
    "            for z_t_tuple in z_vectors_t:\n",
    "                z_t = np.array(z_t_tuple)\n",
    "                action_values = []\n",
    "\n",
    "                # Calculate current belief vector\n",
    "                belief_t = self._calculate_posterior(z_t)\n",
    "\n",
    "                # Iterate over all possible actions\n",
    "                for a_t in range(self.n):\n",
    "                    # Probability of finding the target if we search cell a_t\n",
    "                    p_success = (1 - self.gamma1[a_t]) * belief_t[a_t]\n",
    "                    p_fail = 1 - ((1 - self.gamma1[a_t]) * belief_t[a_t])\n",
    "\n",
    "                    # Get the value of the state we transition to upon failure\n",
    "                    next_z_tuple = tuple(z_t + np.eye(self.n, dtype=int)[a_t])\n",
    "                    val_if_fail = self.J_values[t + 1][next_z_tuple]\n",
    "\n",
    "                    # Expected value for this action (Bellman equation)\n",
    "                    # Reward for success is 1, reward for failure is 0\n",
    "                    expected_value = (p_success * 1.0) - self.c[a_t] + (p_fail * val_if_fail)\n",
    "                    action_values.append(expected_value)\n",
    "\n",
    "                # Find best action and store value/policy\n",
    "                best_value = np.max(action_values)\n",
    "                best_action = np.argmax(action_values)\n",
    "                J_t[z_t_tuple] = best_value\n",
    "                policy_t[z_t_tuple] = best_action\n",
    "\n",
    "            self.J_values[t] = J_t\n",
    "            self.Policy[t] = policy_t\n",
    "            #end_time = time.time()\n",
    "            # print(f\"Completed t={t}. Found {len(z_vectors_t)} states. Took {end_time - start_time:.2f}s.\")\n",
    "\n",
    "        # print(\"DP solver finished.\")\n",
    "\n",
    "    def get_optimal_value(self) -> float:\n",
    "        \"\"\"Returns the optimal value J(z_0).\"\"\"\n",
    "        initial_z = tuple([0] * self.n)\n",
    "        return self.J_values[0][initial_z]\n",
    "# --- End of SingleObjectDPSolver ---\n",
    "\n",
    "\n",
    "# --- Start of TwoObjectDPSolver (copied from Uu36DFAN5dbB) ---\n",
    "class TwoObjectDPSolver:\n",
    "    \"\"\"\n",
    "    An exact Dynamic Programming solver for the two-object search problem.\n",
    "\n",
    "    This class uses backward recursion to find the optimal policy and the\n",
    "    maximum probability of detecting the primary target (O1).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n: int, T: int, p0: np.ndarray, gamma1: np.ndarray, gamma2: np.ndarray, c: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initializes the solver with the problem parameters.\n",
    "\n",
    "        Args:\n",
    "            n (int): The number of cells.\n",
    "            T (int): The time horizon.\n",
    "            p0 (np.ndarray): The n x n prior joint probability matrix.\n",
    "            gamma1 (np.ndarray): Miss-detection rates for O1.\n",
    "            gamma2 (np.ndarray): Miss-detection rates for O2.\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.T = T\n",
    "        self.p0 = p0\n",
    "        self.gamma1 = gamma1\n",
    "        self.gamma2 = gamma2\n",
    "        self.c = c\n",
    "\n",
    "        # Data structures to store the results\n",
    "        self.J_values: Dict[int, Dict[Tuple, float]] = {}  # Value function J_t(s_t)\n",
    "        self.Policy: Dict[int, Dict[Tuple, int]] = {}      # Policy mu_t(s_t)\n",
    "\n",
    "        # Pre-calculate initial conditional priors for efficiency\n",
    "        self._p0_conditionals = self._precompute_p0_conditionals()\n",
    "\n",
    "    def _precompute_p0_conditionals(self) -> List[np.ndarray]:\n",
    "        \"\"\"Pre-calculates P(O1=i | O2=k) for all k.\"\"\"\n",
    "        conditionals = []\n",
    "        for k in range(self.n):\n",
    "            marginal_o2 = np.sum(self.p0[:, k])\n",
    "            if marginal_o2 > 0:\n",
    "                conditionals.append(self.p0[:, k] / marginal_o2)\n",
    "            else:\n",
    "                # If O2 can never be in cell k, the conditional is undefined. Use zeros.\n",
    "                conditionals.append(np.zeros(self.n))\n",
    "        return conditionals\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None) # Memoization for performance\n",
    "    def _generate_z_vectors(t: int, n: int) -> List[Tuple[int, ...]]:\n",
    "        \"\"\"Recursively generates all state vectors z_t where sum(z_i) = t.\"\"\"\n",
    "        if n == 1:\n",
    "            return [(t,)]\n",
    "\n",
    "        vectors = []\n",
    "        for i in range(t + 1):\n",
    "            for sub_vector in TwoObjectDPSolver._generate_z_vectors(t - i, n - 1):\n",
    "                vectors.append((i,) + sub_vector)\n",
    "        \"\"\"if n == 5:\n",
    "           for vec in vectors:\n",
    "               print(vec)\"\"\"\n",
    "        return vectors\n",
    "\n",
    "    def _calculate_joint_posterior(self, z_vector: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculates p(O1=i, O2=j | z).\"\"\"\n",
    "        g1_z = np.power(self.gamma1, z_vector)\n",
    "        g2_z = np.power(self.gamma2, z_vector)\n",
    "        likelihood = np.outer(g1_z, g2_z)\n",
    "        numerator = self.p0 * likelihood\n",
    "        norm = np.sum(numerator)\n",
    "        return numerator / norm if norm > 0 else numerator\n",
    "\n",
    "    def _calculate_conditional_posterior(self, z_vector: np.ndarray, o2_loc: int) -> np.ndarray:\n",
    "        \"\"\"Calculates p(O1=i | z, O2=o2_loc).\"\"\"\n",
    "        p0_cond = self._p0_conditionals[o2_loc]\n",
    "        g1_z = np.power(self.gamma1, z_vector)\n",
    "        numerator = g1_z * p0_cond\n",
    "        norm = np.sum(numerator)\n",
    "        return numerator / norm if norm > 0 else numerator\n",
    "\n",
    "    def solve(self):\n",
    "        \"\"\"\n",
    "        Executes the backward recursion to solve the DP problem.\n",
    "        \"\"\"\n",
    "        #print(\"Starting DP solver...\")\n",
    "        # --- Initialization at T ---\n",
    "        #print(f\"Initializing for T={self.T}...\")\n",
    "        self.J_values[self.T] = {}\n",
    "        z_vectors_T = self._generate_z_vectors(self.T, self.n)\n",
    "        for z_T in z_vectors_T:\n",
    "            for theta2 in range(self.n + 1):\n",
    "                state = (z_T, theta2)\n",
    "                self.J_values[self.T][state] = 0.0\n",
    "\n",
    "        # --- Backward Recursion ---\n",
    "        for t in range(self.T - 1, -1, -1):\n",
    "            #start_time = time.time()\n",
    "            J_t = {}\n",
    "            policy_t = {}\n",
    "            z_vectors_t = self._generate_z_vectors(t, self.n)\n",
    "\n",
    "            for z_t_tuple in z_vectors_t:\n",
    "                z_t = np.array(z_t_tuple)\n",
    "                for theta2 in range(self.n + 1):\n",
    "                    if t == 0 and theta2 > 0:\n",
    "                        break\n",
    "                    current_state = (z_t_tuple, theta2)\n",
    "                    action_values = []\n",
    "\n",
    "                    # Calculate belief based on current state\n",
    "                    if theta2 == 0:\n",
    "                        belief = self._calculate_joint_posterior(z_t)\n",
    "                    else:\n",
    "                        o2_loc = theta2 - 1\n",
    "                        belief = self._calculate_conditional_posterior(z_t, o2_loc)\n",
    "\n",
    "                    # Iterate over all possible actions\n",
    "                    for a_t in range(self.n):\n",
    "                        next_z_tuple = tuple(z_t + np.eye(self.n, dtype=int)[a_t])\n",
    "\n",
    "                        # --- Case 1: Both objects hidden ---\n",
    "                        if theta2 == 0:\n",
    "                            p_marginal_o1_at_a = np.sum(belief[a_t, :])\n",
    "\n",
    "                            # Prob of success (finding O1)\n",
    "                            p_success = (1 - self.gamma1[a_t]) * p_marginal_o1_at_a\n",
    "\n",
    "                            # Prob of finding O2 only\n",
    "                            p_cond_sum = np.sum(belief[np.arange(self.n) != a_t, a_t])\n",
    "                            p_find_o2_only = (1 - self.gamma2[a_t]) * (self.gamma1[a_t] * belief[a_t, a_t] + p_cond_sum)\n",
    "\n",
    "                            # Prob of finding nothing\n",
    "                            p_nothing = 1 - p_success - p_find_o2_only\n",
    "\n",
    "                            # Future values from next stage\n",
    "                            val_if_nothing = self.J_values[t + 1][(next_z_tuple, 0)]\n",
    "                            val_if_found_o2 = self.J_values[t + 1][(next_z_tuple, a_t + 1)]\n",
    "\n",
    "                            # Expected value for this action\n",
    "                            expected_value = (p_success * 1.0) - self.c[a_t]+ \\\n",
    "                                             (p_nothing * val_if_nothing) + \\\n",
    "                                             (p_find_o2_only * val_if_found_o2)\n",
    "                            action_values.append(expected_value)\n",
    "\n",
    "                        # --- Case 2: O2 has been found ---\n",
    "                        else:\n",
    "                            p_o1_at_a = belief[a_t]\n",
    "                            p_success = (1 - self.gamma1[a_t]) * p_o1_at_a\n",
    "                            p_fail = 1 - p_success\n",
    "\n",
    "                            val_if_fail = self.J_values[t + 1][(next_z_tuple, theta2)]\n",
    "\n",
    "                            expected_value = (p_success * 1.0) - self.c[a_t]+ (p_fail * val_if_fail)\n",
    "                            action_values.append(expected_value)\n",
    "\n",
    "                    # Find best action and store value/policy\n",
    "                    best_value = np.max(action_values)\n",
    "                    best_action = np.argmax(action_values)\n",
    "                    J_t[current_state] = best_value\n",
    "                    policy_t[current_state] = best_action\n",
    "\n",
    "            self.J_values[t] = J_t\n",
    "            self.Policy[t] = policy_t\n",
    "            #end_time = time.time()\n",
    "            #print(f\"Completed t={t}. Found {len(z_vectors_t)} states. Took {end_time - start_time:.2f}s.\")\n",
    "\n",
    "        #print(\"DP solver finished.\")\n",
    "\n",
    "    def get_optimal_value(self) -> float:\n",
    "        \"\"\"Returns the optimal value J(s_0).\"\"\"\n",
    "        initial_z = tuple([0] * self.n)\n",
    "        initial_state = (initial_z, 0)\n",
    "        return self.J_values[0][initial_state]\n",
    "# --- End of TwoObjectDPSolver ---\n",
    "\n",
    "\n",
    "class KStepLookaheadSolver:\n",
    "    \"\"\"\n",
    "    A k-step lookahead solver using rollout with a greedy policy for value estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n: int, T: int, initial_episode_belief: Any, # Can be (n,n) or (n,)\n",
    "                 gamma1: np.ndarray, gamma2: np.ndarray, theta2_at_init: int, k: int, c: np.ndarray, rollout_sims: int = 100, solver_seed: int = None):\n",
    "        self.n = n\n",
    "        self.T_global = T # Store global horizon for rollout remaining_horizon calculation\n",
    "        self.initial_episode_belief = initial_episode_belief # This is the current belief from the global episode\n",
    "        self.gamma1 = gamma1\n",
    "        self.gamma2 = gamma2\n",
    "        self.c = c\n",
    "        self.k = k # Lookahead horizon for *this* solver instance\n",
    "        self.rollout_sims = rollout_sims\n",
    "        self.theta2_at_init = theta2_at_init # O2 state when this k-step solver was created\n",
    "\n",
    "        self.J_values: Dict[int, Dict[Tuple, float]] = {}\n",
    "        self.Policy: Dict[int, Dict[Tuple, int]] = {}\n",
    "\n",
    "        # Initialize a seeded random number generator for reproducible rollouts\n",
    "        self.rng = np.random.default_rng(solver_seed)\n",
    "\n",
    "        # Precompute conditionals from the initial joint belief if theta2_at_init == 0\n",
    "        # These are used if O2 is hidden initially, but discovered within the k-step lookahead\n",
    "        if self.theta2_at_init == 0 and self.initial_episode_belief.ndim == 2:\n",
    "            self._precomputed_conditionals_from_initial_joint = self._precompute_conditionals_from_joint(self.initial_episode_belief)\n",
    "        else:\n",
    "            self._precomputed_conditionals_from_initial_joint = None\n",
    "\n",
    "    def _precompute_conditionals_from_joint(self, joint_p: np.ndarray) -> List[np.ndarray]:\n",
    "        \"\"\"Pre-calculates P(O1=i | O2=k) for all k from a given joint prior.\"\"\"\n",
    "        conditionals = []\n",
    "        for k_idx in range(self.n):\n",
    "            marginal_o2 = np.sum(joint_p[:, k_idx])\n",
    "            conditionals.append(joint_p[:, k_idx] / marginal_o2 if marginal_o2 > 0 else np.zeros(self.n))\n",
    "        return conditionals\n",
    "\n",
    "    def _calculate_posterior_at_step_in_k_lookahead(self, z_relative: np.ndarray, current_theta2_in_k_step: int) -> Any:\n",
    "        \"\"\"\n",
    "        Calculates the posterior belief for a state (z_relative, current_theta2_in_k_step)\n",
    "        within this k-step lookahead segment, starting from `self.initial_episode_belief`.\n",
    "        `z_relative`: observations accumulated *within this k-step window*.\n",
    "        `current_theta2_in_k_step`: the theta2 state *within this k-step window*.\n",
    "        \"\"\"\n",
    "        if current_theta2_in_k_step == 0: # O2 is still hidden\n",
    "            # If self.theta2_at_init was already > 0, this case should not happen for a consistent path\n",
    "            if self.initial_episode_belief.ndim == 1:\n",
    "                # This implies an inconsistency, should raise error or handle carefully\n",
    "                return np.zeros((self.n, self.n)) # Return a zero joint if initial was marginal\n",
    "\n",
    "            g1_z = np.power(self.gamma1, z_relative)\n",
    "            g2_z = np.power(self.gamma2, z_relative)\n",
    "            likelihood = np.outer(g1_z, g2_z)\n",
    "            numerator = self.initial_episode_belief * likelihood\n",
    "            norm = np.sum(numerator)\n",
    "            return numerator / norm if norm > 0 else numerator\n",
    "        else: # O2 is found (either initially or within this k-step segment)\n",
    "            if self.theta2_at_init > 0:\n",
    "                # O2 was already found when KStepLookaheadSolver was instantiated\n",
    "                # initial_episode_belief is already the marginal p(O1 | O2=actual_loc)\n",
    "                base_prior_for_o1 = self.initial_episode_belief\n",
    "            else:\n",
    "                # O2 was hidden initially but found within the k-step lookahead\n",
    "                # We need to get the conditional prior for O1 from the initial joint belief\n",
    "                o2_loc = current_theta2_in_k_step - 1\n",
    "                base_prior_for_o1 = self._precomputed_conditionals_from_initial_joint[o2_loc]\n",
    "\n",
    "            g1_z = np.power(self.gamma1, z_relative)\n",
    "            numerator = g1_z * base_prior_for_o1\n",
    "            norm = np.sum(numerator)\n",
    "            return numerator / norm if norm > 0 else numerator\n",
    "\n",
    "    def _select_greedy_action(self, belief: np.ndarray, theta2_for_greedy: int) -> int:\n",
    "        \"\"\"Selects a greedy action based on the current belief.\"\"\"\n",
    "        if theta2_for_greedy == 0: # Belief is joint (n,n)\n",
    "            p_marginal_o1 = np.sum(belief, axis=1)\n",
    "            immediate_value = (1 - self.gamma1) * p_marginal_o1\n",
    "        else: # Belief is marginal for O1 (n,)\n",
    "            immediate_value = (1 - self.gamma1) * belief\n",
    "        return np.argmax(immediate_value - self.c)\n",
    "\n",
    "\n",
    "    def _run_rollout(self, rollout_z_relative: np.ndarray, rollout_theta2_in_k_step: int, remaining_global_horizon: int) -> float:\n",
    "        \"\"\"\n",
    "        Estimates the value by simulating episodes with a greedy policy.\n",
    "        `rollout_z_relative`: z_vector accumulated *within the k-step lookahead* before rollout.\n",
    "        `rollout_theta2_in_k_step`: theta2 state *within the k-step lookahead* before rollout.\n",
    "        `remaining_global_horizon`: Actual remaining steps in the *global* problem after the k-step lookahead.\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "\n",
    "        for _ in range(self.rollout_sims):\n",
    "            current_z_in_rollout = np.copy(rollout_z_relative)\n",
    "            current_theta2_in_rollout = rollout_theta2_in_k_step\n",
    "\n",
    "            # Sample true object locations for this rollout simulation based on the *current belief*\n",
    "            current_belief_for_sampling = self._calculate_posterior_at_step_in_k_lookahead(current_z_in_rollout, current_theta2_in_rollout)\n",
    "\n",
    "            if current_theta2_in_rollout == 0: # Belief is joint\n",
    "                flat_belief = current_belief_for_sampling.flatten()\n",
    "                if np.sum(flat_belief) > 0:\n",
    "                    choice_idx = self.rng.choice(self.n * self.n, p=flat_belief / np.sum(flat_belief))\n",
    "                    rollout_true_pos_o1, rollout_true_pos_o2 = np.unravel_index(choice_idx, (self.n, self.n))\n",
    "                else:\n",
    "                    rollout_true_pos_o1, rollout_true_pos_o2 = -1, -1\n",
    "            else: # Belief is marginal for O1\n",
    "                if np.sum(current_belief_for_sampling) > 0:\n",
    "                    rollout_true_pos_o1 = self.rng.choice(self.n, p=current_belief_for_sampling / np.sum(current_belief_for_sampling))\n",
    "                    rollout_true_pos_o2 = current_theta2_in_rollout - 1 # O2 location is known\n",
    "                else:\n",
    "                    rollout_true_pos_o1, rollout_true_pos_o2 = -1, -1\n",
    "\n",
    "            episode_rollout_reward = 0.0\n",
    "            for t_rollout in range(remaining_global_horizon): # Iterate for remaining time steps\n",
    "                # Get belief for greedy action decision (no z_relative update yet for this step's decision)\n",
    "                current_belief_for_greedy_action = self._calculate_posterior_at_step_in_k_lookahead(current_z_in_rollout, current_theta2_in_rollout)\n",
    "                action = self._select_greedy_action(current_belief_for_greedy_action, current_theta2_in_rollout)\n",
    "\n",
    "                episode_rollout_reward -= self.c[action]\n",
    "\n",
    "                # Simulate outcome of greedy action for rollout\n",
    "                found_o1_in_rollout = (action == rollout_true_pos_o1) and (self.rng.random() > self.gamma1[action])\n",
    "                if found_o1_in_rollout:\n",
    "                    episode_rollout_reward += 1.0\n",
    "                    break # O1 found, rollout ends\n",
    "\n",
    "                if current_theta2_in_rollout == 0 and action == rollout_true_pos_o2 and (self.rng.random() > self.gamma2[action]):\n",
    "                    current_theta2_in_rollout = action + 1 # O2 found within rollout\n",
    "\n",
    "                current_z_in_rollout[action] += 1 # Update z for next rollout step\n",
    "            total_reward += episode_rollout_reward\n",
    "        return total_reward / self.rollout_sims\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None)\n",
    "    def _generate_z_vectors(t: int, n: int) -> List[Tuple[int, ...]]:\n",
    "        if n == 1:\n",
    "            return [(t,)]\n",
    "        vectors = []\n",
    "        for i in range(t + 1):\n",
    "            for sub_vector in KStepLookaheadSolver._generate_z_vectors(t - i, n - 1):\n",
    "                vectors.append((i,) + sub_vector)\n",
    "        return vectors\n",
    "\n",
    "    def solve(self): # For theta2_at_init == 0\n",
    "        \"\"\"\n",
    "        Executes DP for k steps from self.initial_episode_belief, for theta2_at_init = 0.\n",
    "        \"\"\"\n",
    "        self.J_values[self.k] = {}\n",
    "        z_vectors_k = self._generate_z_vectors(self.k, self.n)\n",
    "        for z_k_relative_tuple in z_vectors_k:\n",
    "            for theta2_in_k_step in range(self.n + 1):\n",
    "                state = (z_k_relative_tuple, theta2_in_k_step)\n",
    "                z_k_relative = np.array(z_k_relative_tuple)\n",
    "                # Value at the leaf node of k-step DP is estimated by rollout\n",
    "                self.J_values[self.k][state] = self._run_rollout(z_k_relative, theta2_in_k_step, self.T_global - self.k)\n",
    "\n",
    "\n",
    "        for t_k_relative in range(self.k - 1, -1, -1):\n",
    "            J_t = {}\n",
    "            policy_t = {}\n",
    "            z_vectors_t = self._generate_z_vectors(t_k_relative, self.n)\n",
    "\n",
    "            for z_t_relative_tuple in z_vectors_t:\n",
    "                for theta2_in_k_step in range(self.n + 1):\n",
    "                    current_state_in_k_step = (z_t_relative_tuple, theta2_in_k_step)\n",
    "                    action_values = []\n",
    "                    z_t_relative = np.array(z_t_relative_tuple)\n",
    "\n",
    "                    current_belief_in_k_step = self._calculate_posterior_at_step_in_k_lookahead(z_t_relative, theta2_in_k_step)\n",
    "\n",
    "                    for action_k in range(self.n):\n",
    "                        next_z_relative_tuple = tuple(z_t_relative + np.eye(self.n, dtype=int)[action_k])\n",
    "\n",
    "                        if theta2_in_k_step == 0: # O2 still hidden in this k-step segment\n",
    "                            # Current belief is joint (n,n)\n",
    "                            p_marginal_o1_at_a = np.sum(current_belief_in_k_step[action_k, :])\n",
    "                            p_success = (1 - self.gamma1[action_k]) * p_marginal_o1_at_a\n",
    "\n",
    "                            # Prob of finding O2 only\n",
    "                            p_find_o2_only = (1 - self.gamma2[action_k]) * (self.gamma1[action_k] * current_belief_in_k_step[action_k, action_k] + np.sum(current_belief_in_k_step[np.arange(self.n) != action_k, action_k]))\n",
    "\n",
    "                            p_nothing = 1 - p_success - p_find_o2_only\n",
    "\n",
    "                            val_if_nothing = self.J_values[t_k_relative + 1][(next_z_relative_tuple, 0)]\n",
    "                            val_if_found_o2 = self.J_values[t_k_relative + 1][(next_z_relative_tuple, action_k + 1)]\n",
    "\n",
    "                            expected_value = (p_success * 1.0) - self.c[action_k] + \\\n",
    "                                             (p_nothing * val_if_nothing) + \\\n",
    "                                             (p_find_o2_only * val_if_found_o2)\n",
    "                            action_values.append(expected_value)\n",
    "\n",
    "                        else: # O2 already found (theta2_in_k_step > 0)\n",
    "                            # Current belief is marginal for O1 (n,)\n",
    "                            p_o1_at_a = current_belief_in_k_step[action_k]\n",
    "                            p_success = (1 - self.gamma1[action_k]) * p_o1_at_a\n",
    "                            p_fail = 1 - p_success\n",
    "\n",
    "                            val_if_fail = self.J_values[t_k_relative + 1][(next_z_relative_tuple, theta2_in_k_step)]\n",
    "\n",
    "                            expected_value = (p_success * 1.0) - self.c[action_k] + (p_fail * val_if_fail)\n",
    "                            action_values.append(expected_value)\n",
    "\n",
    "                    best_value = np.max(action_values)\n",
    "                    best_action = np.argmax(action_values)\n",
    "                    J_t[current_state_in_k_step] = best_value\n",
    "                    policy_t[current_state_in_k_step] = best_action\n",
    "\n",
    "                self.J_values[t_k_relative] = J_t\n",
    "                self.Policy[t_k_relative] = policy_t\n",
    "\n",
    "    def solve_with_theta(self): # For theta2_at_init > 0\n",
    "        \"\"\"\n",
    "        Executes DP for k steps from self.initial_episode_belief, for theta2_at_init > 0.\n",
    "        In this case, O2 is already found, so theta2 is fixed throughout the k steps.\n",
    "        \"\"\"\n",
    "        fixed_theta2_in_k_step = self.theta2_at_init\n",
    "\n",
    "        self.J_values[self.k] = {}\n",
    "        z_vectors_k = self._generate_z_vectors(self.k, self.n)\n",
    "        for z_k_relative_tuple in z_vectors_k:\n",
    "            z_k_relative = np.array(z_k_relative_tuple)\n",
    "            state = (z_k_relative_tuple, fixed_theta2_in_k_step)\n",
    "            # Value at the leaf node of k-step DP is estimated by rollout\n",
    "            self.J_values[self.k][state] = self._run_rollout(z_k_relative, fixed_theta2_in_k_step, self.T_global - self.k)\n",
    "\n",
    "        for t_k_relative in range(self.k - 1, -1, -1):\n",
    "            J_t = {}\n",
    "            policy_t = {}\n",
    "            z_vectors_t = self._generate_z_vectors(t_k_relative, self.n)\n",
    "\n",
    "            for z_t_relative_tuple in z_vectors_t:\n",
    "                z_t_relative = np.array(z_t_relative_tuple)\n",
    "                current_state_in_k_step = (z_t_relative_tuple, fixed_theta2_in_k_step)\n",
    "                action_values = []\n",
    "\n",
    "                current_belief_in_k_step = self._calculate_posterior_at_step_in_k_lookahead(z_t_relative, fixed_theta2_in_k_step)\n",
    "\n",
    "                for action_k in range(self.n):\n",
    "                    next_z_relative_tuple = tuple(z_t_relative + np.eye(self.n, dtype=int)[action_k])\n",
    "\n",
    "                    p_o1_at_a = current_belief_in_k_step[action_k]\n",
    "                    p_success = (1 - self.gamma1[action_k]) * p_o1_at_a\n",
    "                    p_fail = 1 - p_success\n",
    "\n",
    "                    val_if_fail = self.J_values[t_k_relative + 1][(next_z_relative_tuple, fixed_theta2_in_k_step)]\n",
    "\n",
    "                    expected_value = (p_success * 1.0) - self.c[action_k] + (p_fail * val_if_fail)\n",
    "                    action_values.append(expected_value)\n",
    "\n",
    "                best_value = np.max(action_values)\n",
    "                best_action = np.argmax(action_values)\n",
    "                J_t[current_state_in_k_step] = best_value\n",
    "                policy_t[current_state_in_k_step] = best_action\n",
    "\n",
    "            self.J_values[t_k_relative] = J_t\n",
    "            self.Policy[t_k_relative] = policy_t\n",
    "\n",
    "\n",
    "# Helper function definitions (assuming they are available or defined above this class)\n",
    "# Need to ensure calculate_joint_posterior and calculate_conditional_posterior are accessible.\n",
    "# Let's define them here for clarity, or assume they are in a common helpers file.\n",
    "\n",
    "# Re-including helper functions for self-containment\n",
    "def calculate_joint_posterior(z_vector: np.ndarray, initial_prior_joint: np.ndarray,\n",
    "                              gamma1: np.ndarray, gamma2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculates the joint posterior belief p(O1=i, O2=j | z) based on initial joint prior.\"\"\"\n",
    "    g1_z = np.power(gamma1, z_vector)\n",
    "    g2_z = np.power(gamma2, z_vector)\n",
    "    likelihood = np.outer(g1_z, g2_z)\n",
    "    numerator = initial_prior_joint * likelihood\n",
    "    norm = np.sum(numerator)\n",
    "    return numerator / norm if norm > 0 else numerator\n",
    "\n",
    "def calculate_conditional_posterior(z_vector: np.ndarray, o2_loc: int,\n",
    "                                    precomputed_conditionals: List[np.ndarray], gamma1: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculates the conditional posterior belief p(O1=i | z, O2=k) based on precomputed conditionals and current z_vector.\"\"\"\n",
    "    p0_conditional = precomputed_conditionals[o2_loc]\n",
    "    g1_z = np.power(gamma1, z_vector)\n",
    "    numerator = g1_z * p0_conditional\n",
    "    norm = np.sum(numerator)\n",
    "    return numerator / norm if norm > 0 else numerator\n",
    "\n",
    "def precompute_p0_conditionals(n: int, p0_joint: np.ndarray) -> list:\n",
    "    \"\"\"Pre-calculates P(O1=i | O2=k) for all k from a joint prior matrix.\"\"\"\n",
    "    if p0_joint.ndim != 2: # Should be (n,n) for joint prior\n",
    "        raise ValueError(\"p0 must be a 2D joint probability matrix for precomputing conditionals.\")\n",
    "    conditionals = []\n",
    "    for k in range(n):\n",
    "        marginal_o2 = np.sum(p0_joint[:, k])\n",
    "        conditionals.append(p0_joint[:, k] / marginal_o2 if marginal_o2 > 0 else np.zeros(n))\n",
    "    return conditionals\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Adaptive Re-planning Episode Simulation\n",
    "# =============================================================================\n",
    "\n",
    "def run_k_step_episode(n: int, T: int, p0_initial_joint: np.ndarray,\n",
    "                                    gamma1: np.ndarray, gamma2: np.ndarray, c: np.ndarray, k: int, ROLLOUT_SIMULATIONS: int, episode_seed: int) -> Tuple[bool, int, float, int]:\n",
    "    \"\"\"\n",
    "    Simulates a single episode using k-step lookahead with greedy rollout and then full DP.\n",
    "    Returns success status, time of detection (or -1 if failed), accumulated reward, and episode length.\n",
    "    \"\"\"\n",
    "    # 1. Secretly determine the true locations for the entire episode\n",
    "    flat_p0 = p0_initial_joint.flatten()\n",
    "    if np.sum(flat_p0) == 0: # Handle case where prior is all zeros\n",
    "         true_pos_o1, true_pos_o2 = -1, -1 # Indicate no target\n",
    "    else:\n",
    "        # Use np.random directly as its state is set by np.random.seed(episode_seed)\n",
    "        choice_index = np.random.choice(n * n, p=flat_p0 / np.sum(flat_p0)) # Normalize\n",
    "        true_pos_o1, true_pos_o2 = np.unravel_index(choice_index, (n, n))\n",
    "\n",
    "    # 2. Initialize global state and reward variables\n",
    "    z_vector_global = np.zeros(n, dtype=int) # Accumulated search history for the entire episode\n",
    "    theta2_global = 0 # O2 state: 0=hidden, >0=found at cell (theta2_global-1)\n",
    "    current_belief_global = np.copy(p0_initial_joint) # Evolving belief state (joint or marginal)\n",
    "\n",
    "    # Precompute global conditionals from the initial joint prior once\n",
    "    global_p0_conditionals = precompute_p0_conditionals(n, p0_initial_joint)\n",
    "\n",
    "    accumulated_reward = 0.0\n",
    "    detection_time = -1\n",
    "    episode_length = 0\n",
    "\n",
    "    # Phase 1: K-step lookahead for (T-k) time steps\n",
    "    for t_global in range(T - k): # t_global is the current time step in the entire episode\n",
    "        episode_length += 1\n",
    "\n",
    "        # Create a unique, reproducible seed for this KStepLookaheadSolver instance\n",
    "        solver_instance_seed = episode_seed * T + t_global\n",
    "\n",
    "        # Instantiate K-step lookahead solver with the current global state (belief, z_vector, theta2)\n",
    "        # The KStepLookaheadSolver will calculate policies for `k` steps starting from a relative z=0.\n",
    "        solver_instance = KStepLookaheadSolver(n=n, T=T-t_global,\n",
    "                                                initial_episode_belief=current_belief_global,\n",
    "                                                gamma1=gamma1, gamma2=gamma2,\n",
    "                                                theta2_at_init=theta2_global,\n",
    "                                                k=k, c=c, rollout_sims=ROLLOUT_SIMULATIONS,\n",
    "                                                solver_seed=solver_instance_seed)\n",
    "\n",
    "        if theta2_global == 0: # If O2 is currently hidden, use the general solve method\n",
    "            solver_instance.solve()\n",
    "        else: # If O2 is currently found, use the solve method specialized for a fixed theta2\n",
    "            solver_instance.solve_with_theta()\n",
    "\n",
    "        # Get the optimal action for the current global step (t_global) from the k-step solver's policy\n",
    "        # The k-step solver's policy is for its *relative* time t=0 and *relative* z=0.\n",
    "        action = solver_instance.Policy[0][(tuple([0]*n), theta2_global)]\n",
    "\n",
    "        # Deduct cost for the action\n",
    "        accumulated_reward -= c[action]\n",
    "\n",
    "        # Simulate outcome of the action\n",
    "        found_o1 = (action == true_pos_o1) and (np.random.random() > gamma1[action])\n",
    "        if found_o1:\n",
    "            accumulated_reward += 1.0 # Reward for finding O1\n",
    "            detection_time = t_global + 1 # Absolute time of detection\n",
    "            return True, detection_time, accumulated_reward, episode_length # Mission Success!\n",
    "\n",
    "        # Check if O2 is found *at this current global step* (only if it was previously hidden)\n",
    "        if theta2_global == 0 and action == true_pos_o2 and (np.random.random() > gamma2[action]):\n",
    "            theta2_global = action + 1 # O2 found at cell `action`, update state\n",
    "\n",
    "        # Update global z_vector based on the action taken\n",
    "        z_vector_global[action] += 1\n",
    "\n",
    "        # Update global belief state based on the action and observed outcome\n",
    "        if theta2_global == 0: # If O2 is still hidden globally\n",
    "            current_belief_global = calculate_joint_posterior(z_vector_global, p0_initial_joint, gamma1, gamma2)\n",
    "        else: # If O2 has been found globally\n",
    "            current_belief_global = calculate_conditional_posterior(z_vector_global, theta2_global - 1, global_p0_conditionals, gamma1)\n",
    "\n",
    "    # Phase 2: After T-k steps, switch to full DP for the remaining k steps\n",
    "    # The remaining time horizon for this DP problem is `k`\n",
    "    # The DP solvers operate on their own relative z_vector, taking `current_belief_global` as their starting prior.\n",
    "    final_z_for_dp = np.zeros(n, dtype=int) # z_vector for the DP solver starts from 0 relative to its own horizon\n",
    "    final_theta2_for_dp = theta2_global # O2 state is carried over to the DP phase\n",
    "    final_policy = None\n",
    "\n",
    "    if final_theta2_for_dp == 0: # O2 is still hidden, use TwoObjectDPSolver\n",
    "        dp_solver = TwoObjectDPSolver(n=n, T=k, p0=current_belief_global, gamma1=gamma1, gamma2=gamma2, c=c)\n",
    "        dp_solver.solve()\n",
    "        final_policy = dp_solver.Policy\n",
    "    else: # O2 has been found, use SingleObjectDPSolver\n",
    "        dp_solver = SingleObjectDPSolver(n=n, T=k, p0_marginal=current_belief_global, gamma1=gamma1, c=c)\n",
    "        dp_solver.solve()\n",
    "        final_policy = dp_solver.Policy\n",
    "\n",
    "    for t_dp in range(k): # Iterate for the remaining `k` time steps using the final DP policy\n",
    "        episode_length += 1\n",
    "\n",
    "        # Determine the state key for the DP policy based on theta2\n",
    "        state_for_dp = (tuple(final_z_for_dp), final_theta2_for_dp) if final_theta2_for_dp == 0 else tuple(final_z_for_dp)\n",
    "\n",
    "        if t_dp not in final_policy or state_for_dp not in final_policy[t_dp]:\n",
    "            # This case should ideally not happen if the DP policy is complete for the horizon `k`\n",
    "            # print(f\"Warning: DP Policy not found for state {state_for_dp} at time {t_dp}. Ending episode.\")\n",
    "            return False, detection_time, accumulated_reward, episode_length\n",
    "\n",
    "        action = final_policy[t_dp][state_for_dp]\n",
    "        accumulated_reward -= c[action]\n",
    "\n",
    "        # Simulate outcome of the action in the DP phase\n",
    "        if action == true_pos_o1 and (np.random.random() > gamma1[action]):\n",
    "            accumulated_reward += 1.0 # Reward for finding O1\n",
    "            detection_time = (T - k) + (t_dp + 1) # Absolute time of detection\n",
    "            return True, detection_time, accumulated_reward, episode_length # Mission Success!\n",
    "\n",
    "        # Check for O2 discovery during DP phase, if O2 was still hidden\n",
    "        if final_theta2_for_dp == 0 and action == true_pos_o2 and (np.random.random() > gamma2[action]):\n",
    "            # If O2 is found here, the state `final_theta2_for_dp` is updated, but the DP policy\n",
    "            # was computed assuming `final_theta2_for_dp` was `0` for the entire `k` steps.\n",
    "            # A more advanced adaptive approach would re-plan here with a new SingleObjectDPSolver.\n",
    "            # For this simplified model, we just update the state and continue with the same policy.\n",
    "            final_theta2_for_dp = action + 1\n",
    "\n",
    "        # Update z_vector for the DP solver's internal state\n",
    "        final_z_for_dp[action] += 1\n",
    "\n",
    "    return False, detection_time, accumulated_reward, episode_length # Mission Failed\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. MAIN EXPERIMENT SCRIPT (Modified for multiple seeds)\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Problem Definition ---\n",
    "    NUM_CELLS = 5\n",
    "    TIME_HORIZON = 10\n",
    "    LOOKAHEAD_K = 1 # Set your desired lookahead depth here. If LOOKAHEAD_K == T, it's pure KStepLookahead.\n",
    "                      # If LOOKAHEAD_K == 0, it's pure TwoObjectDPSolver (adaptive replanning).\n",
    "    NUM_EPISODES_PER_SEED = 100\n",
    "    ROLLOUT_SIMULATIONS = 100 # Number of simulations for rollout value estimation\n",
    "    NUM_SEEDS = 100 # Number of different seeds to run\n",
    "\n",
    "    # Problem parameters (same for all seeds)\n",
    "    prior = np.array([\n",
    "        [0.152,  0.0039, 0.003,  0.0108, 0.011],\n",
    "        [0.0038, 0.0052, 0.117,  0.0162, 0.165],\n",
    "        [0.0057, 0.195,  0.015,  0.009,  0.011],\n",
    "        [0.0038, 0.0091, 0.0075, 0.027,  0.011],\n",
    "        [0.0247, 0.0468, 0.0075, 0.117,  0.022]\n",
    "    ])\n",
    "\n",
    "    gammas1 = np.array([0.8, 0.65, 0.82, 0.75, 0.7])\n",
    "    gammas2 = np.array([0.2, 0.1, 0.25, 0.15, 0.2])\n",
    "    c= np.array([0.15,0.2,0.25,0.1,0.2])\n",
    "\n",
    "    all_success_rates = []\n",
    "    all_detection_times = []\n",
    "    all_rewards = []\n",
    "    all_episode_lengths = []\n",
    "    total_experiment_start_time = time.time()\n",
    "\n",
    "    print(\"--- Running Empirical Experiment for k-step lookahead Policy over Multiple Seeds ---\")\n",
    "    print(f\"Number of seeds: {NUM_SEEDS}\")\n",
    "    print(f\"Number of evaluation episodes per seed: {NUM_EPISODES_PER_SEED}\")\n",
    "    print(f\"Lookahead depth (k): {LOOKAHEAD_K}\")\n",
    "    print(f\"Rollout simulations: {ROLLOUT_SIMULATIONS}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "\n",
    "    for seed in range(NUM_SEEDS):\n",
    "        print(f\"\\n--- Running with Seed: {seed} ---\")\n",
    "        np.random.seed(seed) # Set seed for main episode simulation\n",
    "\n",
    "        num_successes = 0\n",
    "        detection_times_this_seed = []\n",
    "        rewards_this_seed = []\n",
    "        episode_lengths_this_seed = []\n",
    "        start_time_this_seed = time.time()\n",
    "\n",
    "        for i in range(NUM_EPISODES_PER_SEED):\n",
    "            # print(f\"Episode : {i}\") # Uncomment for detailed per-episode output\n",
    "            success, detection_time, reward, episode_length = run_k_step_episode(NUM_CELLS, TIME_HORIZON, prior, gammas1, gammas2, c, LOOKAHEAD_K, ROLLOUT_SIMULATIONS, seed)\n",
    "            rewards_this_seed.append(reward)\n",
    "            episode_lengths_this_seed.append(episode_length)\n",
    "\n",
    "            if success:\n",
    "                num_successes += 1\n",
    "                detection_times_this_seed.append(detection_time)\n",
    "                # print(f\"    (Episode {i+1} Result) SUCCESS at time {detection_time} with reward {reward:.4f}\")\n",
    "            # else:\n",
    "                # print(f\"    (Episode {i+1} Result) FAILURE with reward {reward:.4f}\")\n",
    "\n",
    "\n",
    "        end_time_this_seed = time.time()\n",
    "        success_rate_this_seed = num_successes / NUM_EPISODES_PER_SEED\n",
    "        all_success_rates.append(success_rate_this_seed)\n",
    "        all_detection_times.extend(detection_times_this_seed)\n",
    "        all_rewards.extend(rewards_this_seed)\n",
    "        all_episode_lengths.extend(episode_lengths_this_seed)\n",
    "\n",
    "\n",
    "        print(f\"--- Seed {seed} Results ---\")\n",
    "        print(f\"Success Rate: {success_rate_this_seed:.4f} ({success_rate_this_seed*100:.2f}%)\")\n",
    "        if detection_times_this_seed:\n",
    "            print(f\"Average Detection Time (Successful Episodes): {np.mean(detection_times_this_seed):.2f}\")\n",
    "        else:\n",
    "            print(\"Average Detection Time (Successful Episodes): N/A (no successes)\")\n",
    "        print(f\"Average Episode Reward: {np.mean(rewards_this_seed):.4f}\")\n",
    "        print(f\"Average Episode Length: {np.mean(episode_lengths_this_seed):.2f}\")\n",
    "        print(f\"Time taken for this seed: {end_time_this_seed - start_time_this_seed:.2f} seconds\")\n",
    "\n",
    "\n",
    "    total_experiment_end_time = time.time()\n",
    "\n",
    "    # --- 3.4. Display Final Results ---\n",
    "    mean_success_rate = np.mean(all_success_rates)\n",
    "    std_success_rate = np.std(all_success_rates)\n",
    "    mean_detection_time = np.mean(all_detection_times) if all_detection_times else -1\n",
    "    std_detection_time = np.std(all_detection_times) if all_detection_times else -1\n",
    "    mean_reward = np.mean(all_rewards)\n",
    "    std_reward = np.std(all_rewards)\n",
    "    mean_episode_length = np.mean(all_episode_lengths)\n",
    "    std_episode_length = np.std(all_episode_lengths)\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Overall {LOOKAHEAD_K}-step Lookahead Policy Results ---\")\n",
    "    print(f\"Total experiment time across {NUM_SEEDS} seeds: {total_experiment_end_time - total_experiment_start_time:.2f} seconds\")\n",
    "    print(f\"Average Success Rate over {NUM_SEEDS} seeds and {NUM_EPISODES_PER_SEED} episodes each: {mean_success_rate:.4f} ({mean_success_rate*100:.2f}%)\")\n",
    "    print(f\"Standard Deviation of Success Rate: {std_success_rate:.4f}\")\n",
    "    if all_detection_times:\n",
    "        print(f\"Average Detection Time (Successful Episodes) across all seeds: {mean_detection_time:.2f}\")\n",
    "        print(f\"Standard Deviation of Detection Time: {std_detection_time:.2f}\")\n",
    "    else:\n",
    "        print(\"Average Detection Time (Successful Episodes) across all seeds: N/A (no successes)\")\n",
    "\n",
    "    print(f\"Average Episode Reward across all seeds: {mean_reward:.4f}\")\n",
    "    print(f\"Standard Deviation of Reward: {std_reward:.4f}\")\n",
    "    print(f\"Average Episode Length across all seeds: {mean_episode_length:.2f}\")\n",
    "    print(f\"Standard Deviation of Episode Length: {std_episode_length:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
