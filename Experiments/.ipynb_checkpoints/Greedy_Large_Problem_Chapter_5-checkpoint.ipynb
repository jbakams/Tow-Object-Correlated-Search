{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txTdQaQVhkrA",
        "outputId": "72fa2deb-2df0-4987-cec7-0004f45eb910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Calculating Benchmark for Single-Object Search (Ignoring O2) ---\n",
            "Original Joint Prior Shape: (5, 5)\n",
            "Calculated Marginal Prior for O1 (p(O1)): [0.1807 0.3072 0.2357 0.0584 0.218 ]\n",
            "--------------------\n",
            "\n",
            "--- Running Empirical Experiment for Single-Object DP Policy over Multiple Seeds ---\n",
            "Number of seeds: 100\n",
            "Number of evaluation episodes per seed: 100\n",
            "--------------------\n",
            "\n",
            "--- Running with Seed: 0 ---\n",
            "--- Seed 0 Results ---\n",
            "Success Rate: 0.3600 (36.00%)\n",
            "Average Detection Time (Successful Episodes): 2.64\n",
            "Average Episode Reward: -0.6775\n",
            "Average Episode Length: 7.35\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 1 ---\n",
            "--- Seed 1 Results ---\n",
            "Success Rate: 0.2900 (29.00%)\n",
            "Average Detection Time (Successful Episodes): 2.97\n",
            "Average Episode Reward: -0.8325\n",
            "Average Episode Length: 7.96\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 2 ---\n",
            "--- Seed 2 Results ---\n",
            "Success Rate: 0.3600 (36.00%)\n",
            "Average Detection Time (Successful Episodes): 2.89\n",
            "Average Episode Reward: -0.6900\n",
            "Average Episode Length: 7.44\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 3 ---\n",
            "--- Seed 3 Results ---\n",
            "Success Rate: 0.3700 (37.00%)\n",
            "Average Detection Time (Successful Episodes): 2.38\n",
            "Average Episode Reward: -0.6500\n",
            "Average Episode Length: 7.18\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 4 ---\n",
            "--- Seed 4 Results ---\n",
            "Success Rate: 0.3200 (32.00%)\n",
            "Average Detection Time (Successful Episodes): 3.59\n",
            "Average Episode Reward: -0.7965\n",
            "Average Episode Length: 7.95\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 5 ---\n",
            "--- Seed 5 Results ---\n",
            "Success Rate: 0.3300 (33.00%)\n",
            "Average Detection Time (Successful Episodes): 3.36\n",
            "Average Episode Reward: -0.7635\n",
            "Average Episode Length: 7.81\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 6 ---\n",
            "--- Seed 6 Results ---\n",
            "Success Rate: 0.2700 (27.00%)\n",
            "Average Detection Time (Successful Episodes): 2.48\n",
            "Average Episode Reward: -0.8410\n",
            "Average Episode Length: 7.97\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 7 ---\n",
            "--- Seed 7 Results ---\n",
            "Success Rate: 0.3800 (38.00%)\n",
            "Average Detection Time (Successful Episodes): 2.63\n",
            "Average Episode Reward: -0.6365\n",
            "Average Episode Length: 7.20\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 8 ---\n",
            "--- Seed 8 Results ---\n",
            "Success Rate: 0.1800 (18.00%)\n",
            "Average Detection Time (Successful Episodes): 2.56\n",
            "Average Episode Reward: -1.0120\n",
            "Average Episode Length: 8.66\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 9 ---\n",
            "--- Seed 9 Results ---\n",
            "Success Rate: 0.3200 (32.00%)\n",
            "Average Detection Time (Successful Episodes): 2.66\n",
            "Average Episode Reward: -0.7515\n",
            "Average Episode Length: 7.65\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 10 ---\n",
            "--- Seed 10 Results ---\n",
            "Success Rate: 0.3800 (38.00%)\n",
            "Average Detection Time (Successful Episodes): 2.79\n",
            "Average Episode Reward: -0.6570\n",
            "Average Episode Length: 7.26\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 11 ---\n",
            "--- Seed 11 Results ---\n",
            "Success Rate: 0.2300 (23.00%)\n",
            "Average Detection Time (Successful Episodes): 2.96\n",
            "Average Episode Reward: -0.9250\n",
            "Average Episode Length: 8.38\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 12 ---\n",
            "--- Seed 12 Results ---\n",
            "Success Rate: 0.2900 (29.00%)\n",
            "Average Detection Time (Successful Episodes): 2.72\n",
            "Average Episode Reward: -0.8140\n",
            "Average Episode Length: 7.89\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 13 ---\n",
            "--- Seed 13 Results ---\n",
            "Success Rate: 0.3200 (32.00%)\n",
            "Average Detection Time (Successful Episodes): 2.66\n",
            "Average Episode Reward: -0.7535\n",
            "Average Episode Length: 7.65\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 14 ---\n",
            "--- Seed 14 Results ---\n",
            "Success Rate: 0.3700 (37.00%)\n",
            "Average Detection Time (Successful Episodes): 3.08\n",
            "Average Episode Reward: -0.6810\n",
            "Average Episode Length: 7.44\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 15 ---\n",
            "--- Seed 15 Results ---\n",
            "Success Rate: 0.3300 (33.00%)\n",
            "Average Detection Time (Successful Episodes): 2.91\n",
            "Average Episode Reward: -0.7515\n",
            "Average Episode Length: 7.66\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 16 ---\n",
            "--- Seed 16 Results ---\n",
            "Success Rate: 0.3800 (38.00%)\n",
            "Average Detection Time (Successful Episodes): 2.74\n",
            "Average Episode Reward: -0.6485\n",
            "Average Episode Length: 7.24\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 17 ---\n",
            "--- Seed 17 Results ---\n",
            "Success Rate: 0.3500 (35.00%)\n",
            "Average Detection Time (Successful Episodes): 2.77\n",
            "Average Episode Reward: -0.7000\n",
            "Average Episode Length: 7.47\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 18 ---\n",
            "--- Seed 18 Results ---\n",
            "Success Rate: 0.3700 (37.00%)\n",
            "Average Detection Time (Successful Episodes): 3.35\n",
            "Average Episode Reward: -0.6960\n",
            "Average Episode Length: 7.54\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 19 ---\n",
            "--- Seed 19 Results ---\n",
            "Success Rate: 0.3400 (34.00%)\n",
            "Average Detection Time (Successful Episodes): 2.94\n",
            "Average Episode Reward: -0.7350\n",
            "Average Episode Length: 7.60\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 20 ---\n",
            "--- Seed 20 Results ---\n",
            "Success Rate: 0.3000 (30.00%)\n",
            "Average Detection Time (Successful Episodes): 2.60\n",
            "Average Episode Reward: -0.7900\n",
            "Average Episode Length: 7.78\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 21 ---\n",
            "--- Seed 21 Results ---\n",
            "Success Rate: 0.3200 (32.00%)\n",
            "Average Detection Time (Successful Episodes): 2.38\n",
            "Average Episode Reward: -0.7355\n",
            "Average Episode Length: 7.56\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 22 ---\n",
            "--- Seed 22 Results ---\n",
            "Success Rate: 0.2600 (26.00%)\n",
            "Average Detection Time (Successful Episodes): 3.42\n",
            "Average Episode Reward: -0.8870\n",
            "Average Episode Length: 8.29\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 23 ---\n",
            "--- Seed 23 Results ---\n",
            "Success Rate: 0.3400 (34.00%)\n",
            "Average Detection Time (Successful Episodes): 2.32\n",
            "Average Episode Reward: -0.6960\n",
            "Average Episode Length: 7.39\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 24 ---\n",
            "--- Seed 24 Results ---\n",
            "Success Rate: 0.3500 (35.00%)\n",
            "Average Detection Time (Successful Episodes): 2.69\n",
            "Average Episode Reward: -0.6990\n",
            "Average Episode Length: 7.44\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 25 ---\n",
            "--- Seed 25 Results ---\n",
            "Success Rate: 0.3100 (31.00%)\n",
            "Average Detection Time (Successful Episodes): 3.10\n",
            "Average Episode Reward: -0.7900\n",
            "Average Episode Length: 7.86\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 26 ---\n",
            "--- Seed 26 Results ---\n",
            "Success Rate: 0.2800 (28.00%)\n",
            "Average Detection Time (Successful Episodes): 3.21\n",
            "Average Episode Reward: -0.8495\n",
            "Average Episode Length: 8.10\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 27 ---\n",
            "--- Seed 27 Results ---\n",
            "Success Rate: 0.2300 (23.00%)\n",
            "Average Detection Time (Successful Episodes): 2.43\n",
            "Average Episode Reward: -0.9180\n",
            "Average Episode Length: 8.26\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 28 ---\n",
            "--- Seed 28 Results ---\n",
            "Success Rate: 0.2900 (29.00%)\n",
            "Average Detection Time (Successful Episodes): 3.14\n",
            "Average Episode Reward: -0.8300\n",
            "Average Episode Length: 8.01\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 29 ---\n",
            "--- Seed 29 Results ---\n",
            "Success Rate: 0.2500 (25.00%)\n",
            "Average Detection Time (Successful Episodes): 2.92\n",
            "Average Episode Reward: -0.8915\n",
            "Average Episode Length: 8.23\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 30 ---\n",
            "--- Seed 30 Results ---\n",
            "Success Rate: 0.3700 (37.00%)\n",
            "Average Detection Time (Successful Episodes): 2.49\n",
            "Average Episode Reward: -0.6550\n",
            "Average Episode Length: 7.22\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 31 ---\n",
            "--- Seed 31 Results ---\n",
            "Success Rate: 0.4200 (42.00%)\n",
            "Average Detection Time (Successful Episodes): 2.62\n",
            "Average Episode Reward: -0.5685\n",
            "Average Episode Length: 6.90\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 32 ---\n",
            "--- Seed 32 Results ---\n",
            "Success Rate: 0.3600 (36.00%)\n",
            "Average Detection Time (Successful Episodes): 2.28\n",
            "Average Episode Reward: -0.6605\n",
            "Average Episode Length: 7.22\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 33 ---\n",
            "--- Seed 33 Results ---\n",
            "Success Rate: 0.3400 (34.00%)\n",
            "Average Detection Time (Successful Episodes): 3.26\n",
            "Average Episode Reward: -0.7435\n",
            "Average Episode Length: 7.71\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 34 ---\n",
            "--- Seed 34 Results ---\n",
            "Success Rate: 0.3800 (38.00%)\n",
            "Average Detection Time (Successful Episodes): 2.61\n",
            "Average Episode Reward: -0.6410\n",
            "Average Episode Length: 7.19\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 35 ---\n",
            "--- Seed 35 Results ---\n",
            "Success Rate: 0.3900 (39.00%)\n",
            "Average Detection Time (Successful Episodes): 2.36\n",
            "Average Episode Reward: -0.6050\n",
            "Average Episode Length: 7.02\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 36 ---\n",
            "--- Seed 36 Results ---\n",
            "Success Rate: 0.3800 (38.00%)\n",
            "Average Detection Time (Successful Episodes): 2.66\n",
            "Average Episode Reward: -0.6365\n",
            "Average Episode Length: 7.21\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 37 ---\n",
            "--- Seed 37 Results ---\n",
            "Success Rate: 0.2800 (28.00%)\n",
            "Average Detection Time (Successful Episodes): 2.96\n",
            "Average Episode Reward: -0.8405\n",
            "Average Episode Length: 8.03\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 38 ---\n",
            "--- Seed 38 Results ---\n",
            "Success Rate: 0.3100 (31.00%)\n",
            "Average Detection Time (Successful Episodes): 2.26\n",
            "Average Episode Reward: -0.7525\n",
            "Average Episode Length: 7.60\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 39 ---\n",
            "--- Seed 39 Results ---\n",
            "Success Rate: 0.3300 (33.00%)\n",
            "Average Detection Time (Successful Episodes): 2.85\n",
            "Average Episode Reward: -0.7395\n",
            "Average Episode Length: 7.64\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 40 ---\n",
            "--- Seed 40 Results ---\n",
            "Success Rate: 0.3200 (32.00%)\n",
            "Average Detection Time (Successful Episodes): 2.81\n",
            "Average Episode Reward: -0.7575\n",
            "Average Episode Length: 7.70\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 41 ---\n",
            "--- Seed 41 Results ---\n",
            "Success Rate: 0.4000 (40.00%)\n",
            "Average Detection Time (Successful Episodes): 2.35\n",
            "Average Episode Reward: -0.5820\n",
            "Average Episode Length: 6.94\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 42 ---\n",
            "--- Seed 42 Results ---\n",
            "Success Rate: 0.2900 (29.00%)\n",
            "Average Detection Time (Successful Episodes): 2.62\n",
            "Average Episode Reward: -0.8135\n",
            "Average Episode Length: 7.86\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 43 ---\n",
            "--- Seed 43 Results ---\n",
            "Success Rate: 0.3200 (32.00%)\n",
            "Average Detection Time (Successful Episodes): 2.78\n",
            "Average Episode Reward: -0.7585\n",
            "Average Episode Length: 7.69\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 44 ---\n",
            "--- Seed 44 Results ---\n",
            "Success Rate: 0.3100 (31.00%)\n",
            "Average Detection Time (Successful Episodes): 2.55\n",
            "Average Episode Reward: -0.7680\n",
            "Average Episode Length: 7.69\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 45 ---\n",
            "--- Seed 45 Results ---\n",
            "Success Rate: 0.2600 (26.00%)\n",
            "Average Detection Time (Successful Episodes): 3.23\n",
            "Average Episode Reward: -0.8855\n",
            "Average Episode Length: 8.24\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 46 ---\n",
            "--- Seed 46 Results ---\n",
            "Success Rate: 0.2900 (29.00%)\n",
            "Average Detection Time (Successful Episodes): 2.10\n",
            "Average Episode Reward: -0.7870\n",
            "Average Episode Length: 7.71\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 47 ---\n",
            "--- Seed 47 Results ---\n",
            "Success Rate: 0.3500 (35.00%)\n",
            "Average Detection Time (Successful Episodes): 2.51\n",
            "Average Episode Reward: -0.6920\n",
            "Average Episode Length: 7.38\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 48 ---\n",
            "--- Seed 48 Results ---\n",
            "Success Rate: 0.3200 (32.00%)\n",
            "Average Detection Time (Successful Episodes): 2.38\n",
            "Average Episode Reward: -0.7435\n",
            "Average Episode Length: 7.56\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 49 ---\n",
            "--- Seed 49 Results ---\n",
            "Success Rate: 0.3400 (34.00%)\n",
            "Average Detection Time (Successful Episodes): 2.82\n",
            "Average Episode Reward: -0.7290\n",
            "Average Episode Length: 7.56\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 50 ---\n",
            "--- Seed 50 Results ---\n",
            "Success Rate: 0.3200 (32.00%)\n",
            "Average Detection Time (Successful Episodes): 2.56\n",
            "Average Episode Reward: -0.7490\n",
            "Average Episode Length: 7.62\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 51 ---\n",
            "--- Seed 51 Results ---\n",
            "Success Rate: 0.3800 (38.00%)\n",
            "Average Detection Time (Successful Episodes): 2.58\n",
            "Average Episode Reward: -0.6395\n",
            "Average Episode Length: 7.18\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 52 ---\n",
            "--- Seed 52 Results ---\n",
            "Success Rate: 0.3900 (39.00%)\n",
            "Average Detection Time (Successful Episodes): 2.62\n",
            "Average Episode Reward: -0.6220\n",
            "Average Episode Length: 7.12\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 53 ---\n",
            "--- Seed 53 Results ---\n",
            "Success Rate: 0.3500 (35.00%)\n",
            "Average Detection Time (Successful Episodes): 2.97\n",
            "Average Episode Reward: -0.7160\n",
            "Average Episode Length: 7.54\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 54 ---\n",
            "--- Seed 54 Results ---\n",
            "Success Rate: 0.3100 (31.00%)\n",
            "Average Detection Time (Successful Episodes): 3.48\n",
            "Average Episode Reward: -0.8095\n",
            "Average Episode Length: 7.98\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 55 ---\n",
            "--- Seed 55 Results ---\n",
            "Success Rate: 0.3700 (37.00%)\n",
            "Average Detection Time (Successful Episodes): 2.89\n",
            "Average Episode Reward: -0.6805\n",
            "Average Episode Length: 7.37\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 56 ---\n",
            "--- Seed 56 Results ---\n",
            "Success Rate: 0.2600 (26.00%)\n",
            "Average Detection Time (Successful Episodes): 2.23\n",
            "Average Episode Reward: -0.8505\n",
            "Average Episode Length: 7.98\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 57 ---\n",
            "--- Seed 57 Results ---\n",
            "Success Rate: 0.2600 (26.00%)\n",
            "Average Detection Time (Successful Episodes): 2.73\n",
            "Average Episode Reward: -0.8720\n",
            "Average Episode Length: 8.11\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 58 ---\n",
            "--- Seed 58 Results ---\n",
            "Success Rate: 0.3300 (33.00%)\n",
            "Average Detection Time (Successful Episodes): 3.12\n",
            "Average Episode Reward: -0.7560\n",
            "Average Episode Length: 7.73\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 59 ---\n",
            "--- Seed 59 Results ---\n",
            "Success Rate: 0.2500 (25.00%)\n",
            "Average Detection Time (Successful Episodes): 1.72\n",
            "Average Episode Reward: -0.8485\n",
            "Average Episode Length: 7.93\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 60 ---\n",
            "--- Seed 60 Results ---\n",
            "Success Rate: 0.3500 (35.00%)\n",
            "Average Detection Time (Successful Episodes): 3.00\n",
            "Average Episode Reward: -0.7150\n",
            "Average Episode Length: 7.55\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 61 ---\n",
            "--- Seed 61 Results ---\n",
            "Success Rate: 0.2800 (28.00%)\n",
            "Average Detection Time (Successful Episodes): 2.64\n",
            "Average Episode Reward: -0.8305\n",
            "Average Episode Length: 7.94\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 62 ---\n",
            "--- Seed 62 Results ---\n",
            "Success Rate: 0.3700 (37.00%)\n",
            "Average Detection Time (Successful Episodes): 2.78\n",
            "Average Episode Reward: -0.6675\n",
            "Average Episode Length: 7.33\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 63 ---\n",
            "--- Seed 63 Results ---\n",
            "Success Rate: 0.3400 (34.00%)\n",
            "Average Detection Time (Successful Episodes): 2.53\n",
            "Average Episode Reward: -0.7120\n",
            "Average Episode Length: 7.46\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 64 ---\n",
            "--- Seed 64 Results ---\n",
            "Success Rate: 0.4300 (43.00%)\n",
            "Average Detection Time (Successful Episodes): 3.60\n",
            "Average Episode Reward: -0.6005\n",
            "Average Episode Length: 7.25\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 65 ---\n",
            "--- Seed 65 Results ---\n",
            "Success Rate: 0.3600 (36.00%)\n",
            "Average Detection Time (Successful Episodes): 2.72\n",
            "Average Episode Reward: -0.6890\n",
            "Average Episode Length: 7.38\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 66 ---\n",
            "--- Seed 66 Results ---\n",
            "Success Rate: 0.3500 (35.00%)\n",
            "Average Detection Time (Successful Episodes): 2.23\n",
            "Average Episode Reward: -0.6740\n",
            "Average Episode Length: 7.28\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 67 ---\n",
            "--- Seed 67 Results ---\n",
            "Success Rate: 0.3100 (31.00%)\n",
            "Average Detection Time (Successful Episodes): 2.87\n",
            "Average Episode Reward: -0.7875\n",
            "Average Episode Length: 7.79\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 68 ---\n",
            "--- Seed 68 Results ---\n",
            "Success Rate: 0.3200 (32.00%)\n",
            "Average Detection Time (Successful Episodes): 2.44\n",
            "Average Episode Reward: -0.7450\n",
            "Average Episode Length: 7.58\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 69 ---\n",
            "--- Seed 69 Results ---\n",
            "Success Rate: 0.3400 (34.00%)\n",
            "Average Detection Time (Successful Episodes): 3.06\n",
            "Average Episode Reward: -0.7325\n",
            "Average Episode Length: 7.64\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 70 ---\n",
            "--- Seed 70 Results ---\n",
            "Success Rate: 0.4100 (41.00%)\n",
            "Average Detection Time (Successful Episodes): 2.83\n",
            "Average Episode Reward: -0.5950\n",
            "Average Episode Length: 7.06\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 71 ---\n",
            "--- Seed 71 Results ---\n",
            "Success Rate: 0.3900 (39.00%)\n",
            "Average Detection Time (Successful Episodes): 2.69\n",
            "Average Episode Reward: -0.6270\n",
            "Average Episode Length: 7.15\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 72 ---\n",
            "--- Seed 72 Results ---\n",
            "Success Rate: 0.3400 (34.00%)\n",
            "Average Detection Time (Successful Episodes): 2.94\n",
            "Average Episode Reward: -0.7290\n",
            "Average Episode Length: 7.60\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 73 ---\n",
            "--- Seed 73 Results ---\n",
            "Success Rate: 0.4000 (40.00%)\n",
            "Average Detection Time (Successful Episodes): 2.42\n",
            "Average Episode Reward: -0.5925\n",
            "Average Episode Length: 6.97\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 74 ---\n",
            "--- Seed 74 Results ---\n",
            "Success Rate: 0.3500 (35.00%)\n",
            "Average Detection Time (Successful Episodes): 2.40\n",
            "Average Episode Reward: -0.6860\n",
            "Average Episode Length: 7.34\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 75 ---\n",
            "--- Seed 75 Results ---\n",
            "Success Rate: 0.3000 (30.00%)\n",
            "Average Detection Time (Successful Episodes): 3.27\n",
            "Average Episode Reward: -0.8140\n",
            "Average Episode Length: 7.98\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 76 ---\n",
            "--- Seed 76 Results ---\n",
            "Success Rate: 0.3200 (32.00%)\n",
            "Average Detection Time (Successful Episodes): 2.62\n",
            "Average Episode Reward: -0.7495\n",
            "Average Episode Length: 7.64\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 77 ---\n",
            "--- Seed 77 Results ---\n",
            "Success Rate: 0.3000 (30.00%)\n",
            "Average Detection Time (Successful Episodes): 2.80\n",
            "Average Episode Reward: -0.7940\n",
            "Average Episode Length: 7.84\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 78 ---\n",
            "--- Seed 78 Results ---\n",
            "Success Rate: 0.3700 (37.00%)\n",
            "Average Detection Time (Successful Episodes): 3.86\n",
            "Average Episode Reward: -0.7190\n",
            "Average Episode Length: 7.73\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 79 ---\n",
            "--- Seed 79 Results ---\n",
            "Success Rate: 0.3600 (36.00%)\n",
            "Average Detection Time (Successful Episodes): 2.56\n",
            "Average Episode Reward: -0.6730\n",
            "Average Episode Length: 7.32\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 80 ---\n",
            "--- Seed 80 Results ---\n",
            "Success Rate: 0.3200 (32.00%)\n",
            "Average Detection Time (Successful Episodes): 2.97\n",
            "Average Episode Reward: -0.7685\n",
            "Average Episode Length: 7.75\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 81 ---\n",
            "--- Seed 81 Results ---\n",
            "Success Rate: 0.4100 (41.00%)\n",
            "Average Detection Time (Successful Episodes): 2.66\n",
            "Average Episode Reward: -0.5870\n",
            "Average Episode Length: 6.99\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 82 ---\n",
            "--- Seed 82 Results ---\n",
            "Success Rate: 0.3300 (33.00%)\n",
            "Average Detection Time (Successful Episodes): 2.76\n",
            "Average Episode Reward: -0.7400\n",
            "Average Episode Length: 7.61\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 83 ---\n",
            "--- Seed 83 Results ---\n",
            "Success Rate: 0.3400 (34.00%)\n",
            "Average Detection Time (Successful Episodes): 3.35\n",
            "Average Episode Reward: -0.7495\n",
            "Average Episode Length: 7.74\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 84 ---\n",
            "--- Seed 84 Results ---\n",
            "Success Rate: 0.2500 (25.00%)\n",
            "Average Detection Time (Successful Episodes): 2.76\n",
            "Average Episode Reward: -0.8905\n",
            "Average Episode Length: 8.19\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 85 ---\n",
            "--- Seed 85 Results ---\n",
            "Success Rate: 0.3900 (39.00%)\n",
            "Average Detection Time (Successful Episodes): 2.26\n",
            "Average Episode Reward: -0.6020\n",
            "Average Episode Length: 6.98\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 86 ---\n",
            "--- Seed 86 Results ---\n",
            "Success Rate: 0.2900 (29.00%)\n",
            "Average Detection Time (Successful Episodes): 2.69\n",
            "Average Episode Reward: -0.8095\n",
            "Average Episode Length: 7.88\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 87 ---\n",
            "--- Seed 87 Results ---\n",
            "Success Rate: 0.4000 (40.00%)\n",
            "Average Detection Time (Successful Episodes): 2.35\n",
            "Average Episode Reward: -0.5900\n",
            "Average Episode Length: 6.94\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 88 ---\n",
            "--- Seed 88 Results ---\n",
            "Success Rate: 0.3600 (36.00%)\n",
            "Average Detection Time (Successful Episodes): 2.72\n",
            "Average Episode Reward: -0.6825\n",
            "Average Episode Length: 7.38\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 89 ---\n",
            "--- Seed 89 Results ---\n",
            "Success Rate: 0.3000 (30.00%)\n",
            "Average Detection Time (Successful Episodes): 2.73\n",
            "Average Episode Reward: -0.7975\n",
            "Average Episode Length: 7.82\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 90 ---\n",
            "--- Seed 90 Results ---\n",
            "Success Rate: 0.3000 (30.00%)\n",
            "Average Detection Time (Successful Episodes): 3.33\n",
            "Average Episode Reward: -0.8165\n",
            "Average Episode Length: 8.00\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 91 ---\n",
            "--- Seed 91 Results ---\n",
            "Success Rate: 0.3300 (33.00%)\n",
            "Average Detection Time (Successful Episodes): 3.27\n",
            "Average Episode Reward: -0.7590\n",
            "Average Episode Length: 7.78\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 92 ---\n",
            "--- Seed 92 Results ---\n",
            "Success Rate: 0.3800 (38.00%)\n",
            "Average Detection Time (Successful Episodes): 2.89\n",
            "Average Episode Reward: -0.6600\n",
            "Average Episode Length: 7.30\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 93 ---\n",
            "--- Seed 93 Results ---\n",
            "Success Rate: 0.3500 (35.00%)\n",
            "Average Detection Time (Successful Episodes): 2.86\n",
            "Average Episode Reward: -0.7065\n",
            "Average Episode Length: 7.50\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 94 ---\n",
            "--- Seed 94 Results ---\n",
            "Success Rate: 0.3700 (37.00%)\n",
            "Average Detection Time (Successful Episodes): 2.38\n",
            "Average Episode Reward: -0.6480\n",
            "Average Episode Length: 7.18\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 95 ---\n",
            "--- Seed 95 Results ---\n",
            "Success Rate: 0.3400 (34.00%)\n",
            "Average Detection Time (Successful Episodes): 2.76\n",
            "Average Episode Reward: -0.7205\n",
            "Average Episode Length: 7.54\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 96 ---\n",
            "--- Seed 96 Results ---\n",
            "Success Rate: 0.3700 (37.00%)\n",
            "Average Detection Time (Successful Episodes): 3.05\n",
            "Average Episode Reward: -0.6830\n",
            "Average Episode Length: 7.43\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 97 ---\n",
            "--- Seed 97 Results ---\n",
            "Success Rate: 0.4000 (40.00%)\n",
            "Average Detection Time (Successful Episodes): 2.90\n",
            "Average Episode Reward: -0.6250\n",
            "Average Episode Length: 7.16\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 98 ---\n",
            "--- Seed 98 Results ---\n",
            "Success Rate: 0.3300 (33.00%)\n",
            "Average Detection Time (Successful Episodes): 2.52\n",
            "Average Episode Reward: -0.7195\n",
            "Average Episode Length: 7.53\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 99 ---\n",
            "--- Seed 99 Results ---\n",
            "Success Rate: 0.3700 (37.00%)\n",
            "Average Detection Time (Successful Episodes): 2.46\n",
            "Average Episode Reward: -0.6510\n",
            "Average Episode Length: 7.21\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Overall Single-Object DP Policy Results ---\n",
            "Total experiment time across 100 seeds: 1.53 seconds\n",
            "Average Success Rate over 100 seeds and 100 episodes each: 0.3337 (33.37%)\n",
            "Standard Deviation of Success Rate: 0.0465\n",
            "Average Detection Time (Successful Episodes) across all seeds: 2.77\n",
            "Standard Deviation of Detection Time: 1.97\n",
            "Average Episode Reward across all seeds: -0.7334\n",
            "Standard Deviation of Reward: 0.8869\n",
            "Average Episode Length across all seeds: 7.59\n",
            "Standard Deviation of Episode Length: 3.60\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from functools import lru_cache\n",
        "from typing import Dict, List, Tuple\n",
        "import time\n",
        "\n",
        "class SingleObjectDPSolver:\n",
        "    \"\"\"\n",
        "    An exact Dynamic Programming solver for a single-object search problem.\n",
        "\n",
        "    This class uses backward recursion to find the optimal policy based on a\n",
        "    marginal prior distribution for a single target.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n: int, T: int, p0_marginal: np.ndarray, gamma1: np.ndarray, c: np.ndarray):\n",
        "        \"\"\"\n",
        "        Initializes the solver with the single-object problem parameters.\n",
        "\n",
        "        Args:\n",
        "            n (int): The number of cells.\n",
        "            T (int): The time horizon.\n",
        "            p0_marginal (np.ndarray): The n-element marginal prior probability vector for O1.\n",
        "            gamma1 (np.ndarray): Miss-detection rates for O1.\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.T = T\n",
        "        self.p0_marginal = p0_marginal\n",
        "        self.gamma1 = gamma1\n",
        "        self.c = c\n",
        "\n",
        "        # Data structures to store the results\n",
        "        self.J_values: Dict[int, Dict[Tuple[int, ...], float]] = {}  # Value function J_t(z_t)\n",
        "        self.Policy: Dict[int, Dict[Tuple[int, ...], int]] = {}      # Policy mu_t(z_t)\n",
        "\n",
        "    @staticmethod\n",
        "    @lru_cache(maxsize=None) # Memoization for performance\n",
        "    def _generate_z_vectors(t: int, n: int) -> List[Tuple[int, ...]]:\n",
        "        \"\"\"Recursively generates all state vectors z_t where sum(z_i) = t.\"\"\"\n",
        "        if n == 1:\n",
        "            return [(t,)]\n",
        "\n",
        "        vectors = []\n",
        "        for i in range(t + 1):\n",
        "            for sub_vector in SingleObjectDPSolver._generate_z_vectors(t - i, n - 1):\n",
        "                vectors.append((i,) + sub_vector)\n",
        "        return vectors\n",
        "\n",
        "    def _calculate_posterior(self, z_vector: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Calculates the posterior belief p(O1=i | z).\"\"\"\n",
        "        g1_z = np.power(self.gamma1, z_vector)\n",
        "        numerator = self.p0_marginal * g1_z\n",
        "        norm = np.sum(numerator)\n",
        "        return numerator / norm if norm > 0 else numerator\n",
        "\n",
        "    def solve(self):\n",
        "        \"\"\"\n",
        "        Executes the backward recursion to solve the DP problem.\n",
        "        \"\"\"\n",
        "        # print(\"Starting Single-Object DP solver...\")\n",
        "        # --- Initialization at T ---\n",
        "        # print(f\"Initializing for T={self.T}...\")\n",
        "        self.J_values[self.T] = {}\n",
        "        z_vectors_T = self._generate_z_vectors(self.T, self.n)\n",
        "        for z_T in z_vectors_T:\n",
        "            self.J_values[self.T][z_T] = 0.0\n",
        "\n",
        "        # --- Backward Recursion ---\n",
        "        for t in range(self.T - 1, -1, -1):\n",
        "            start_time = time.time()\n",
        "            J_t = {}\n",
        "            policy_t = {}\n",
        "            z_vectors_t = self._generate_z_vectors(t, self.n)\n",
        "\n",
        "            for z_t_tuple in z_vectors_t:\n",
        "                z_t = np.array(z_t_tuple)\n",
        "                action_values = []\n",
        "\n",
        "                # Calculate current belief vector\n",
        "                belief_t = self._calculate_posterior(z_t)\n",
        "\n",
        "                # Iterate over all possible actions\n",
        "                for a_t in range(self.n):\n",
        "                    # Probability of finding the target if we search cell a_t\n",
        "                    p_success = (1 - self.gamma1[a_t]) * belief_t[a_t]\n",
        "                    p_fail = 1 - ((1 - self.gamma1[a_t]) * belief_t[a_t])\n",
        "\n",
        "                    # Get the value of the state we transition to upon failure\n",
        "                    next_z_tuple = tuple(z_t + np.eye(self.n, dtype=int)[a_t])\n",
        "                    val_if_fail = self.J_values[t + 1][next_z_tuple]\n",
        "\n",
        "                    # Expected value for this action (Bellman equation)\n",
        "                    # Reward for success is 1, reward for failure is 0\n",
        "                    expected_value = (p_success * 1.0) - self.c[a_t] + (p_fail * val_if_fail)\n",
        "                    action_values.append(expected_value)\n",
        "\n",
        "                # Find best action and store value/policy\n",
        "                best_value = np.max(action_values)\n",
        "                best_action = np.argmax(action_values)\n",
        "                J_t[z_t_tuple] = best_value\n",
        "                policy_t[z_t_tuple] = best_action\n",
        "\n",
        "            self.J_values[t] = J_t\n",
        "            self.Policy[t] = policy_t\n",
        "            end_time = time.time()\n",
        "            # print(f\"Completed t={t}. Found {len(z_vectors_t)} states. Took {end_time - start_time:.2f}s.\")\n",
        "\n",
        "        # print(\"DP solver finished.\")\n",
        "\n",
        "    def get_optimal_value(self) -> float:\n",
        "        \"\"\"Returns the optimal value J(z_0).\"\"\"\n",
        "        initial_z = tuple([0] * self.n)\n",
        "        return self.J_values[0][initial_z]\n",
        "\n",
        "# Function to run a single episode for the single-object solver\n",
        "def run_single_object_episode(n: int, T: int, p0_marginal: np.ndarray, gamma1: np.ndarray, c: np.ndarray, policy: Dict, episode_seed: int) -> Tuple[bool, int, float, int]:\n",
        "    \"\"\"\n",
        "    Simulates a single episode using a given single-object DP policy.\n",
        "    Returns success status, time of detection (or -1 if failed), accumulated reward, and episode length.\n",
        "    \"\"\"\n",
        "    # Create a local RNG for this episode for statistical independence\n",
        "    rng = np.random.default_rng(episode_seed)\n",
        "\n",
        "    # Secretly determine the true location of O1\n",
        "    if np.sum(p0_marginal) == 0:\n",
        "         true_pos_o1 = -1\n",
        "    else:\n",
        "        true_pos_o1 = rng.choice(n, p=p0_marginal / np.sum(p0_marginal))\n",
        "\n",
        "    z_vector = np.zeros(n, dtype=int)\n",
        "    accumulated_reward = 0.0\n",
        "    detection_time = -1\n",
        "    episode_length = 0\n",
        "\n",
        "    for t in range(T):\n",
        "        episode_length += 1  # Increment episode length at each time step\n",
        "        state = tuple(z_vector)\n",
        "        if t not in policy or state not in policy[t]:\n",
        "            # This case should ideally not happen if the policy is complete for the horizon\n",
        "            print(f\"Warning: Policy not found for state {state} at time {t}. Using greedy action.\")\n",
        "            # Fallback to greedy action if policy is missing\n",
        "            belief = SingleObjectDPSolver(n, T, p0_marginal, gamma1, c)._calculate_posterior(z_vector)\n",
        "            action = np.argmax((1 - gamma1) * belief - c)\n",
        "        else:\n",
        "             action = policy[t][state]\n",
        "\n",
        "        # Deduct cost for the action\n",
        "        accumulated_reward -= c[action]\n",
        "\n",
        "        # Simulate outcome\n",
        "        if action == true_pos_o1 and rng.random() > gamma1[action]: # Use local RNG\n",
        "            accumulated_reward += 1.0 # Reward for finding O1\n",
        "            detection_time = t + 1 # Time is absolute\n",
        "            return True, detection_time, accumulated_reward, episode_length # Mission Success!\n",
        "\n",
        "        # Update state\n",
        "        z_vector[action] += 1\n",
        "\n",
        "    return False, detection_time, accumulated_reward, episode_length # Mission Failed\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Problem Definition ---\n",
        "    NUM_CELLS = 5\n",
        "    TIME_HORIZON = 10\n",
        "    NUM_EPISODES_PER_SEED = 100  # Number of episodes to run for each seed\n",
        "    NUM_SEEDS = 100  # Number of different seeds to run\n",
        "\n",
        "    # The original JOINT prior matrix\n",
        "    prior_joint = np.array([\n",
        "        [0.152,  0.0039, 0.003,  0.0108, 0.011],\n",
        "        [0.0038, 0.0052, 0.117,  0.0162, 0.165],\n",
        "        [0.0057, 0.195,  0.015,  0.009,  0.011],\n",
        "        [0.0038, 0.0091, 0.0075, 0.027,  0.011],\n",
        "        [0.0247, 0.0468, 0.0075, 0.117,  0.022]\n",
        "    ])\n",
        "\n",
        "    # Miss-detection rates for O1\n",
        "    gammas1 = np.array([0.8, 0.65, 0.82, 0.75, 0.7])\n",
        "    c = np.array([0.15,0.2,0.25,0.1,0.2])\n",
        "\n",
        "    # --- KEY STEP: Calculate the marginal prior for O1 ---\n",
        "    # We sum the probabilities across the columns (axis=1) for each row\n",
        "    p0_marginal_o1 = np.sum(prior_joint, axis=1)\n",
        "\n",
        "    print(\"--- Calculating Benchmark for Single-Object Search (Ignoring O2) ---\")\n",
        "    print(f\"Original Joint Prior Shape: {prior_joint.shape}\")\n",
        "    print(f\"Calculated Marginal Prior for O1 (p(O1)): {np.round(p0_marginal_o1, 4)}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    all_success_rates = []\n",
        "    all_detection_times = []\n",
        "    all_rewards = []\n",
        "    all_episode_lengths = [] # New list to store episode lengths\n",
        "    total_experiment_start_time = time.time()\n",
        "\n",
        "    print(f\"\\n--- Running Empirical Experiment for Single-Object DP Policy over Multiple Seeds ---\")\n",
        "    print(f\"Number of seeds: {NUM_SEEDS}\")\n",
        "    print(f\"Number of evaluation episodes per seed: {NUM_EPISODES_PER_SEED}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    solver = SingleObjectDPSolver(n=NUM_CELLS, T=TIME_HORIZON, p0_marginal=p0_marginal_o1, gamma1=gammas1, c=c)\n",
        "    solver.solve()\n",
        "    optimal_policy = solver.Policy\n",
        "\n",
        "    for seed in range(NUM_SEEDS):\n",
        "        print(f\"\\n--- Running with Seed: {seed} ---\")\n",
        "        # np.random.seed(seed) # Removed global seeding here to rely on per-episode seeding\n",
        "\n",
        "        # --- Solve the problem (Policy calculation) ---\n",
        "        # The policy depends only on the parameters and the seed for sampling true location,\n",
        "        # but we recalculate it per seed to isolate seed effects on the policy calculation itself if any\n",
        "        # (though for exact DP with fixed parameters, it should be the same)\n",
        "\n",
        "\n",
        "        # --- Run the Experiment for this seed ---\n",
        "        num_successes = 0\n",
        "        detection_times_this_seed = []\n",
        "        rewards_this_seed = []\n",
        "        episode_lengths_this_seed = [] # New list for this seed\n",
        "        start_time_this_seed = time.time()\n",
        "\n",
        "        for i in range(NUM_EPISODES_PER_SEED):\n",
        "            episode_unique_seed = seed * NUM_EPISODES_PER_SEED + i\n",
        "            success, detection_time, reward, episode_length = run_single_object_episode(\n",
        "                NUM_CELLS, TIME_HORIZON, p0_marginal_o1, gammas1, c, optimal_policy, episode_unique_seed\n",
        "            )\n",
        "            rewards_this_seed.append(reward)\n",
        "            episode_lengths_this_seed.append(episode_length) # Append episode length\n",
        "\n",
        "            if success:\n",
        "                num_successes += 1\n",
        "                detection_times_this_seed.append(detection_time)\n",
        "\n",
        "        end_time_this_seed = time.time()\n",
        "        success_rate_this_seed = num_successes / NUM_EPISODES_PER_SEED\n",
        "        all_success_rates.append(success_rate_this_seed)\n",
        "        all_detection_times.extend(detection_times_this_seed)\n",
        "        all_rewards.extend(rewards_this_seed)\n",
        "        all_episode_lengths.extend(episode_lengths_this_seed) # Extend overall list\n",
        "\n",
        "        print(f\"--- Seed {seed} Results ---\")\n",
        "        print(f\"Success Rate: {success_rate_this_seed:.4f} ({success_rate_this_seed*100:.2f}%)\")\n",
        "        if detection_times_this_seed:\n",
        "            print(f\"Average Detection Time (Successful Episodes): {np.mean(detection_times_this_seed):.2f}\")\n",
        "        else:\n",
        "            print(\"Average Detection Time (Successful Episodes): N/A (no successes)\")\n",
        "        print(f\"Average Episode Reward: {np.mean(rewards_this_seed):.4f}\")\n",
        "        print(f\"Average Episode Length: {np.mean(episode_lengths_this_seed):.2f}\") # Display average episode length\n",
        "        print(f\"Time taken for this seed: {end_time_this_seed - start_time_this_seed:.2f} seconds\")\n",
        "\n",
        "    total_experiment_end_time = time.time()\n",
        "\n",
        "    # --- Display Final Results ---\n",
        "    mean_success_rate = np.mean(all_success_rates)\n",
        "    std_success_rate = np.std(all_success_rates)\n",
        "    mean_detection_time = np.mean(all_detection_times) if all_detection_times else -1\n",
        "    std_detection_time = np.std(all_detection_times) if all_detection_times else -1\n",
        "    mean_reward = np.mean(all_rewards)\n",
        "    std_reward = np.std(all_rewards)\n",
        "    mean_episode_length = np.mean(all_episode_lengths) # Calculate overall average episode length\n",
        "    std_episode_length = np.std(all_episode_lengths) # Calculate overall standard deviation of episode length\n",
        "\n",
        "\n",
        "    print(\"\\n--- Overall Single-Object DP Policy Results ---\")\n",
        "    print(f\"Total experiment time across {NUM_SEEDS} seeds: {total_experiment_end_time - total_experiment_start_time:.2f} seconds\")\n",
        "    print(f\"Average Success Rate over {NUM_SEEDS} seeds and {NUM_EPISODES_PER_SEED} episodes each: {mean_success_rate:.4f} ({mean_success_rate*100:.2f}%)\")\n",
        "    print(f\"Standard Deviation of Success Rate: {std_success_rate:.4f}\")\n",
        "    if all_detection_times:\n",
        "        print(f\"Average Detection Time (Successful Episodes) across all seeds: {mean_detection_time:.2f}\")\n",
        "        print(f\"Standard Deviation of Detection Time: {std_detection_time:.2f}\")\n",
        "    else:\n",
        "        print(\"Average Detection Time (Successful Episodes) across all seeds: N/A (no successes)\")\n",
        "\n",
        "    print(f\"Average Episode Reward across all seeds: {mean_reward:.4f}\")\n",
        "    print(f\"Standard Deviation of Reward: {std_reward:.4f}\")\n",
        "    print(f\"Average Episode Length across all seeds: {mean_episode_length:.2f}\") # Display overall average episode length\n",
        "    print(f\"Standard Deviation of Episode Length: {std_episode_length:.2f}\") # Display overall standard deviation of episode length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu36DFAN5dbB",
        "outputId": "466d543b-fd73-41d2-97d9-87b7f650b443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting DP solver...\n",
            "Initializing for T=10...\n",
            "Completed t=9. Found 715 states. Took 0.64s.\n",
            "Completed t=8. Found 495 states. Took 0.43s.\n",
            "Completed t=7. Found 330 states. Took 0.30s.\n",
            "Completed t=6. Found 210 states. Took 0.17s.\n",
            "Completed t=5. Found 126 states. Took 0.11s.\n",
            "Completed t=4. Found 70 states. Took 0.06s.\n",
            "Completed t=3. Found 35 states. Took 0.03s.\n",
            "Completed t=2. Found 15 states. Took 0.01s.\n",
            "Completed t=1. Found 5 states. Took 0.00s.\n",
            "Completed t=0. Found 1 states. Took 0.00s.\n",
            "DP solver finished.\n",
            "\n",
            "--- DP Solver Results ---\n",
            "Optimal Probability of Success: -0.4601 (-46.01%)\n",
            "\n",
            "First 3 steps of the optimal policy from the initial state:\n",
            "t=0: From state z=[0, 0, 0, 0, 0], theta2=0 -> Search cell 1\n",
            "t=1: From state z=[0, 1, 0, 0, 0], theta2=0 -> Search cell 3\n",
            "t=2: From state z=[0, 1, 0, 1, 0], theta2=0 -> Search cell 0\n",
            "t=3: From state z=[1, 1, 0, 1, 0], theta2=0 -> Search cell 1\n",
            "t=4: From state z=[1, 2, 0, 1, 0], theta2=0 -> Search cell 1\n",
            "t=5: From state z=[1, 3, 0, 1, 0], theta2=0 -> Search cell 3\n",
            "t=6: From state z=[1, 3, 0, 2, 0], theta2=0 -> Search cell 0\n",
            "t=7: From state z=[2, 3, 0, 2, 0], theta2=0 -> Search cell 1\n",
            "t=8: From state z=[2, 4, 0, 2, 0], theta2=0 -> Search cell 1\n",
            "t=9: From state z=[2, 5, 0, 2, 0], theta2=0 -> Search cell 3\n",
            "probility comuted : {((0, 0, 0, 0, 0), 0): np.float64(-0.46008379604604555)}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from functools import lru_cache\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import time\n",
        "\n",
        "class TwoObjectDPSolver:\n",
        "    \"\"\"\n",
        "    An exact Dynamic Programming solver for the two-object search problem.\n",
        "\n",
        "    This class uses backward recursion to find the optimal policy and the\n",
        "    maximum probability of detecting the primary target (O1).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n: int, T: int, p0: np.ndarray, gamma1: np.ndarray, gamma2: np.ndarray, c: np.ndarray):\n",
        "        \"\"\"\n",
        "        Initializes the solver with the problem parameters.\n",
        "\n",
        "        Args:\n",
        "            n (int): The number of cells.\n",
        "            T (int): The time horizon.\n",
        "            p0 (np.ndarray): The n x n prior joint probability matrix.\n",
        "            gamma1 (np.ndarray): Miss-detection rates for O1.\n",
        "            gamma2 (np.ndarray): Miss-detection rates for O2.\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.T = T\n",
        "        self.p0 = p0\n",
        "        self.gamma1 = gamma1\n",
        "        self.gamma2 = gamma2\n",
        "        self.c = c\n",
        "\n",
        "        # Data structures to store the results\n",
        "        self.J_values: Dict[int, Dict[Tuple, float]] = {}  # Value function J_t(s_t)\n",
        "        self.Policy: Dict[int, Dict[Tuple, int]] = {}      # Policy mu_t(s_t)\n",
        "\n",
        "        # Pre-calculate initial conditional priors for efficiency\n",
        "        self._p0_conditionals = self._precompute_p0_conditionals()\n",
        "\n",
        "    def _precompute_p0_conditionals(self) -> List[np.ndarray]:\n",
        "        \"\"\"Pre-calculates P(O1=i | O2=k) for all k.\"\"\"\n",
        "        conditionals = []\n",
        "        for k in range(self.n):\n",
        "            marginal_o2 = np.sum(self.p0[:, k])\n",
        "            if marginal_o2 > 0:\n",
        "                conditionals.append(self.p0[:, k] / marginal_o2)\n",
        "            else:\n",
        "                # If O2 can never be in cell k, the conditional is undefined. Use zeros.\n",
        "                conditionals.append(np.zeros(self.n))\n",
        "        return conditionals\n",
        "\n",
        "    @staticmethod\n",
        "    @lru_cache(maxsize=None) # Memoization for performance\n",
        "    def _generate_z_vectors(t: int, n: int) -> List[Tuple[int, ...]]:\n",
        "        \"\"\"Recursively generates all state vectors z_t where sum(z_i) = t.\"\"\"\n",
        "        if n == 1:\n",
        "            return [(t,)]\n",
        "\n",
        "        vectors = []\n",
        "        for i in range(t + 1):\n",
        "            for sub_vector in TwoObjectDPSolver._generate_z_vectors(t - i, n - 1):\n",
        "                vectors.append((i,) + sub_vector)\n",
        "        \"\"\"if n == 5:\n",
        "           for vec in vectors:\n",
        "               print(vec)\"\"\"\n",
        "        return vectors\n",
        "\n",
        "    def _calculate_joint_posterior(self, z_vector: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Calculates p(O1=i, O2=j | z).\"\"\"\n",
        "        g1_z = np.power(self.gamma1, z_vector)\n",
        "        g2_z = np.power(self.gamma2, z_vector)\n",
        "        likelihood = np.outer(g1_z, g2_z)\n",
        "        numerator = self.p0 * likelihood\n",
        "        norm = np.sum(numerator)\n",
        "        return numerator / norm if norm > 0 else numerator\n",
        "\n",
        "    def _calculate_conditional_posterior(self, z_vector: np.ndarray, o2_loc: int) -> np.ndarray:\n",
        "        \"\"\"Calculates p(O1=i | z, O2=o2_loc).\"\"\"\n",
        "        p0_cond = self._p0_conditionals[o2_loc]\n",
        "        g1_z = np.power(self.gamma1, z_vector)\n",
        "        numerator = g1_z * p0_cond\n",
        "        norm = np.sum(numerator)\n",
        "        return numerator / norm if norm > 0 else numerator\n",
        "\n",
        "    def solve(self):\n",
        "        \"\"\"\n",
        "        Executes the backward recursion to solve the DP problem.\n",
        "        \"\"\"\n",
        "        print(\"Starting DP solver...\")\n",
        "        # --- Initialization at T ---\n",
        "        print(f\"Initializing for T={self.T}...\")\n",
        "        self.J_values[self.T] = {}\n",
        "        z_vectors_T = self._generate_z_vectors(self.T, self.n)\n",
        "        for z_T in z_vectors_T:\n",
        "            for theta2 in range(self.n + 1):\n",
        "                state = (z_T, theta2)\n",
        "                self.J_values[self.T][state] = 0.0\n",
        "\n",
        "        # --- Backward Recursion ---\n",
        "        for t in range(self.T - 1, -1, -1):\n",
        "            start_time = time.time()\n",
        "            J_t = {}\n",
        "            policy_t = {}\n",
        "            z_vectors_t = self._generate_z_vectors(t, self.n)\n",
        "\n",
        "            for z_t_tuple in z_vectors_t:\n",
        "                z_t = np.array(z_t_tuple)\n",
        "                for theta2 in range(self.n + 1):\n",
        "                    if t == 0 and theta2 > 0:\n",
        "                        break\n",
        "                    current_state = (z_t_tuple, theta2)\n",
        "                    action_values = []\n",
        "\n",
        "                    # Calculate belief based on current state\n",
        "                    if theta2 == 0:\n",
        "                        belief = self._calculate_joint_posterior(z_t)\n",
        "                    else:\n",
        "                        o2_loc = theta2 - 1\n",
        "                        belief = self._calculate_conditional_posterior(z_t, o2_loc)\n",
        "\n",
        "                    # Iterate over all possible actions\n",
        "                    for a_t in range(self.n):\n",
        "                        next_z_tuple = tuple(z_t + np.eye(self.n, dtype=int)[a_t])\n",
        "\n",
        "                        # --- Case 1: Both objects hidden ---\n",
        "                        if theta2 == 0:\n",
        "                            p_marginal_o1_at_a = np.sum(belief[a_t, :])\n",
        "\n",
        "                            # Prob of success (finding O1)\n",
        "                            p_success = (1 - self.gamma1[a_t]) * p_marginal_o1_at_a\n",
        "\n",
        "                            # Prob of finding O2 only\n",
        "                            p_cond_sum = np.sum(belief[np.arange(self.n) != a_t, a_t])\n",
        "                            p_find_o2_only = (1 - self.gamma2[a_t]) * (self.gamma1[a_t] * belief[a_t, a_t] + p_cond_sum)\n",
        "\n",
        "                            # Prob of finding nothing\n",
        "                            p_nothing = 1 - p_success - p_find_o2_only\n",
        "\n",
        "                            # Future values from next stage\n",
        "                            val_if_nothing = self.J_values[t + 1][(next_z_tuple, 0)]\n",
        "                            val_if_found_o2 = self.J_values[t + 1][(next_z_tuple, a_t + 1)]\n",
        "\n",
        "                            # Expected value for this action\n",
        "                            expected_value = (p_success * 1.0) - self.c[a_t]+ \\\n",
        "                                             (p_nothing * val_if_nothing) + \\\n",
        "                                             (p_find_o2_only * val_if_found_o2)\n",
        "                            action_values.append(expected_value)\n",
        "\n",
        "                        # --- Case 2: O2 has been found ---\n",
        "                        else:\n",
        "                            p_o1_at_a = belief[a_t]\n",
        "                            p_success = (1 - self.gamma1[a_t]) * p_o1_at_a\n",
        "                            p_fail = 1 - p_success\n",
        "\n",
        "                            val_if_fail = self.J_values[t + 1][(next_z_tuple, theta2)]\n",
        "\n",
        "                            expected_value = (p_success * 1.0) - self.c[a_t]+ (p_fail * val_if_fail)\n",
        "                            action_values.append(expected_value)\n",
        "\n",
        "                    # Find best action and store value/policy\n",
        "                    best_value = np.max(action_values)\n",
        "                    best_action = np.argmax(action_values)\n",
        "                    J_t[current_state] = best_value\n",
        "                    policy_t[current_state] = best_action\n",
        "\n",
        "            self.J_values[t] = J_t\n",
        "            self.Policy[t] = policy_t\n",
        "            end_time = time.time()\n",
        "            print(f\"Completed t={t}. Found {len(z_vectors_t)} states. Took {end_time - start_time:.2f}s.\")\n",
        "\n",
        "        print(\"DP solver finished.\")\n",
        "\n",
        "    def get_optimal_value(self) -> float:\n",
        "        \"\"\"Returns the optimal value J(s_0).\"\"\"\n",
        "        initial_z = tuple([0] * self.n)\n",
        "        initial_state = (initial_z, 0)\n",
        "        return self.J_values[0][initial_state]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Problem Definition ---\n",
        "    # Using the same parameters as the PPO training for direct comparison\n",
        "    NUM_CELLS = 5\n",
        "    TIME_HORIZON = 10\n",
        "\n",
        "    prior = np.array([\n",
        "        [0.152,  0.0039, 0.003,  0.0108, 0.011],\n",
        "        [0.0038, 0.0052, 0.117,  0.0162, 0.165],\n",
        "        [0.0057, 0.195,  0.015,  0.009,  0.011],\n",
        "        [0.0038, 0.0091, 0.0075, 0.027,  0.011],\n",
        "        [0.0247, 0.0468, 0.0075, 0.117,  0.022]\n",
        "    ])\n",
        "\n",
        "    gammas1 = np.array([0.8, 0.65, 0.82, 0.75, 0.7])\n",
        "    gammas2 = np.array([0.2, 0.1, 0.25, 0.15, 0.2])\n",
        "    c= [0.15,0.2,0.25,0.1,0.2]\n",
        "\n",
        "\n",
        "\n",
        "    # --- Solve the problem ---\n",
        "    solver = TwoObjectDPSolver(n=NUM_CELLS, T=TIME_HORIZON, p0=prior, gamma1=gammas1, gamma2=gammas2, c=c)\n",
        "    solver.solve()\n",
        "\n",
        "    # --- Display Results ---\n",
        "    optimal_prob_of_success = solver.get_optimal_value()\n",
        "    print(\"\\n--- DP Solver Results ---\")\n",
        "    print(f\"Optimal Probability of Success: {optimal_prob_of_success:.4f} ({optimal_prob_of_success*100:.2f}%)\")\n",
        "\n",
        "    # Show the first few steps of the optimal policy\n",
        "    z = tuple([0] * NUM_CELLS)\n",
        "    theta2 = 0\n",
        "    print(\"\\nFirst 3 steps of the optimal policy from the initial state:\")\n",
        "    for t in range(10):\n",
        "        state = (z, theta2)\n",
        "        action = solver.Policy[t][state]\n",
        "        print(f\"t={t}: From state z={list(z)}, theta2={theta2} -> Search cell {action}\")\n",
        "        # Update z for the next step demonstration\n",
        "        z_list = list(z)\n",
        "        z_list[action] += 1\n",
        "        z = tuple(z_list)\n",
        "\n",
        "\n",
        "print(\"probility comuted :\", solver.J_values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e87hajzv5p50",
        "outputId": "d4baf37f-8e11-4afc-cdc2-68536f9966aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Empirical Experiment for Adaptive Re-planning Policy over Multiple Seeds ---\n",
            "Number of seeds: 100\n",
            "Number of evaluation episodes per seed: 100\n",
            "--------------------\n",
            "--- Pre-computing the initial Two-Object DP policy ---\n",
            "probility comuted : {((0, 0, 0, 0, 0), 0): np.float64(-0.46008379604604555)}\n",
            "--------------------\n",
            "\n",
            "--- Running with Seed: 0 ---\n",
            "--- Seed 0 Results ---\n",
            "Success Rate: 0.5900 (59.00%)\n",
            "Average Detection Time (Successful Episodes): 3.92\n",
            "Average Episode Reward: -0.5120\n",
            "Average Episode Length: 6.41\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 1 ---\n",
            "--- Seed 1 Results ---\n",
            "Success Rate: 0.5700 (57.00%)\n",
            "Average Detection Time (Successful Episodes): 4.54\n",
            "Average Episode Reward: -0.5820\n",
            "Average Episode Length: 6.89\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 2 ---\n",
            "--- Seed 2 Results ---\n",
            "Success Rate: 0.5900 (59.00%)\n",
            "Average Detection Time (Successful Episodes): 4.03\n",
            "Average Episode Reward: -0.5215\n",
            "Average Episode Length: 6.48\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 3 ---\n",
            "--- Seed 3 Results ---\n",
            "Success Rate: 0.7100 (71.00%)\n",
            "Average Detection Time (Successful Episodes): 4.25\n",
            "Average Episode Reward: -0.3015\n",
            "Average Episode Length: 5.92\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 4 ---\n",
            "--- Seed 4 Results ---\n",
            "Success Rate: 0.6300 (63.00%)\n",
            "Average Detection Time (Successful Episodes): 4.37\n",
            "Average Episode Reward: -0.4500\n",
            "Average Episode Length: 6.45\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 5 ---\n",
            "--- Seed 5 Results ---\n",
            "Success Rate: 0.6300 (63.00%)\n",
            "Average Detection Time (Successful Episodes): 4.37\n",
            "Average Episode Reward: -0.4675\n",
            "Average Episode Length: 6.45\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 6 ---\n",
            "--- Seed 6 Results ---\n",
            "Success Rate: 0.6400 (64.00%)\n",
            "Average Detection Time (Successful Episodes): 4.66\n",
            "Average Episode Reward: -0.4650\n",
            "Average Episode Length: 6.58\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 7 ---\n",
            "--- Seed 7 Results ---\n",
            "Success Rate: 0.7200 (72.00%)\n",
            "Average Detection Time (Successful Episodes): 3.68\n",
            "Average Episode Reward: -0.1990\n",
            "Average Episode Length: 5.45\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 8 ---\n",
            "--- Seed 8 Results ---\n",
            "Success Rate: 0.6200 (62.00%)\n",
            "Average Detection Time (Successful Episodes): 4.50\n",
            "Average Episode Reward: -0.5130\n",
            "Average Episode Length: 6.59\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 9 ---\n",
            "--- Seed 9 Results ---\n",
            "Success Rate: 0.6400 (64.00%)\n",
            "Average Detection Time (Successful Episodes): 4.34\n",
            "Average Episode Reward: -0.4225\n",
            "Average Episode Length: 6.38\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 10 ---\n",
            "--- Seed 10 Results ---\n",
            "Success Rate: 0.6400 (64.00%)\n",
            "Average Detection Time (Successful Episodes): 4.45\n",
            "Average Episode Reward: -0.4590\n",
            "Average Episode Length: 6.45\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 11 ---\n",
            "--- Seed 11 Results ---\n",
            "Success Rate: 0.6200 (62.00%)\n",
            "Average Detection Time (Successful Episodes): 4.90\n",
            "Average Episode Reward: -0.5295\n",
            "Average Episode Length: 6.84\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 12 ---\n",
            "--- Seed 12 Results ---\n",
            "Success Rate: 0.5900 (59.00%)\n",
            "Average Detection Time (Successful Episodes): 4.00\n",
            "Average Episode Reward: -0.4915\n",
            "Average Episode Length: 6.46\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 13 ---\n",
            "--- Seed 13 Results ---\n",
            "Success Rate: 0.6100 (61.00%)\n",
            "Average Detection Time (Successful Episodes): 4.70\n",
            "Average Episode Reward: -0.5210\n",
            "Average Episode Length: 6.77\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 14 ---\n",
            "--- Seed 14 Results ---\n",
            "Success Rate: 0.7300 (73.00%)\n",
            "Average Detection Time (Successful Episodes): 4.37\n",
            "Average Episode Reward: -0.2770\n",
            "Average Episode Length: 5.89\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 15 ---\n",
            "--- Seed 15 Results ---\n",
            "Success Rate: 0.6300 (63.00%)\n",
            "Average Detection Time (Successful Episodes): 4.17\n",
            "Average Episode Reward: -0.4340\n",
            "Average Episode Length: 6.33\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 16 ---\n",
            "--- Seed 16 Results ---\n",
            "Success Rate: 0.6800 (68.00%)\n",
            "Average Detection Time (Successful Episodes): 3.76\n",
            "Average Episode Reward: -0.2755\n",
            "Average Episode Length: 5.76\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 17 ---\n",
            "--- Seed 17 Results ---\n",
            "Success Rate: 0.5100 (51.00%)\n",
            "Average Detection Time (Successful Episodes): 4.35\n",
            "Average Episode Reward: -0.6660\n",
            "Average Episode Length: 7.12\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 18 ---\n",
            "--- Seed 18 Results ---\n",
            "Success Rate: 0.6800 (68.00%)\n",
            "Average Detection Time (Successful Episodes): 4.21\n",
            "Average Episode Reward: -0.3365\n",
            "Average Episode Length: 6.06\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 19 ---\n",
            "--- Seed 19 Results ---\n",
            "Success Rate: 0.6200 (62.00%)\n",
            "Average Detection Time (Successful Episodes): 4.42\n",
            "Average Episode Reward: -0.4690\n",
            "Average Episode Length: 6.54\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 20 ---\n",
            "--- Seed 20 Results ---\n",
            "Success Rate: 0.6900 (69.00%)\n",
            "Average Detection Time (Successful Episodes): 4.16\n",
            "Average Episode Reward: -0.3300\n",
            "Average Episode Length: 5.97\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 21 ---\n",
            "--- Seed 21 Results ---\n",
            "Success Rate: 0.5800 (58.00%)\n",
            "Average Detection Time (Successful Episodes): 3.93\n",
            "Average Episode Reward: -0.5185\n",
            "Average Episode Length: 6.48\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 22 ---\n",
            "--- Seed 22 Results ---\n",
            "Success Rate: 0.5700 (57.00%)\n",
            "Average Detection Time (Successful Episodes): 5.02\n",
            "Average Episode Reward: -0.6290\n",
            "Average Episode Length: 7.16\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 23 ---\n",
            "--- Seed 23 Results ---\n",
            "Success Rate: 0.6100 (61.00%)\n",
            "Average Detection Time (Successful Episodes): 3.79\n",
            "Average Episode Reward: -0.4335\n",
            "Average Episode Length: 6.21\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 24 ---\n",
            "--- Seed 24 Results ---\n",
            "Success Rate: 0.6200 (62.00%)\n",
            "Average Detection Time (Successful Episodes): 3.98\n",
            "Average Episode Reward: -0.4475\n",
            "Average Episode Length: 6.27\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 25 ---\n",
            "--- Seed 25 Results ---\n",
            "Success Rate: 0.5900 (59.00%)\n",
            "Average Detection Time (Successful Episodes): 4.54\n",
            "Average Episode Reward: -0.5290\n",
            "Average Episode Length: 6.78\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 26 ---\n",
            "--- Seed 26 Results ---\n",
            "Success Rate: 0.6600 (66.00%)\n",
            "Average Detection Time (Successful Episodes): 4.97\n",
            "Average Episode Reward: -0.4770\n",
            "Average Episode Length: 6.68\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 27 ---\n",
            "--- Seed 27 Results ---\n",
            "Success Rate: 0.6000 (60.00%)\n",
            "Average Detection Time (Successful Episodes): 3.83\n",
            "Average Episode Reward: -0.4685\n",
            "Average Episode Length: 6.30\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 28 ---\n",
            "--- Seed 28 Results ---\n",
            "Success Rate: 0.6300 (63.00%)\n",
            "Average Detection Time (Successful Episodes): 4.62\n",
            "Average Episode Reward: -0.4870\n",
            "Average Episode Length: 6.61\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 29 ---\n",
            "--- Seed 29 Results ---\n",
            "Success Rate: 0.5800 (58.00%)\n",
            "Average Detection Time (Successful Episodes): 5.17\n",
            "Average Episode Reward: -0.6215\n",
            "Average Episode Length: 7.20\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 30 ---\n",
            "--- Seed 30 Results ---\n",
            "Success Rate: 0.6400 (64.00%)\n",
            "Average Detection Time (Successful Episodes): 3.86\n",
            "Average Episode Reward: -0.3820\n",
            "Average Episode Length: 6.07\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 31 ---\n",
            "--- Seed 31 Results ---\n",
            "Success Rate: 0.6100 (61.00%)\n",
            "Average Detection Time (Successful Episodes): 3.67\n",
            "Average Episode Reward: -0.4370\n",
            "Average Episode Length: 6.14\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 32 ---\n",
            "--- Seed 32 Results ---\n",
            "Success Rate: 0.6100 (61.00%)\n",
            "Average Detection Time (Successful Episodes): 3.62\n",
            "Average Episode Reward: -0.4410\n",
            "Average Episode Length: 6.11\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 33 ---\n",
            "--- Seed 33 Results ---\n",
            "Success Rate: 0.6100 (61.00%)\n",
            "Average Detection Time (Successful Episodes): 4.79\n",
            "Average Episode Reward: -0.5330\n",
            "Average Episode Length: 6.82\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 34 ---\n",
            "--- Seed 34 Results ---\n",
            "Success Rate: 0.7100 (71.00%)\n",
            "Average Detection Time (Successful Episodes): 4.07\n",
            "Average Episode Reward: -0.2635\n",
            "Average Episode Length: 5.79\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 35 ---\n",
            "--- Seed 35 Results ---\n",
            "Success Rate: 0.6500 (65.00%)\n",
            "Average Detection Time (Successful Episodes): 4.03\n",
            "Average Episode Reward: -0.3760\n",
            "Average Episode Length: 6.12\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 36 ---\n",
            "--- Seed 36 Results ---\n",
            "Success Rate: 0.5700 (57.00%)\n",
            "Average Detection Time (Successful Episodes): 3.54\n",
            "Average Episode Reward: -0.4895\n",
            "Average Episode Length: 6.32\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 37 ---\n",
            "--- Seed 37 Results ---\n",
            "Success Rate: 0.5800 (58.00%)\n",
            "Average Detection Time (Successful Episodes): 4.38\n",
            "Average Episode Reward: -0.5730\n",
            "Average Episode Length: 6.74\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 38 ---\n",
            "--- Seed 38 Results ---\n",
            "Success Rate: 0.7000 (70.00%)\n",
            "Average Detection Time (Successful Episodes): 4.16\n",
            "Average Episode Reward: -0.3195\n",
            "Average Episode Length: 5.91\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 39 ---\n",
            "--- Seed 39 Results ---\n",
            "Success Rate: 0.7200 (72.00%)\n",
            "Average Detection Time (Successful Episodes): 4.29\n",
            "Average Episode Reward: -0.2840\n",
            "Average Episode Length: 5.89\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 40 ---\n",
            "--- Seed 40 Results ---\n",
            "Success Rate: 0.6900 (69.00%)\n",
            "Average Detection Time (Successful Episodes): 4.49\n",
            "Average Episode Reward: -0.3620\n",
            "Average Episode Length: 6.20\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 41 ---\n",
            "--- Seed 41 Results ---\n",
            "Success Rate: 0.6300 (63.00%)\n",
            "Average Detection Time (Successful Episodes): 3.90\n",
            "Average Episode Reward: -0.4010\n",
            "Average Episode Length: 6.16\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 42 ---\n",
            "--- Seed 42 Results ---\n",
            "Success Rate: 0.6300 (63.00%)\n",
            "Average Detection Time (Successful Episodes): 4.22\n",
            "Average Episode Reward: -0.4335\n",
            "Average Episode Length: 6.36\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 43 ---\n",
            "--- Seed 43 Results ---\n",
            "Success Rate: 0.6300 (63.00%)\n",
            "Average Detection Time (Successful Episodes): 3.84\n",
            "Average Episode Reward: -0.4035\n",
            "Average Episode Length: 6.12\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 44 ---\n",
            "--- Seed 44 Results ---\n",
            "Success Rate: 0.6600 (66.00%)\n",
            "Average Detection Time (Successful Episodes): 4.48\n",
            "Average Episode Reward: -0.4315\n",
            "Average Episode Length: 6.36\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 45 ---\n",
            "--- Seed 45 Results ---\n",
            "Success Rate: 0.6600 (66.00%)\n",
            "Average Detection Time (Successful Episodes): 4.41\n",
            "Average Episode Reward: -0.4240\n",
            "Average Episode Length: 6.31\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 46 ---\n",
            "--- Seed 46 Results ---\n",
            "Success Rate: 0.5700 (57.00%)\n",
            "Average Detection Time (Successful Episodes): 4.04\n",
            "Average Episode Reward: -0.5415\n",
            "Average Episode Length: 6.60\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 47 ---\n",
            "--- Seed 47 Results ---\n",
            "Success Rate: 0.7000 (70.00%)\n",
            "Average Detection Time (Successful Episodes): 4.29\n",
            "Average Episode Reward: -0.3105\n",
            "Average Episode Length: 6.00\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 48 ---\n",
            "--- Seed 48 Results ---\n",
            "Success Rate: 0.6200 (62.00%)\n",
            "Average Detection Time (Successful Episodes): 4.13\n",
            "Average Episode Reward: -0.4785\n",
            "Average Episode Length: 6.36\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 49 ---\n",
            "--- Seed 49 Results ---\n",
            "Success Rate: 0.6100 (61.00%)\n",
            "Average Detection Time (Successful Episodes): 4.16\n",
            "Average Episode Reward: -0.5070\n",
            "Average Episode Length: 6.44\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 50 ---\n",
            "--- Seed 50 Results ---\n",
            "Success Rate: 0.5900 (59.00%)\n",
            "Average Detection Time (Successful Episodes): 4.34\n",
            "Average Episode Reward: -0.5315\n",
            "Average Episode Length: 6.66\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 51 ---\n",
            "--- Seed 51 Results ---\n",
            "Success Rate: 0.5700 (57.00%)\n",
            "Average Detection Time (Successful Episodes): 4.21\n",
            "Average Episode Reward: -0.5500\n",
            "Average Episode Length: 6.70\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 52 ---\n",
            "--- Seed 52 Results ---\n",
            "Success Rate: 0.6300 (63.00%)\n",
            "Average Detection Time (Successful Episodes): 4.17\n",
            "Average Episode Reward: -0.4350\n",
            "Average Episode Length: 6.33\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 53 ---\n",
            "--- Seed 53 Results ---\n",
            "Success Rate: 0.6400 (64.00%)\n",
            "Average Detection Time (Successful Episodes): 4.14\n",
            "Average Episode Reward: -0.4030\n",
            "Average Episode Length: 6.25\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 54 ---\n",
            "--- Seed 54 Results ---\n",
            "Success Rate: 0.6300 (63.00%)\n",
            "Average Detection Time (Successful Episodes): 4.71\n",
            "Average Episode Reward: -0.4785\n",
            "Average Episode Length: 6.67\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 55 ---\n",
            "--- Seed 55 Results ---\n",
            "Success Rate: 0.6200 (62.00%)\n",
            "Average Detection Time (Successful Episodes): 4.24\n",
            "Average Episode Reward: -0.4640\n",
            "Average Episode Length: 6.43\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 56 ---\n",
            "--- Seed 56 Results ---\n",
            "Success Rate: 0.5600 (56.00%)\n",
            "Average Detection Time (Successful Episodes): 4.66\n",
            "Average Episode Reward: -0.5910\n",
            "Average Episode Length: 7.01\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 57 ---\n",
            "--- Seed 57 Results ---\n",
            "Success Rate: 0.5700 (57.00%)\n",
            "Average Detection Time (Successful Episodes): 4.53\n",
            "Average Episode Reward: -0.5715\n",
            "Average Episode Length: 6.88\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 58 ---\n",
            "--- Seed 58 Results ---\n",
            "Success Rate: 0.6500 (65.00%)\n",
            "Average Detection Time (Successful Episodes): 4.38\n",
            "Average Episode Reward: -0.4105\n",
            "Average Episode Length: 6.35\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 59 ---\n",
            "--- Seed 59 Results ---\n",
            "Success Rate: 0.6400 (64.00%)\n",
            "Average Detection Time (Successful Episodes): 4.58\n",
            "Average Episode Reward: -0.4545\n",
            "Average Episode Length: 6.53\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 60 ---\n",
            "--- Seed 60 Results ---\n",
            "Success Rate: 0.6400 (64.00%)\n",
            "Average Detection Time (Successful Episodes): 3.92\n",
            "Average Episode Reward: -0.3795\n",
            "Average Episode Length: 6.11\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 61 ---\n",
            "--- Seed 61 Results ---\n",
            "Success Rate: 0.6500 (65.00%)\n",
            "Average Detection Time (Successful Episodes): 4.32\n",
            "Average Episode Reward: -0.4325\n",
            "Average Episode Length: 6.31\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 62 ---\n",
            "--- Seed 62 Results ---\n",
            "Success Rate: 0.6000 (60.00%)\n",
            "Average Detection Time (Successful Episodes): 4.12\n",
            "Average Episode Reward: -0.5065\n",
            "Average Episode Length: 6.47\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 63 ---\n",
            "--- Seed 63 Results ---\n",
            "Success Rate: 0.5800 (58.00%)\n",
            "Average Detection Time (Successful Episodes): 4.47\n",
            "Average Episode Reward: -0.5525\n",
            "Average Episode Length: 6.79\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 64 ---\n",
            "--- Seed 64 Results ---\n",
            "Success Rate: 0.6900 (69.00%)\n",
            "Average Detection Time (Successful Episodes): 4.55\n",
            "Average Episode Reward: -0.3695\n",
            "Average Episode Length: 6.24\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 65 ---\n",
            "--- Seed 65 Results ---\n",
            "Success Rate: 0.6600 (66.00%)\n",
            "Average Detection Time (Successful Episodes): 4.36\n",
            "Average Episode Reward: -0.4035\n",
            "Average Episode Length: 6.28\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 66 ---\n",
            "--- Seed 66 Results ---\n",
            "Success Rate: 0.6100 (61.00%)\n",
            "Average Detection Time (Successful Episodes): 3.49\n",
            "Average Episode Reward: -0.4090\n",
            "Average Episode Length: 6.03\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 67 ---\n",
            "--- Seed 67 Results ---\n",
            "Success Rate: 0.6400 (64.00%)\n",
            "Average Detection Time (Successful Episodes): 4.73\n",
            "Average Episode Reward: -0.4710\n",
            "Average Episode Length: 6.63\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 68 ---\n",
            "--- Seed 68 Results ---\n",
            "Success Rate: 0.5900 (59.00%)\n",
            "Average Detection Time (Successful Episodes): 4.56\n",
            "Average Episode Reward: -0.5620\n",
            "Average Episode Length: 6.79\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 69 ---\n",
            "--- Seed 69 Results ---\n",
            "Success Rate: 0.6100 (61.00%)\n",
            "Average Detection Time (Successful Episodes): 4.48\n",
            "Average Episode Reward: -0.5135\n",
            "Average Episode Length: 6.63\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 70 ---\n",
            "--- Seed 70 Results ---\n",
            "Success Rate: 0.6700 (67.00%)\n",
            "Average Detection Time (Successful Episodes): 3.96\n",
            "Average Episode Reward: -0.3440\n",
            "Average Episode Length: 5.95\n",
            "Time taken for this seed: 0.03 seconds\n",
            "\n",
            "--- Running with Seed: 71 ---\n",
            "--- Seed 71 Results ---\n",
            "Success Rate: 0.6600 (66.00%)\n",
            "Average Detection Time (Successful Episodes): 4.17\n",
            "Average Episode Reward: -0.3725\n",
            "Average Episode Length: 6.15\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 72 ---\n",
            "--- Seed 72 Results ---\n",
            "Success Rate: 0.5900 (59.00%)\n",
            "Average Detection Time (Successful Episodes): 4.24\n",
            "Average Episode Reward: -0.5315\n",
            "Average Episode Length: 6.60\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 73 ---\n",
            "--- Seed 73 Results ---\n",
            "Success Rate: 0.6000 (60.00%)\n",
            "Average Detection Time (Successful Episodes): 3.82\n",
            "Average Episode Reward: -0.4595\n",
            "Average Episode Length: 6.29\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 74 ---\n",
            "--- Seed 74 Results ---\n",
            "Success Rate: 0.7100 (71.00%)\n",
            "Average Detection Time (Successful Episodes): 4.44\n",
            "Average Episode Reward: -0.3185\n",
            "Average Episode Length: 6.05\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 75 ---\n",
            "--- Seed 75 Results ---\n",
            "Success Rate: 0.6100 (61.00%)\n",
            "Average Detection Time (Successful Episodes): 4.67\n",
            "Average Episode Reward: -0.5385\n",
            "Average Episode Length: 6.75\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 76 ---\n",
            "--- Seed 76 Results ---\n",
            "Success Rate: 0.5200 (52.00%)\n",
            "Average Detection Time (Successful Episodes): 3.58\n",
            "Average Episode Reward: -0.5865\n",
            "Average Episode Length: 6.66\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 77 ---\n",
            "--- Seed 77 Results ---\n",
            "Success Rate: 0.6300 (63.00%)\n",
            "Average Detection Time (Successful Episodes): 4.22\n",
            "Average Episode Reward: -0.4290\n",
            "Average Episode Length: 6.36\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 78 ---\n",
            "--- Seed 78 Results ---\n",
            "Success Rate: 0.5700 (57.00%)\n",
            "Average Detection Time (Successful Episodes): 4.63\n",
            "Average Episode Reward: -0.5735\n",
            "Average Episode Length: 6.94\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 79 ---\n",
            "--- Seed 79 Results ---\n",
            "Success Rate: 0.6600 (66.00%)\n",
            "Average Detection Time (Successful Episodes): 4.02\n",
            "Average Episode Reward: -0.3615\n",
            "Average Episode Length: 6.05\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 80 ---\n",
            "--- Seed 80 Results ---\n",
            "Success Rate: 0.7100 (71.00%)\n",
            "Average Detection Time (Successful Episodes): 4.70\n",
            "Average Episode Reward: -0.3370\n",
            "Average Episode Length: 6.24\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 81 ---\n",
            "--- Seed 81 Results ---\n",
            "Success Rate: 0.6900 (69.00%)\n",
            "Average Detection Time (Successful Episodes): 4.28\n",
            "Average Episode Reward: -0.3070\n",
            "Average Episode Length: 6.05\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 82 ---\n",
            "--- Seed 82 Results ---\n",
            "Success Rate: 0.5700 (57.00%)\n",
            "Average Detection Time (Successful Episodes): 4.58\n",
            "Average Episode Reward: -0.5715\n",
            "Average Episode Length: 6.91\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 83 ---\n",
            "--- Seed 83 Results ---\n",
            "Success Rate: 0.5800 (58.00%)\n",
            "Average Detection Time (Successful Episodes): 4.34\n",
            "Average Episode Reward: -0.5560\n",
            "Average Episode Length: 6.72\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 84 ---\n",
            "--- Seed 84 Results ---\n",
            "Success Rate: 0.6000 (60.00%)\n",
            "Average Detection Time (Successful Episodes): 4.30\n",
            "Average Episode Reward: -0.5040\n",
            "Average Episode Length: 6.58\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 85 ---\n",
            "--- Seed 85 Results ---\n",
            "Success Rate: 0.6200 (62.00%)\n",
            "Average Detection Time (Successful Episodes): 3.90\n",
            "Average Episode Reward: -0.4190\n",
            "Average Episode Length: 6.22\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 86 ---\n",
            "--- Seed 86 Results ---\n",
            "Success Rate: 0.5700 (57.00%)\n",
            "Average Detection Time (Successful Episodes): 4.35\n",
            "Average Episode Reward: -0.5805\n",
            "Average Episode Length: 6.78\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Running with Seed: 87 ---\n",
            "--- Seed 87 Results ---\n",
            "Success Rate: 0.6100 (61.00%)\n",
            "Average Detection Time (Successful Episodes): 3.61\n",
            "Average Episode Reward: -0.4160\n",
            "Average Episode Length: 6.10\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 88 ---\n",
            "--- Seed 88 Results ---\n",
            "Success Rate: 0.6300 (63.00%)\n",
            "Average Detection Time (Successful Episodes): 4.48\n",
            "Average Episode Reward: -0.4710\n",
            "Average Episode Length: 6.52\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 89 ---\n",
            "--- Seed 89 Results ---\n",
            "Success Rate: 0.6100 (61.00%)\n",
            "Average Detection Time (Successful Episodes): 4.28\n",
            "Average Episode Reward: -0.5020\n",
            "Average Episode Length: 6.51\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 90 ---\n",
            "--- Seed 90 Results ---\n",
            "Success Rate: 0.6000 (60.00%)\n",
            "Average Detection Time (Successful Episodes): 4.73\n",
            "Average Episode Reward: -0.5615\n",
            "Average Episode Length: 6.84\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 91 ---\n",
            "--- Seed 91 Results ---\n",
            "Success Rate: 0.6000 (60.00%)\n",
            "Average Detection Time (Successful Episodes): 4.45\n",
            "Average Episode Reward: -0.5310\n",
            "Average Episode Length: 6.67\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 92 ---\n",
            "--- Seed 92 Results ---\n",
            "Success Rate: 0.6200 (62.00%)\n",
            "Average Detection Time (Successful Episodes): 4.32\n",
            "Average Episode Reward: -0.4730\n",
            "Average Episode Length: 6.48\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 93 ---\n",
            "--- Seed 93 Results ---\n",
            "Success Rate: 0.6800 (68.00%)\n",
            "Average Detection Time (Successful Episodes): 3.91\n",
            "Average Episode Reward: -0.3220\n",
            "Average Episode Length: 5.86\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 94 ---\n",
            "--- Seed 94 Results ---\n",
            "Success Rate: 0.6100 (61.00%)\n",
            "Average Detection Time (Successful Episodes): 3.87\n",
            "Average Episode Reward: -0.4385\n",
            "Average Episode Length: 6.26\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 95 ---\n",
            "--- Seed 95 Results ---\n",
            "Success Rate: 0.6800 (68.00%)\n",
            "Average Detection Time (Successful Episodes): 4.22\n",
            "Average Episode Reward: -0.3335\n",
            "Average Episode Length: 6.07\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 96 ---\n",
            "--- Seed 96 Results ---\n",
            "Success Rate: 0.6200 (62.00%)\n",
            "Average Detection Time (Successful Episodes): 4.40\n",
            "Average Episode Reward: -0.4720\n",
            "Average Episode Length: 6.53\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 97 ---\n",
            "--- Seed 97 Results ---\n",
            "Success Rate: 0.6300 (63.00%)\n",
            "Average Detection Time (Successful Episodes): 4.10\n",
            "Average Episode Reward: -0.4325\n",
            "Average Episode Length: 6.28\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 98 ---\n",
            "--- Seed 98 Results ---\n",
            "Success Rate: 0.6300 (63.00%)\n",
            "Average Detection Time (Successful Episodes): 4.21\n",
            "Average Episode Reward: -0.4415\n",
            "Average Episode Length: 6.35\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Running with Seed: 99 ---\n",
            "--- Seed 99 Results ---\n",
            "Success Rate: 0.6000 (60.00%)\n",
            "Average Detection Time (Successful Episodes): 3.82\n",
            "Average Episode Reward: -0.4615\n",
            "Average Episode Length: 6.29\n",
            "Time taken for this seed: 0.01 seconds\n",
            "\n",
            "--- Overall Adaptive Re-planning Policy Results ---\n",
            "Total experiment time across 100 seeds: 3.12 seconds\n",
            "Average Success Rate over 100 seeds and 100 episodes each: 0.6263 (62.63%)\n",
            "Standard Deviation of Success Rate: 0.0439\n",
            "Average Detection Time (Successful Episodes) across all seeds: 4.26\n",
            "Standard Deviation of Detection Time (Successful Episodes) across all seeds: 2.43\n",
            "Average Episode Reward across all seeds: -0.4536\n",
            "Standard Deviation of Reward: 0.9729\n",
            "Average Episode Length across all seeds: 6.40\n",
            "Standard Deviation of Episode Length: 3.38\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from functools import lru_cache\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import time\n",
        "\n",
        "# Paste the entire TwoObjectDPSolver class definition here\n",
        "class TwoObjectDPSolver:\n",
        "    \"\"\"\n",
        "    An exact Dynamic Programming solver for the two-object search problem.\n",
        "\n",
        "    This class uses backward recursion to find the optimal policy and the\n",
        "    maximum probability of detecting the primary target (O1).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n: int, T: int, p0: np.ndarray, gamma1: np.ndarray, gamma2: np.ndarray, c: np.ndarray):\n",
        "        \"\"\"\n",
        "        Initializes the solver with the problem parameters.\n",
        "\n",
        "        Args:\n",
        "            n (int): The number of cells.\n",
        "            T (int): The time horizon.\n",
        "            p0 (np.ndarray): The n x n prior joint probability matrix.\n",
        "            gamma1 (np.ndarray): Miss-detection rates for O1.\n",
        "            gamma2 (np.ndarray): Miss-detection rates for O2.\n",
        "            c (np.ndarray): The cost vector for actions.\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.T = T\n",
        "        self.p0 = p0\n",
        "        self.gamma1 = gamma1\n",
        "        self.gamma2 = gamma2\n",
        "        self.c = c\n",
        "\n",
        "        # Data structures to store the results\n",
        "        self.J_values: Dict[int, Dict[Tuple, float]] = {}  # Value function J_t(s_t)\n",
        "        self.Policy: Dict[int, Dict[Tuple, int]] = {}      # Policy mu_t(s_t)\n",
        "\n",
        "        # Pre-calculate initial conditional priors for efficiency\n",
        "        self._p0_conditionals = self._precompute_p0_conditionals()\n",
        "\n",
        "    def _precompute_p0_conditionals(self) -> List[np.ndarray]:\n",
        "        \"\"\"Pre-calculates P(O1=i | O2=k) for all k.\"\"\"\n",
        "        conditionals = []\n",
        "        for k in range(self.n):\n",
        "            marginal_o2 = np.sum(self.p0[:, k])\n",
        "            if marginal_o2 > 0:\n",
        "                conditionals.append(self.p0[:, k] / marginal_o2)\n",
        "            else:\n",
        "                # If O2 can never be in cell k, the conditional is undefined. Use zeros.\n",
        "                conditionals.append(np.zeros(self.n))\n",
        "        return conditionals\n",
        "\n",
        "    @staticmethod\n",
        "    @lru_cache(maxsize=None) # Memoization for performance\n",
        "    def _generate_z_vectors(t: int, n: int) -> List[Tuple[int, ...]]:\n",
        "        \"\"\"Recursively generates all state vectors z_t where sum(z_i) = t.\"\"\"\n",
        "        if n == 1:\n",
        "            return [(t,)]\n",
        "\n",
        "        vectors = []\n",
        "        for i in range(t + 1):\n",
        "            for sub_vector in TwoObjectDPSolver._generate_z_vectors(t - i, n - 1):\n",
        "                vectors.append((i,) + sub_vector)\n",
        "        \"\"\"if n == 5:\n",
        "           for vec in vectors:\n",
        "               print(vec)\"\"\"\n",
        "        return vectors\n",
        "\n",
        "\n",
        "    def _calculate_joint_posterior(self, z_vector: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Calculates p(O1=i, O2=j | z).\"\"\"\n",
        "        g1_z = np.power(self.gamma1, z_vector)\n",
        "        g2_z = np.power(self.gamma2, z_vector)\n",
        "        likelihood = np.outer(g1_z, g2_z)\n",
        "        numerator = self.p0 * likelihood\n",
        "        norm = np.sum(numerator)\n",
        "        return numerator / norm if norm > 0 else numerator\n",
        "\n",
        "    def _calculate_conditional_posterior(self, z_vector: np.ndarray, o2_loc: int) -> np.ndarray:\n",
        "        \"\"\"Calculates p(O1=i | z, O2=o2_loc).\"\"\"\n",
        "        p0_cond = self._p0_conditionals[o2_loc]\n",
        "        g1_z = np.power(self.gamma1, z_vector)\n",
        "        numerator = g1_z * p0_cond\n",
        "        norm = np.sum(numerator)\n",
        "        return numerator / norm if norm > 0 else numerator\n",
        "\n",
        "    def solve(self):\n",
        "        \"\"\"\n",
        "        Executes the backward recursion to solve the DP problem.\n",
        "        \"\"\"\n",
        "        #print(\"Starting DP solver...\")\n",
        "        # --- Initialization at T ---\n",
        "        #print(f\"Initializing for T={self.T}...\")\n",
        "        self.J_values[self.T] = {}\n",
        "        z_vectors_T = self._generate_z_vectors(self.T, self.n)\n",
        "        for z_T in z_vectors_T:\n",
        "            for theta2 in range(self.n + 1):\n",
        "                state = (z_T, theta2)\n",
        "                self.J_values[self.T][state] = 0.0\n",
        "\n",
        "\n",
        "        # --- Backward Recursion ---\n",
        "        for t in range(self.T - 1, -1, -1):\n",
        "            start_time = time.time()\n",
        "            J_t = {}\n",
        "            policy_t = {}\n",
        "            z_vectors_t = self._generate_z_vectors(t, self.n)\n",
        "\n",
        "\n",
        "            for z_t_tuple in z_vectors_t:\n",
        "                z_t = np.array(z_t_tuple)\n",
        "                for theta2 in range(self.n + 1):\n",
        "                    if t == 0 and theta2 > 0:\n",
        "                        break\n",
        "                    current_state = (z_t_tuple, theta2)\n",
        "                    action_values = []\n",
        "\n",
        "                    # Calculate belief based on current state\n",
        "                    if theta2 == 0:\n",
        "                        belief = self._calculate_joint_posterior(z_t)\n",
        "                    else:\n",
        "                        o2_loc = theta2 - 1\n",
        "                        belief = self._calculate_conditional_posterior(z_t, o2_loc)\n",
        "\n",
        "                    # Iterate over all possible actions\n",
        "                    for a_t in range(self.n):\n",
        "                        next_z_tuple = tuple(z_t + np.eye(self.n, dtype=int)[a_t])\n",
        "\n",
        "                        # --- Case 1: Both objects hidden ---\n",
        "                        if theta2 == 0:\n",
        "                            p_marginal_o1_at_a = np.sum(belief[a_t, :])\n",
        "\n",
        "                            # Prob of success (finding O1)\n",
        "                            p_success = (1 - self.gamma1[a_t]) * p_marginal_o1_at_a\n",
        "\n",
        "                            # Prob of finding O2 only\n",
        "                            p_cond_sum = np.sum(belief[np.arange(self.n) != a_t, a_t])\n",
        "                            p_find_o2_only = (1 - self.gamma2[a_t]) * (self.gamma1[a_t] * belief[a_t, a_t] + p_cond_sum)\n",
        "\n",
        "\n",
        "                            # Prob of finding nothing\n",
        "                            p_nothing = 1 - p_success - p_find_o2_only\n",
        "\n",
        "                            # Future values from next stage\n",
        "                            val_if_nothing = self.J_values[t + 1][(next_z_tuple, 0)]\n",
        "                            val_if_found_o2 = self.J_values[t + 1][(next_z_tuple, a_t + 1)]\n",
        "\n",
        "                            # Expected value for this action\n",
        "                            expected_value = (p_success * 1.0) - self.c[a_t] + \\\n",
        "                                             (p_nothing * val_if_nothing) + \\\n",
        "                                             (p_find_o2_only * val_if_found_o2)\n",
        "                            action_values.append(expected_value)\n",
        "\n",
        "                        # --- Case 2: O2 has been found ---\n",
        "                        else:\n",
        "                            p_o1_at_a = belief[a_t]\n",
        "                            p_success = (1 - self.gamma1[a_t]) * p_o1_at_a\n",
        "                            p_fail = 1 - p_success\n",
        "\n",
        "                            val_if_fail = self.J_values[t + 1][(next_z_tuple, theta2)]\n",
        "\n",
        "                            expected_value = (p_success * 1.0) - self.c[a_t] + (p_fail * val_if_fail)\n",
        "                            action_values.append(expected_value)\n",
        "\n",
        "\n",
        "                    # Find best action and store value/policy\n",
        "                    best_value = np.max(action_values)\n",
        "                    best_action = np.argmax(action_values)\n",
        "                    J_t[current_state] = best_value\n",
        "                    policy_t[current_state] = best_action\n",
        "\n",
        "            self.J_values[t] = J_t\n",
        "            self.Policy[t] = policy_t\n",
        "            end_time = time.time()\n",
        "            #print(f\"Completed t={t}. Found {len(z_vectors_t)} states. Took {end_time - start_time:.2f}s.\")\n",
        "\n",
        "        #print(\"DP solver finished.\")\n",
        "\n",
        "    def get_optimal_value(self) -> float:\n",
        "        \"\"\"Returns the optimal value J(s_0).\"\"\"\n",
        "        initial_z = tuple([0] * self.n)\n",
        "        initial_state = (initial_z, 0)\n",
        "        return self.J_values[0][initial_state]\n",
        "\n",
        "\n",
        "# Assume SingleObjectDPSolver and TwoObjectDPSolver are defined elsewhere and available\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Adaptive Re-planning Episode Simulation (Copied from e87hajzv5p50)\n",
        "# =============================================================================\n",
        "\n",
        "def run_adaptive_replanning_episode(n: int, T: int, p0_joint: np.ndarray,\n",
        "                                    gamma1: np.ndarray, gamma2: np.ndarray,\n",
        "                                    initial_two_obj_policy: Dict, c: np.ndarray, episode_seed: int) -> Tuple[bool, int, float, int]:\n",
        "    \"\"\"\n",
        "    Simulates a single episode using a given two-object DP policy with adaptive re-planning if O2 is found.\n",
        "    Returns success status, time of detection (or -1 if failed), accumulated reward, and episode length.\n",
        "    \"\"\"\n",
        "    # Create a local RNG for this episode for statistical independence\n",
        "    rng = np.random.default_rng(episode_seed)\n",
        "\n",
        "    # 1. Secretly determine the true locations\n",
        "    flat_p0 = p0_joint.flatten()\n",
        "    if np.sum(flat_p0) == 0: # Handle case where prior is all zeros\n",
        "         true_pos_o1, true_pos_o2 = -1, -1 # Indicate no target\n",
        "    else:\n",
        "        choice_index = rng.choice(n * n, p=flat_p0 / np.sum(flat_p0)) # Normalize using local RNG\n",
        "        true_pos_o1, true_pos_o2 = np.unravel_index(choice_index, (n, n))\n",
        "\n",
        "\n",
        "    # 2. Initialize state and reward\n",
        "    z_vector = np.zeros(n, dtype=int)\n",
        "    theta2 = 0\n",
        "    current_policy = initial_two_obj_policy # Use the pre-computed initial policy\n",
        "    accumulated_reward = 0.0\n",
        "    detection_time = -1\n",
        "    episode_length = 0\n",
        "\n",
        "    # 3. Run the search for T steps\n",
        "    for t in range(T):\n",
        "        episode_length += 1 # Increment episode length at each time step\n",
        "        # --- Get action from the current policy ---\n",
        "        state = (tuple(z_vector), theta2) # State is always (z_vector, theta2)\n",
        "        if t not in current_policy or state not in current_policy[t]:\n",
        "             print(f\"Warning: Policy not found for state {state} at time {t}. Using greedy action (fallback).\")\n",
        "             # Fallback to greedy action if policy is missing (shouldn't happen with a full DP policy)\n",
        "             # For simplicity and assuming the initial_two_obj_policy is complete, we won't implement the fallback here\n",
        "             return False, detection_time, accumulated_reward, episode_length # End episode if policy missing\n",
        "\n",
        "\n",
        "        action = current_policy[t][state]\n",
        "\n",
        "\n",
        "        # Deduct cost for the action\n",
        "        accumulated_reward -= c[action]\n",
        "\n",
        "        # --- Simulate outcome of the action ---\n",
        "        found_o1 = (action == true_pos_o1) and (rng.random() > gamma1[action]) # Use local RNG\n",
        "        if found_o1:\n",
        "            accumulated_reward += 1.0 # Reward for finding O1\n",
        "            detection_time = t + 1 # Time is absolute\n",
        "            return True, detection_time, accumulated_reward, episode_length # Mission Success!\n",
        "\n",
        "        # --- Check if we found O2 ---\n",
        "        if theta2 == 0 and action == true_pos_o2 and (rng.random() > gamma2[action]): # Use local RNG\n",
        "            #print(f\"    (Episode Info) O2 found at t={t} in cell {action}. Updating state.\")\n",
        "            theta2 = action + 1 # Update theta2\n",
        "\n",
        "\n",
        "        # Update state for the next time step\n",
        "        z_vector[action] += 1\n",
        "\n",
        "\n",
        "    return False, detection_time, accumulated_reward, episode_length # Mission Failed\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 2. MAIN EXPERIMENT SCRIPT (Modified for multiple seeds)\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Problem Definition ---\n",
        "    NUM_CELLS = 5\n",
        "    TIME_HORIZON = 10\n",
        "    NUM_EPISODES_PER_SEED = 100 # Number of episodes to run for each seed\n",
        "    NUM_SEEDS = 100 # Number of different seeds to run\n",
        "\n",
        "    # Problem parameters (same for all seeds)\n",
        "    prior = np.array([\n",
        "        [0.152,  0.0039, 0.003,  0.0108, 0.011],\n",
        "        [0.0038, 0.0052, 0.117,  0.0162, 0.165],\n",
        "        [0.0057, 0.195,  0.015,  0.009,  0.011],\n",
        "        [0.0038, 0.0091, 0.0075, 0.027,  0.011],\n",
        "        [0.0247, 0.0468, 0.0075, 0.117,  0.022]\n",
        "    ])\n",
        "\n",
        "    gammas1 = np.array([0.8, 0.65, 0.82, 0.75, 0.7])\n",
        "    gammas2 = np.array([0.2, 0.1, 0.25, 0.15, 0.2])\n",
        "    c= np.array([0.15,0.2,0.25,0.1,0.2])\n",
        "\n",
        "    all_success_rates = []\n",
        "    all_detection_times = []\n",
        "    all_rewards = []\n",
        "    all_episode_lengths = [] # New list to store episode lengths\n",
        "    total_experiment_start_time = time.time()\n",
        "\n",
        "    print(\"--- Running Empirical Experiment for Adaptive Re-planning Policy over Multiple Seeds ---\")\n",
        "    print(f\"Number of seeds: {NUM_SEEDS}\")\n",
        "    print(f\"Number of evaluation episodes per seed: {NUM_EPISODES_PER_SEED}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "\n",
        "    # --- Pre-computation Phase (initial policy calculation - done once) ---\n",
        "    print(\"--- Pre-computing the initial Two-Object DP policy ---\")\n",
        "    initial_solver = TwoObjectDPSolver(NUM_CELLS, TIME_HORIZON, prior, gammas1, gammas2, c)\n",
        "    initial_solver.solve()\n",
        "    initial_policy = initial_solver.Policy\n",
        "    print(\"probility comuted :\", initial_solver.J_values[0])\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "\n",
        "    for seed in range(NUM_SEEDS):\n",
        "        print(f\"\\n--- Running with Seed: {seed} ---\")\n",
        "        # np.random.seed(seed) # Removed global seeding here to rely on per-episode seeding\n",
        "\n",
        "        # --- Run the Experiment for this seed ---\n",
        "        num_successes = 0\n",
        "        detection_times_this_seed = []\n",
        "        rewards_this_seed = []\n",
        "        episode_lengths_this_seed = [] # New list for this seed\n",
        "        start_time_this_seed = time.time()\n",
        "\n",
        "        for i in range(NUM_EPISODES_PER_SEED):\n",
        "            # The run_adaptive_replanning_episode function now handles true location sampling internally\n",
        "            # It also now uses the pre-computed initial_policy directly\n",
        "            # Pass a unique seed for each episode to ensure statistical independence\n",
        "            episode_unique_seed = seed * NUM_EPISODES_PER_SEED + i\n",
        "            success, detection_time, reward, episode_length = run_adaptive_replanning_episode(\n",
        "                NUM_CELLS, TIME_HORIZON, prior, gammas1, gammas2, initial_policy, c, episode_unique_seed\n",
        "            )\n",
        "            rewards_this_seed.append(reward) # Append reward to the list\n",
        "            episode_lengths_this_seed.append(episode_length) # Append episode length\n",
        "\n",
        "            if success:\n",
        "                num_successes += 1\n",
        "                detection_times_this_seed.append(detection_time)\n",
        "                #print(f\"    (Episode {i+1} Result) SUCCESS at time {detection_time} with reward {reward:.4f}\")\n",
        "            #else:\n",
        "                #print(f\"    (Episode {i+1} Result) FAILURE with reward {reward:.4f}\")\n",
        "\n",
        "\n",
        "        end_time_this_seed = time.time()\n",
        "        success_rate_this_seed = num_successes / NUM_EPISODES_PER_SEED\n",
        "        all_success_rates.append(success_rate_this_seed)\n",
        "        all_detection_times.extend(detection_times_this_seed)\n",
        "        all_rewards.extend(rewards_this_seed)\n",
        "        all_episode_lengths.extend(episode_lengths_this_seed) # Extend overall list\n",
        "\n",
        "\n",
        "        print(f\"--- Seed {seed} Results ---\")\n",
        "        print(f\"Success Rate: {success_rate_this_seed:.4f} ({success_rate_this_seed*100:.2f}%)\")\n",
        "        if detection_times_this_seed:\n",
        "            print(f\"Average Detection Time (Successful Episodes): {np.mean(detection_times_this_seed):.2f}\")\n",
        "        else:\n",
        "            print(\"Average Detection Time (Successful Episodes): N/A (no successes)\")\n",
        "        print(f\"Average Episode Reward: {np.mean(rewards_this_seed):.4f}\")\n",
        "        print(f\"Average Episode Length: {np.mean(episode_lengths_this_seed):.2f}\") # Display average episode length\n",
        "        print(f\"Time taken for this seed: {end_time_this_seed - start_time_this_seed:.2f} seconds\")\n",
        "\n",
        "\n",
        "    total_experiment_end_time = time.time()\n",
        "\n",
        "    # --- 3.4. Display Final Results ---\n",
        "    mean_success_rate = np.mean(all_success_rates)\n",
        "    std_success_rate = np.std(all_success_rates)\n",
        "    mean_detection_time = np.mean(all_detection_times) if all_detection_times else -1\n",
        "    std_detection_time = np.std(all_detection_times) if all_detection_times else -1\n",
        "    mean_reward = np.mean(all_rewards)\n",
        "    std_reward = np.std(all_rewards)\n",
        "    mean_episode_length = np.mean(all_episode_lengths) # Calculate overall average episode length\n",
        "    std_episode_length = np.std(all_episode_lengths) # Calculate overall standard deviation of episode length\n",
        "\n",
        "\n",
        "    print(\"\\n--- Overall Adaptive Re-planning Policy Results ---\")\n",
        "    print(f\"Total experiment time across {NUM_SEEDS} seeds: {total_experiment_end_time - total_experiment_start_time:.2f} seconds\")\n",
        "    print(f\"Average Success Rate over {NUM_SEEDS} seeds and {NUM_EPISODES_PER_SEED} episodes each: {mean_success_rate:.4f} ({mean_success_rate*100:.2f}%)\")\n",
        "    print(f\"Standard Deviation of Success Rate: {std_success_rate:.4f}\")\n",
        "    if all_detection_times:\n",
        "        print(f\"Average Detection Time (Successful Episodes) across all seeds: {mean_detection_time:.2f}\")\n",
        "        print(f\"Standard Deviation of Detection Time (Successful Episodes) across all seeds: {std_detection_time:.2f}\")\n",
        "    else:\n",
        "        print(\"Average Detection Time (Successful Episodes) across all seeds: N/A (no successes)\")\n",
        "\n",
        "    print(f\"Average Episode Reward across all seeds: {mean_reward:.4f}\")\n",
        "    print(f\"Standard Deviation of Reward: {std_reward:.4f}\")\n",
        "    print(f\"Average Episode Length across all seeds: {mean_episode_length:.2f}\") # Display overall average episode length\n",
        "    print(f\"Standard Deviation of Episode Length: {std_episode_length:.2f}\") # Display overall standard deviation of episode length\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LyGrjBqNum62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CNgLojO6pzq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8586ed1-9ccb-4dc3-9c84-87485c29667d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Empirical Experiment for k-step lookahead Policy over Multiple Seeds ---\n",
            "Number of seeds: 10\n",
            "Number of evaluation episodes per seed: 10\n",
            "Lookahead depth (k): 1\n",
            "Rollout simulations: 10\n",
            "--------------------\n",
            "\n",
            "--- Running with Seed: 0 ---\n",
            "--- Seed 0 Results ---\n",
            "Success Rate: 0.2000 (20.00%)\n",
            "Average Detection Time (Successful Episodes): 13.00\n",
            "Average Episode Reward: -2.7656\n",
            "Average Episode Length: 18.60\n",
            "Time taken for this seed: 222.41 seconds\n",
            "\n",
            "--- Running with Seed: 1 ---\n",
            "--- Seed 1 Results ---\n",
            "Success Rate: 0.0000 (0.00%)\n",
            "Average Detection Time (Successful Episodes): N/A (no successes)\n",
            "Average Episode Reward: -2.5895\n",
            "Average Episode Length: 20.00\n",
            "Time taken for this seed: 213.83 seconds\n",
            "\n",
            "--- Running with Seed: 2 ---\n",
            "--- Seed 2 Results ---\n",
            "Success Rate: 0.2000 (20.00%)\n",
            "Average Detection Time (Successful Episodes): 9.50\n",
            "Average Episode Reward: -2.7598\n",
            "Average Episode Length: 17.90\n",
            "Time taken for this seed: 168.49 seconds\n",
            "\n",
            "--- Running with Seed: 3 ---\n",
            "--- Seed 3 Results ---\n",
            "Success Rate: 0.3000 (30.00%)\n",
            "Average Detection Time (Successful Episodes): 17.67\n",
            "Average Episode Reward: -3.0043\n",
            "Average Episode Length: 19.30\n",
            "Time taken for this seed: 193.14 seconds\n",
            "\n",
            "--- Running with Seed: 4 ---\n",
            "--- Seed 4 Results ---\n",
            "Success Rate: 0.2000 (20.00%)\n",
            "Average Detection Time (Successful Episodes): 12.00\n",
            "Average Episode Reward: -2.8634\n",
            "Average Episode Length: 18.40\n",
            "Time taken for this seed: 205.16 seconds\n",
            "\n",
            "--- Running with Seed: 5 ---\n",
            "--- Seed 5 Results ---\n",
            "Success Rate: 0.2000 (20.00%)\n",
            "Average Detection Time (Successful Episodes): 9.00\n",
            "Average Episode Reward: -2.6226\n",
            "Average Episode Length: 17.80\n",
            "Time taken for this seed: 185.66 seconds\n",
            "\n",
            "--- Running with Seed: 6 ---\n",
            "--- Seed 6 Results ---\n",
            "Success Rate: 0.3000 (30.00%)\n",
            "Average Detection Time (Successful Episodes): 7.33\n",
            "Average Episode Reward: -2.5375\n",
            "Average Episode Length: 16.20\n",
            "Time taken for this seed: 191.08 seconds\n",
            "\n",
            "--- Running with Seed: 7 ---\n",
            "--- Seed 7 Results ---\n",
            "Success Rate: 0.2000 (20.00%)\n",
            "Average Detection Time (Successful Episodes): 12.00\n",
            "Average Episode Reward: -2.6568\n",
            "Average Episode Length: 18.40\n",
            "Time taken for this seed: 197.23 seconds\n",
            "\n",
            "--- Running with Seed: 8 ---\n",
            "--- Seed 8 Results ---\n",
            "Success Rate: 0.3000 (30.00%)\n",
            "Average Detection Time (Successful Episodes): 14.00\n",
            "Average Episode Reward: -3.2408\n",
            "Average Episode Length: 18.20\n",
            "Time taken for this seed: 191.99 seconds\n",
            "\n",
            "--- Running with Seed: 9 ---\n",
            "--- Seed 9 Results ---\n",
            "Success Rate: 0.3000 (30.00%)\n",
            "Average Detection Time (Successful Episodes): 7.33\n",
            "Average Episode Reward: -2.3425\n",
            "Average Episode Length: 16.20\n",
            "Time taken for this seed: 164.64 seconds\n",
            "\n",
            "--- Overall 1-step Lookahead Policy Results ---\n",
            "Total experiment time across 10 seeds: 1933.66 seconds\n",
            "Average Success Rate over 10 seeds and 10 episodes each: 0.2200 (22.00%)\n",
            "Standard Deviation of Success Rate: 0.0872\n",
            "Average Detection Time (Successful Episodes) across all seeds: 11.36\n",
            "Standard Deviation of Detection Time: 6.18\n",
            "Average Episode Reward across all seeds: -2.7383\n",
            "Standard Deviation of Reward: 1.0847\n",
            "Average Episode Length across all seeds: 18.10\n",
            "Standard Deviation of Episode Length: 4.61\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from functools import lru_cache\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import time\n",
        "\n",
        "# --- Start of SingleObjectDPSolver (copied from txTdQaQVhkrA) ---\n",
        "class SingleObjectDPSolver:\n",
        "    \"\"\"\n",
        "    An exact Dynamic Programming solver for a single-object search problem.\n",
        "\n",
        "    This class uses backward recursion to find the optimal policy based on a\n",
        "    marginal prior distribution for a single target.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n: int, T: int, p0_marginal: np.ndarray, gamma1: np.ndarray, c: np.ndarray):\n",
        "        \"\"\"\n",
        "        Initializes the solver with the single-object problem parameters.\n",
        "\n",
        "        Args:\n",
        "            n (int): The number of cells.\n",
        "            T (int): The time horizon.\n",
        "            p0_marginal (np.ndarray): The n-element marginal prior probability vector for O1.\n",
        "            gamma1 (np.ndarray): Miss-detection rates for O1.\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.T = T\n",
        "        self.p0_marginal = p0_marginal\n",
        "        self.gamma1 = gamma1\n",
        "        self.c = c\n",
        "\n",
        "        # Data structures to store the results\n",
        "        self.J_values: Dict[int, Dict[Tuple[int, ...], float]] = {}  # Value function J_t(z_t)\n",
        "        self.Policy: Dict[int, Dict[Tuple[int, ...], int]] = {}      # Policy mu_t(z_t)\n",
        "\n",
        "    @staticmethod\n",
        "    @lru_cache(maxsize=None) # Memoization for performance\n",
        "    def _generate_z_vectors(t: int, n: int) -> List[Tuple[int, ...]]:\n",
        "        \"\"\"Recursively generates all state vectors z_t where sum(z_i) = t.\"\"\"\n",
        "        if n == 1:\n",
        "            return [(t,)]\n",
        "\n",
        "        vectors = []\n",
        "        for i in range(t + 1):\n",
        "            for sub_vector in SingleObjectDPSolver._generate_z_vectors(t - i, n - 1):\n",
        "                vectors.append((i,) + sub_vector)\n",
        "        return vectors\n",
        "\n",
        "    def _calculate_posterior(self, z_vector: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Calculates the posterior belief p(O1=i | z).\"\"\"\n",
        "        g1_z = np.power(self.gamma1, z_vector)\n",
        "        numerator = self.p0_marginal * g1_z\n",
        "        norm = np.sum(numerator)\n",
        "        return numerator / norm if norm > 0 else numerator\n",
        "\n",
        "    def solve(self):\n",
        "        \"\"\"\n",
        "        Executes the backward recursion to solve the DP problem.\n",
        "        \"\"\"\n",
        "        # print(\"Starting Single-Object DP solver...\")\n",
        "        # --- Initialization at T ---\n",
        "        # print(f\"Initializing for T={self.T}...\")\n",
        "        self.J_values[self.T] = {}\n",
        "        z_vectors_T = self._generate_z_vectors(self.T, self.n)\n",
        "        for z_T in z_vectors_T:\n",
        "            self.J_values[self.T][z_T] = 0.0\n",
        "\n",
        "        # --- Backward Recursion ---\n",
        "        for t in range(self.T - 1, -1, -1):\n",
        "            #start_time = time.time()\n",
        "            J_t = {}\n",
        "            policy_t = {}\n",
        "            z_vectors_t = self._generate_z_vectors(t, self.n)\n",
        "\n",
        "            for z_t_tuple in z_vectors_t:\n",
        "                z_t = np.array(z_t_tuple)\n",
        "                action_values = []\n",
        "\n",
        "                # Calculate current belief vector\n",
        "                belief_t = self._calculate_posterior(z_t)\n",
        "\n",
        "                # Iterate over all possible actions\n",
        "                for a_t in range(self.n):\n",
        "                    # Probability of finding the target if we search cell a_t\n",
        "                    p_success = (1 - self.gamma1[a_t]) * belief_t[a_t]\n",
        "                    p_fail = 1 - ((1 - self.gamma1[a_t]) * belief_t[a_t])\n",
        "\n",
        "                    # Get the value of the state we transition to upon failure\n",
        "                    next_z_tuple = tuple(z_t + np.eye(self.n, dtype=int)[a_t])\n",
        "                    val_if_fail = self.J_values[t + 1][next_z_tuple]\n",
        "\n",
        "                    # Expected value for this action (Bellman equation)\n",
        "                    # Reward for success is 1, reward for failure is 0\n",
        "                    expected_value = (p_success * 1.0) - self.c[a_t] + (p_fail * val_if_fail)\n",
        "                    action_values.append(expected_value)\n",
        "\n",
        "                # Find best action and store value/policy\n",
        "                best_value = np.max(action_values)\n",
        "                best_action = np.argmax(action_values)\n",
        "                J_t[z_t_tuple] = best_value\n",
        "                policy_t[z_t_tuple] = best_action\n",
        "\n",
        "            self.J_values[t] = J_t\n",
        "            self.Policy[t] = policy_t\n",
        "            #end_time = time.time()\n",
        "            # print(f\"Completed t={t}. Found {len(z_vectors_t)} states. Took {end_time - start_time:.2f}s.\")\n",
        "\n",
        "        # print(\"DP solver finished.\")\n",
        "\n",
        "    def get_optimal_value(self) -> float:\n",
        "        \"\"\"Returns the optimal value J(z_0).\"\"\"\n",
        "        initial_z = tuple([0] * self.n)\n",
        "        return self.J_values[0][initial_z]\n",
        "# --- End of SingleObjectDPSolver ---\n",
        "\n",
        "\n",
        "# --- Start of TwoObjectDPSolver (copied from Uu36DFAN5dbB) ---\n",
        "class TwoObjectDPSolver:\n",
        "    \"\"\"\n",
        "    An exact Dynamic Programming solver for the two-object search problem.\n",
        "\n",
        "    This class uses backward recursion to find the optimal policy and the\n",
        "    maximum probability of detecting the primary target (O1).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n: int, T: int, p0: np.ndarray, gamma1: np.ndarray, gamma2: np.ndarray, c: np.ndarray):\n",
        "        \"\"\"\n",
        "        Initializes the solver with the problem parameters.\n",
        "\n",
        "        Args:\n",
        "            n (int): The number of cells.\n",
        "            T (int): The time horizon.\n",
        "            p0 (np.ndarray): The n x n prior joint probability matrix.\n",
        "            gamma1 (np.ndarray): Miss-detection rates for O1.\n",
        "            gamma2 (np.ndarray): Miss-detection rates for O2.\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.T = T\n",
        "        self.p0 = p0\n",
        "        self.gamma1 = gamma1\n",
        "        self.gamma2 = gamma2\n",
        "        self.c = c\n",
        "\n",
        "        # Data structures to store the results\n",
        "        self.J_values: Dict[int, Dict[Tuple, float]] = {}  # Value function J_t(s_t)\n",
        "        self.Policy: Dict[int, Dict[Tuple, int]] = {}      # Policy mu_t(s_t)\n",
        "\n",
        "        # Pre-calculate initial conditional priors for efficiency\n",
        "        self._p0_conditionals = self._precompute_p0_conditionals()\n",
        "\n",
        "    def _precompute_p0_conditionals(self) -> List[np.ndarray]:\n",
        "        \"\"\"Pre-calculates P(O1=i | O2=k) for all k.\"\"\"\n",
        "        conditionals = []\n",
        "        for k in range(self.n):\n",
        "            marginal_o2 = np.sum(self.p0[:, k])\n",
        "            if marginal_o2 > 0:\n",
        "                conditionals.append(self.p0[:, k] / marginal_o2)\n",
        "            else:\n",
        "                # If O2 can never be in cell k, the conditional is undefined. Use zeros.\n",
        "                conditionals.append(np.zeros(self.n))\n",
        "        return conditionals\n",
        "\n",
        "    @staticmethod\n",
        "    @lru_cache(maxsize=None) # Memoization for performance\n",
        "    def _generate_z_vectors(t: int, n: int) -> List[Tuple[int, ...]]:\n",
        "        \"\"\"Recursively generates all state vectors z_t where sum(z_i) = t.\"\"\"\n",
        "        if n == 1:\n",
        "            return [(t,)]\n",
        "\n",
        "        vectors = []\n",
        "        for i in range(t + 1):\n",
        "            for sub_vector in TwoObjectDPSolver._generate_z_vectors(t - i, n - 1):\n",
        "                vectors.append((i,) + sub_vector)\n",
        "        \"\"\"if n == 5:\n",
        "           for vec in vectors:\n",
        "               print(vec)\"\"\"\n",
        "        return vectors\n",
        "\n",
        "    def _calculate_joint_posterior(self, z_vector: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Calculates p(O1=i, O2=j | z).\"\"\"\n",
        "        g1_z = np.power(self.gamma1, z_vector)\n",
        "        g2_z = np.power(self.gamma2, z_vector)\n",
        "        likelihood = np.outer(g1_z, g2_z)\n",
        "        numerator = self.p0 * likelihood\n",
        "        norm = np.sum(numerator)\n",
        "        return numerator / norm if norm > 0 else numerator\n",
        "\n",
        "    def _calculate_conditional_posterior(self, z_vector: np.ndarray, o2_loc: int) -> np.ndarray:\n",
        "        \"\"\"Calculates p(O1=i | z, O2=o2_loc).\"\"\"\n",
        "        p0_cond = self._p0_conditionals[o2_loc]\n",
        "        g1_z = np.power(self.gamma1, z_vector)\n",
        "        numerator = g1_z * p0_cond\n",
        "        norm = np.sum(numerator)\n",
        "        return numerator / norm if norm > 0 else numerator\n",
        "\n",
        "    def solve(self):\n",
        "        \"\"\"\n",
        "        Executes the backward recursion to solve the DP problem.\n",
        "        \"\"\"\n",
        "        #print(\"Starting DP solver...\")\n",
        "        # --- Initialization at T ---\n",
        "        #print(f\"Initializing for T={self.T}...\")\n",
        "        self.J_values[self.T] = {}\n",
        "        z_vectors_T = self._generate_z_vectors(self.T, self.n)\n",
        "        for z_T in z_vectors_T:\n",
        "            for theta2 in range(self.n + 1):\n",
        "                state = (z_T, theta2)\n",
        "                self.J_values[self.T][state] = 0.0\n",
        "\n",
        "        # --- Backward Recursion ---\n",
        "        for t in range(self.T - 1, -1, -1):\n",
        "            #start_time = time.time()\n",
        "            J_t = {}\n",
        "            policy_t = {}\n",
        "            z_vectors_t = self._generate_z_vectors(t, self.n)\n",
        "\n",
        "            for z_t_tuple in z_vectors_t:\n",
        "                z_t = np.array(z_t_tuple)\n",
        "                for theta2 in range(self.n + 1):\n",
        "                    if t == 0 and theta2 > 0:\n",
        "                        break\n",
        "                    current_state = (z_t_tuple, theta2)\n",
        "                    action_values = []\n",
        "\n",
        "                    # Calculate belief based on current state\n",
        "                    if theta2 == 0:\n",
        "                        belief = self._calculate_joint_posterior(z_t)\n",
        "                    else:\n",
        "                        o2_loc = theta2 - 1\n",
        "                        belief = self._calculate_conditional_posterior(z_t, o2_loc)\n",
        "\n",
        "                    # Iterate over all possible actions\n",
        "                    for a_t in range(self.n):\n",
        "                        next_z_tuple = tuple(z_t + np.eye(self.n, dtype=int)[a_t])\n",
        "\n",
        "                        # --- Case 1: Both objects hidden ---\n",
        "                        if theta2 == 0:\n",
        "                            p_marginal_o1_at_a = np.sum(belief[a_t, :])\n",
        "\n",
        "                            # Prob of success (finding O1)\n",
        "                            p_success = (1 - self.gamma1[a_t]) * p_marginal_o1_at_a\n",
        "\n",
        "                            # Prob of finding O2 only\n",
        "                            p_cond_sum = np.sum(belief[np.arange(self.n) != a_t, a_t])\n",
        "                            p_find_o2_only = (1 - self.gamma2[a_t]) * (self.gamma1[a_t] * belief[a_t, a_t] + p_cond_sum)\n",
        "\n",
        "                            # Prob of finding nothing\n",
        "                            p_nothing = 1 - p_success - p_find_o2_only\n",
        "\n",
        "                            # Future values from next stage\n",
        "                            val_if_nothing = self.J_values[t + 1][(next_z_tuple, 0)]\n",
        "                            val_if_found_o2 = self.J_values[t + 1][(next_z_tuple, a_t + 1)]\n",
        "\n",
        "                            # Expected value for this action\n",
        "                            expected_value = (p_success * 1.0) - self.c[a_t]+ \\\n",
        "                                             (p_nothing * val_if_nothing) + \\\n",
        "                                             (p_find_o2_only * val_if_found_o2)\n",
        "                            action_values.append(expected_value)\n",
        "\n",
        "                        # --- Case 2: O2 has been found ---\n",
        "                        else:\n",
        "                            p_o1_at_a = belief[a_t]\n",
        "                            p_success = (1 - self.gamma1[a_t]) * p_o1_at_a\n",
        "                            p_fail = 1 - p_success\n",
        "\n",
        "                            val_if_fail = self.J_values[t + 1][(next_z_tuple, theta2)]\n",
        "\n",
        "                            expected_value = (p_success * 1.0) - self.c[a_t]+ (p_fail * val_if_fail)\n",
        "                            action_values.append(expected_value)\n",
        "\n",
        "                    # Find best action and store value/policy\n",
        "                    best_value = np.max(action_values)\n",
        "                    best_action = np.argmax(action_values)\n",
        "                    J_t[current_state] = best_value\n",
        "                    policy_t[current_state] = best_action\n",
        "\n",
        "            self.J_values[t] = J_t\n",
        "            self.Policy[t] = policy_t\n",
        "            #end_time = time.time()\n",
        "            #print(f\"Completed t={t}. Found {len(z_vectors_t)} states. Took {end_time - start_time:.2f}s.\")\n",
        "\n",
        "        #print(\"DP solver finished.\")\n",
        "\n",
        "    def get_optimal_value(self) -> float:\n",
        "        \"\"\"Returns the optimal value J(s_0).\"\"\"\n",
        "        initial_z = tuple([0] * self.n)\n",
        "        initial_state = (initial_z, 0)\n",
        "        return self.J_values[0][initial_state]\n",
        "# --- End of TwoObjectDPSolver ---\n",
        "\n",
        "\n",
        "class KStepLookaheadSolver:\n",
        "    \"\"\"\n",
        "    A k-step lookahead solver using rollout with a greedy policy for value estimation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n: int, T: int, initial_episode_belief: Any, # Can be (n,n) or (n,)\n",
        "                 gamma1: np.ndarray, gamma2: np.ndarray, theta2_at_init: int, k: int, c: np.ndarray, rollout_sims: int = 100, solver_seed: int = None):\n",
        "        self.n = n\n",
        "        self.T_global = T # Store global horizon for rollout remaining_horizon calculation\n",
        "        self.initial_episode_belief = initial_episode_belief # This is the current belief from the global episode\n",
        "        self.gamma1 = gamma1\n",
        "        self.gamma2 = gamma2\n",
        "        self.c = c\n",
        "        self.k = k # Lookahead horizon for *this* solver instance\n",
        "        self.rollout_sims = rollout_sims\n",
        "        self.theta2_at_init = theta2_at_init # O2 state when this k-step solver was created\n",
        "\n",
        "        self.J_values: Dict[int, Dict[Tuple, float]] = {}\n",
        "        self.Policy: Dict[int, Dict[Tuple, int]] = {}\n",
        "\n",
        "        # Initialize a seeded random number generator for reproducible rollouts\n",
        "        self.rng = np.random.default_rng(solver_seed)\n",
        "\n",
        "        # Precompute conditionals from the initial joint belief if theta2_at_init == 0\n",
        "        # These are used if O2 is hidden initially, but discovered within the k-step lookahead\n",
        "        if self.theta2_at_init == 0 and self.initial_episode_belief.ndim == 2:\n",
        "            self._precomputed_conditionals_from_initial_joint = self._precompute_conditionals_from_joint(self.initial_episode_belief)\n",
        "        else:\n",
        "            self._precomputed_conditionals_from_initial_joint = None\n",
        "\n",
        "    def _precompute_conditionals_from_joint(self, joint_p: np.ndarray) -> List[np.ndarray]:\n",
        "        \"\"\"Pre-calculates P(O1=i | O2=k) for all k from a given joint prior.\"\"\"\n",
        "        conditionals = []\n",
        "        for k_idx in range(self.n):\n",
        "            marginal_o2 = np.sum(joint_p[:, k_idx])\n",
        "            conditionals.append(joint_p[:, k_idx] / marginal_o2 if marginal_o2 > 0 else np.zeros(self.n))\n",
        "        return conditionals\n",
        "\n",
        "    def _calculate_posterior_at_step_in_k_lookahead(self, z_relative: np.ndarray, current_theta2_in_k_step: int) -> Any:\n",
        "        \"\"\"\n",
        "        Calculates the posterior belief for a state (z_relative, current_theta2_in_k_step)\n",
        "        within this k-step lookahead segment, starting from `self.initial_episode_belief`.\n",
        "        `z_relative`: observations accumulated *within this k-step window*.\n",
        "        `current_theta2_in_k_step`: the theta2 state *within this k-step window*.\n",
        "        \"\"\"\n",
        "        if current_theta2_in_k_step == 0: # O2 is still hidden\n",
        "            # If self.theta2_at_init was already > 0, this case should not happen for a consistent path\n",
        "            if self.initial_episode_belief.ndim == 1:\n",
        "                # This implies an inconsistency, should raise error or handle carefully\n",
        "                return np.zeros((self.n, self.n)) # Return a zero joint if initial was marginal\n",
        "\n",
        "            g1_z = np.power(self.gamma1, z_relative)\n",
        "            g2_z = np.power(self.gamma2, z_relative)\n",
        "            likelihood = np.outer(g1_z, g2_z)\n",
        "            numerator = self.initial_episode_belief * likelihood\n",
        "            norm = np.sum(numerator)\n",
        "            return numerator / norm if norm > 0 else numerator\n",
        "        else: # O2 is found (either initially or within this k-step segment)\n",
        "            if self.theta2_at_init > 0:\n",
        "                # O2 was already found when KStepLookaheadSolver was instantiated\n",
        "                # initial_episode_belief is already the marginal p(O1 | O2=actual_loc)\n",
        "                base_prior_for_o1 = self.initial_episode_belief\n",
        "            else:\n",
        "                # O2 was hidden initially but found within the k-step lookahead\n",
        "                # We need to get the conditional prior for O1 from the initial joint belief\n",
        "                o2_loc = current_theta2_in_k_step - 1\n",
        "                base_prior_for_o1 = self._precomputed_conditionals_from_initial_joint[o2_loc]\n",
        "\n",
        "            g1_z = np.power(self.gamma1, z_relative)\n",
        "            numerator = g1_z * base_prior_for_o1\n",
        "            norm = np.sum(numerator)\n",
        "            return numerator / norm if norm > 0 else numerator\n",
        "\n",
        "    def _select_greedy_action(self, belief: np.ndarray, theta2_for_greedy: int) -> int:\n",
        "        \"\"\"Selects a greedy action based on the current belief.\"\"\"\n",
        "        if theta2_for_greedy == 0: # Belief is joint (n,n)\n",
        "            p_marginal_o1 = np.sum(belief, axis=1)\n",
        "            immediate_value = (1 - self.gamma1) * p_marginal_o1\n",
        "        else: # Belief is marginal for O1 (n,)\n",
        "            immediate_value = (1 - self.gamma1) * belief\n",
        "        return np.argmax(immediate_value - self.c)\n",
        "\n",
        "\n",
        "    def _run_rollout(self, rollout_z_relative: np.ndarray, rollout_theta2_in_k_step: int, remaining_global_horizon: int) -> float:\n",
        "        \"\"\"\n",
        "        Estimates the value by simulating episodes with a greedy policy.\n",
        "        `rollout_z_relative`: z_vector accumulated *within the k-step lookahead* before rollout.\n",
        "        `rollout_theta2_in_k_step`: theta2 state *within the k-step lookahead* before rollout.\n",
        "        `remaining_global_horizon`: Actual remaining steps in the *global* problem after the k-step lookahead.\n",
        "        \"\"\"\n",
        "        total_reward = 0.0\n",
        "\n",
        "        for _ in range(self.rollout_sims):\n",
        "            current_z_in_rollout = np.copy(rollout_z_relative)\n",
        "            current_theta2_in_rollout = rollout_theta2_in_k_step\n",
        "\n",
        "            # Sample true object locations for this rollout simulation based on the *current belief*\n",
        "            current_belief_for_sampling = self._calculate_posterior_at_step_in_k_lookahead(current_z_in_rollout, current_theta2_in_rollout)\n",
        "\n",
        "            if current_theta2_in_rollout == 0: # Belief is joint\n",
        "                flat_belief = current_belief_for_sampling.flatten()\n",
        "                if np.sum(flat_belief) > 0:\n",
        "                    choice_idx = self.rng.choice(self.n * self.n, p=flat_belief / np.sum(flat_belief))\n",
        "                    rollout_true_pos_o1, rollout_true_pos_o2 = np.unravel_index(choice_idx, (self.n, self.n))\n",
        "                else:\n",
        "                    rollout_true_pos_o1, rollout_true_pos_o2 = -1, -1\n",
        "            else: # Belief is marginal for O1\n",
        "                if np.sum(current_belief_for_sampling) > 0:\n",
        "                    rollout_true_pos_o1 = self.rng.choice(self.n, p=current_belief_for_sampling / np.sum(current_belief_for_sampling))\n",
        "                    rollout_true_pos_o2 = current_theta2_in_rollout - 1 # O2 location is known\n",
        "                else:\n",
        "                    rollout_true_pos_o1, rollout_true_pos_o2 = -1, -1\n",
        "\n",
        "            episode_rollout_reward = 0.0\n",
        "            for t_rollout in range(remaining_global_horizon): # Iterate for remaining time steps\n",
        "                # Get belief for greedy action decision (no z_relative update yet for this step's decision)\n",
        "                current_belief_for_greedy_action = self._calculate_posterior_at_step_in_k_lookahead(current_z_in_rollout, current_theta2_in_rollout)\n",
        "                action = self._select_greedy_action(current_belief_for_greedy_action, current_theta2_in_rollout)\n",
        "\n",
        "                episode_rollout_reward -= self.c[action]\n",
        "\n",
        "                # Simulate outcome of greedy action for rollout\n",
        "                found_o1_in_rollout = (action == rollout_true_pos_o1) and (self.rng.random() > self.gamma1[action])\n",
        "                if found_o1_in_rollout:\n",
        "                    episode_rollout_reward += 1.0\n",
        "                    break # O1 found, rollout ends\n",
        "\n",
        "                if current_theta2_in_rollout == 0 and action == rollout_true_pos_o2 and (self.rng.random() > self.gamma2[action]):\n",
        "                    current_theta2_in_rollout = action + 1 # O2 found within rollout\n",
        "\n",
        "                current_z_in_rollout[action] += 1 # Update z for next rollout step\n",
        "            total_reward += episode_rollout_reward\n",
        "        return total_reward / self.rollout_sims\n",
        "\n",
        "    @staticmethod\n",
        "    @lru_cache(maxsize=None)\n",
        "    def _generate_z_vectors(t: int, n: int) -> List[Tuple[int, ...]]:\n",
        "        if n == 1:\n",
        "            return [(t,)]\n",
        "        vectors = []\n",
        "        for i in range(t + 1):\n",
        "            for sub_vector in KStepLookaheadSolver._generate_z_vectors(t - i, n - 1):\n",
        "                vectors.append((i,) + sub_vector)\n",
        "        return vectors\n",
        "\n",
        "    def solve(self): # For theta2_at_init == 0\n",
        "        \"\"\"\n",
        "        Executes DP for k steps from self.initial_episode_belief, for theta2_at_init = 0.\n",
        "        \"\"\"\n",
        "        self.J_values[self.k] = {}\n",
        "        z_vectors_k = self._generate_z_vectors(self.k, self.n)\n",
        "        for z_k_relative_tuple in z_vectors_k:\n",
        "            for theta2_in_k_step in range(self.n + 1):\n",
        "                state = (z_k_relative_tuple, theta2_in_k_step)\n",
        "                z_k_relative = np.array(z_k_relative_tuple)\n",
        "                # Value at the leaf node of k-step DP is estimated by rollout\n",
        "                self.J_values[self.k][state] = self._run_rollout(z_k_relative, theta2_in_k_step, self.T_global - self.k)\n",
        "\n",
        "\n",
        "        for t_k_relative in range(self.k - 1, -1, -1):\n",
        "            J_t = {}\n",
        "            policy_t = {}\n",
        "            z_vectors_t = self._generate_z_vectors(t_k_relative, self.n)\n",
        "\n",
        "            for z_t_relative_tuple in z_vectors_t:\n",
        "                for theta2_in_k_step in range(self.n + 1):\n",
        "                    current_state_in_k_step = (z_t_relative_tuple, theta2_in_k_step)\n",
        "                    action_values = []\n",
        "                    z_t_relative = np.array(z_t_relative_tuple)\n",
        "\n",
        "                    current_belief_in_k_step = self._calculate_posterior_at_step_in_k_lookahead(z_t_relative, theta2_in_k_step)\n",
        "\n",
        "                    for action_k in range(self.n):\n",
        "                        next_z_relative_tuple = tuple(z_t_relative + np.eye(self.n, dtype=int)[action_k])\n",
        "\n",
        "                        if theta2_in_k_step == 0: # O2 still hidden in this k-step segment\n",
        "                            # Current belief is joint (n,n)\n",
        "                            p_marginal_o1_at_a = np.sum(current_belief_in_k_step[action_k, :])\n",
        "                            p_success = (1 - self.gamma1[action_k]) * p_marginal_o1_at_a\n",
        "\n",
        "                            # Prob of finding O2 only\n",
        "                            p_find_o2_only = (1 - self.gamma2[action_k]) * (self.gamma1[action_k] * current_belief_in_k_step[action_k, action_k] + np.sum(current_belief_in_k_step[np.arange(self.n) != action_k, action_k]))\n",
        "\n",
        "                            p_nothing = 1 - p_success - p_find_o2_only\n",
        "\n",
        "                            val_if_nothing = self.J_values[t_k_relative + 1][(next_z_relative_tuple, 0)]\n",
        "                            val_if_found_o2 = self.J_values[t_k_relative + 1][(next_z_relative_tuple, action_k + 1)]\n",
        "\n",
        "                            expected_value = (p_success * 1.0) - self.c[action_k] + \\\n",
        "                                             (p_nothing * val_if_nothing) + \\\n",
        "                                             (p_find_o2_only * val_if_found_o2)\n",
        "                            action_values.append(expected_value)\n",
        "\n",
        "                        else: # O2 already found (theta2_in_k_step > 0)\n",
        "                            # Current belief is marginal for O1 (n,)\n",
        "                            p_o1_at_a = current_belief_in_k_step[action_k]\n",
        "                            p_success = (1 - self.gamma1[action_k]) * p_o1_at_a\n",
        "                            p_fail = 1 - p_success\n",
        "\n",
        "                            val_if_fail = self.J_values[t_k_relative + 1][(next_z_relative_tuple, theta2_in_k_step)]\n",
        "\n",
        "                            expected_value = (p_success * 1.0) - self.c[action_k] + (p_fail * val_if_fail)\n",
        "                            action_values.append(expected_value)\n",
        "\n",
        "                    best_value = np.max(action_values)\n",
        "                    best_action = np.argmax(action_values)\n",
        "                    J_t[current_state_in_k_step] = best_value\n",
        "                    policy_t[current_state_in_k_step] = best_action\n",
        "\n",
        "                self.J_values[t_k_relative] = J_t\n",
        "                self.Policy[t_k_relative] = policy_t\n",
        "\n",
        "    def solve_with_theta(self): # For theta2_at_init > 0\n",
        "        \"\"\"\n",
        "        Executes DP for k steps from self.initial_episode_belief, for theta2_at_init > 0.\n",
        "        In this case, O2 is already found, so theta2 is fixed throughout the k steps.\n",
        "        \"\"\"\n",
        "        fixed_theta2_in_k_step = self.theta2_at_init\n",
        "\n",
        "        self.J_values[self.k] = {}\n",
        "        z_vectors_k = self._generate_z_vectors(self.k, self.n)\n",
        "        for z_k_relative_tuple in z_vectors_k:\n",
        "            z_k_relative = np.array(z_k_relative_tuple)\n",
        "            state = (z_k_relative_tuple, fixed_theta2_in_k_step)\n",
        "            # Value at the leaf node of k-step DP is estimated by rollout\n",
        "            self.J_values[self.k][state] = self._run_rollout(z_k_relative, fixed_theta2_in_k_step, self.T_global - self.k)\n",
        "\n",
        "        for t_k_relative in range(self.k - 1, -1, -1):\n",
        "            J_t = {}\n",
        "            policy_t = {}\n",
        "            z_vectors_t = self._generate_z_vectors(t_k_relative, self.n)\n",
        "\n",
        "            for z_t_relative_tuple in z_vectors_t:\n",
        "                z_t_relative = np.array(z_t_relative_tuple)\n",
        "                current_state_in_k_step = (z_t_relative_tuple, fixed_theta2_in_k_step)\n",
        "                action_values = []\n",
        "\n",
        "                current_belief_in_k_step = self._calculate_posterior_at_step_in_k_lookahead(z_t_relative, fixed_theta2_in_k_step)\n",
        "\n",
        "                for action_k in range(self.n):\n",
        "                    next_z_relative_tuple = tuple(z_t_relative + np.eye(self.n, dtype=int)[action_k])\n",
        "\n",
        "                    p_o1_at_a = current_belief_in_k_step[action_k]\n",
        "                    p_success = (1 - self.gamma1[action_k]) * p_o1_at_a\n",
        "                    p_fail = 1 - p_success\n",
        "\n",
        "                    val_if_fail = self.J_values[t_k_relative + 1][(next_z_relative_tuple, fixed_theta2_in_k_step)]\n",
        "\n",
        "                    expected_value = (p_success * 1.0) - self.c[action_k] + (p_fail * val_if_fail)\n",
        "                    action_values.append(expected_value)\n",
        "\n",
        "                best_value = np.max(action_values)\n",
        "                best_action = np.argmax(action_values)\n",
        "                J_t[current_state_in_k_step] = best_value\n",
        "                policy_t[current_state_in_k_step] = best_action\n",
        "\n",
        "            self.J_values[t_k_relative] = J_t\n",
        "            self.Policy[t_k_relative] = policy_t\n",
        "\n",
        "\n",
        "# Helper function definitions (assuming they are available or defined above this class)\n",
        "# Need to ensure calculate_joint_posterior and calculate_conditional_posterior are accessible.\n",
        "# Let's define them here for clarity, or assume they are in a common helpers file.\n",
        "\n",
        "# Re-including helper functions for self-containment\n",
        "def calculate_joint_posterior(z_vector: np.ndarray, initial_prior_joint: np.ndarray,\n",
        "                              gamma1: np.ndarray, gamma2: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Calculates the joint posterior belief p(O1=i, O2=j | z) based on initial joint prior.\"\"\"\n",
        "    g1_z = np.power(gamma1, z_vector)\n",
        "    g2_z = np.power(gamma2, z_vector)\n",
        "    likelihood = np.outer(g1_z, g2_z)\n",
        "    numerator = initial_prior_joint * likelihood\n",
        "    norm = np.sum(numerator)\n",
        "    return numerator / norm if norm > 0 else numerator\n",
        "\n",
        "def calculate_conditional_posterior(z_vector: np.ndarray, o2_loc: int,\n",
        "                                    precomputed_conditionals: List[np.ndarray], gamma1: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Calculates the conditional posterior belief p(O1=i | z, O2=k) based on precomputed conditionals and current z_vector.\"\"\"\n",
        "    p0_conditional = precomputed_conditionals[o2_loc]\n",
        "    g1_z = np.power(gamma1, z_vector)\n",
        "    numerator = g1_z * p0_conditional\n",
        "    norm = np.sum(numerator)\n",
        "    return numerator / norm if norm > 0 else numerator\n",
        "\n",
        "def precompute_p0_conditionals(n: int, p0_joint: np.ndarray) -> list:\n",
        "    \"\"\"Pre-calculates P(O1=i | O2=k) for all k from a joint prior matrix.\"\"\"\n",
        "    if p0_joint.ndim != 2: # Should be (n,n) for joint prior\n",
        "        raise ValueError(\"p0 must be a 2D joint probability matrix for precomputing conditionals.\")\n",
        "    conditionals = []\n",
        "    for k in range(n):\n",
        "        marginal_o2 = np.sum(p0_joint[:, k])\n",
        "        conditionals.append(p0_joint[:, k] / marginal_o2 if marginal_o2 > 0 else np.zeros(n))\n",
        "    return conditionals\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Adaptive Re-planning Episode Simulation\n",
        "# =============================================================================\n",
        "\n",
        "def run_k_step_episode(n: int, T: int, p0_initial_joint: np.ndarray,\n",
        "                                    gamma1: np.ndarray, gamma2: np.ndarray, c: np.ndarray, k: int, ROLLOUT_SIMULATIONS: int, episode_seed: int) -> Tuple[bool, int, float, int]:\n",
        "    \"\"\"\n",
        "    Simulates a single episode using k-step lookahead with greedy rollout and then full DP.\n",
        "    Returns success status, time of detection (or -1 if failed), accumulated reward, and episode length.\n",
        "    \"\"\"\n",
        "    # 1. Secretly determine the true locations for the entire episode\n",
        "    flat_p0 = p0_initial_joint.flatten()\n",
        "    if np.sum(flat_p0) == 0: # Handle case where prior is all zeros\n",
        "         true_pos_o1, true_pos_o2 = -1, -1 # Indicate no target\n",
        "    else:\n",
        "        # Use np.random directly as its state is set by np.random.seed(episode_seed)\n",
        "        choice_index = np.random.choice(n * n, p=flat_p0 / np.sum(flat_p0)) # Normalize\n",
        "        true_pos_o1, true_pos_o2 = np.unravel_index(choice_index, (n, n))\n",
        "\n",
        "    # 2. Initialize global state and reward variables\n",
        "    z_vector_global = np.zeros(n, dtype=int) # Accumulated search history for the entire episode\n",
        "    theta2_global = 0 # O2 state: 0=hidden, >0=found at cell (theta2_global-1)\n",
        "    current_belief_global = np.copy(p0_initial_joint) # Evolving belief state (joint or marginal)\n",
        "\n",
        "    # Precompute global conditionals from the initial joint prior once\n",
        "    global_p0_conditionals = precompute_p0_conditionals(n, p0_initial_joint)\n",
        "\n",
        "    accumulated_reward = 0.0\n",
        "    detection_time = -1\n",
        "    episode_length = 0\n",
        "\n",
        "    # Phase 1: K-step lookahead for (T-k) time steps\n",
        "    for t_global in range(T - k): # t_global is the current time step in the entire episode\n",
        "        episode_length += 1\n",
        "\n",
        "        # Create a unique, reproducible seed for this KStepLookaheadSolver instance\n",
        "        solver_instance_seed = episode_seed * T + t_global\n",
        "\n",
        "        # Instantiate K-step lookahead solver with the current global state (belief, z_vector, theta2)\n",
        "        # The KStepLookaheadSolver will calculate policies for `k` steps starting from a relative z=0.\n",
        "        solver_instance = KStepLookaheadSolver(n=n, T=T-t_global,\n",
        "                                                initial_episode_belief=current_belief_global,\n",
        "                                                gamma1=gamma1, gamma2=gamma2,\n",
        "                                                theta2_at_init=theta2_global,\n",
        "                                                k=k, c=c, rollout_sims=ROLLOUT_SIMULATIONS,\n",
        "                                                solver_seed=solver_instance_seed)\n",
        "\n",
        "        if theta2_global == 0: # If O2 is currently hidden, use the general solve method\n",
        "            solver_instance.solve()\n",
        "        else: # If O2 is currently found, use the solve method specialized for a fixed theta2\n",
        "            solver_instance.solve_with_theta()\n",
        "\n",
        "        # Get the optimal action for the current global step (t_global) from the k-step solver's policy\n",
        "        # The k-step solver's policy is for its *relative* time t=0 and *relative* z=0.\n",
        "        action = solver_instance.Policy[0][(tuple([0]*n), theta2_global)]\n",
        "\n",
        "        # Deduct cost for the action\n",
        "        accumulated_reward -= c[action]\n",
        "\n",
        "        # Simulate outcome of the action\n",
        "        found_o1 = (action == true_pos_o1) and (np.random.random() > gamma1[action])\n",
        "        if found_o1:\n",
        "            accumulated_reward += 1.0 # Reward for finding O1\n",
        "            detection_time = t_global + 1 # Absolute time of detection\n",
        "            return True, detection_time, accumulated_reward, episode_length # Mission Success!\n",
        "\n",
        "        # Check if O2 is found *at this current global step* (only if it was previously hidden)\n",
        "        if theta2_global == 0 and action == true_pos_o2 and (np.random.random() > gamma2[action]):\n",
        "            theta2_global = action + 1 # O2 found at cell `action`, update state\n",
        "\n",
        "        # Update global z_vector based on the action taken\n",
        "        z_vector_global[action] += 1\n",
        "\n",
        "        # Update global belief state based on the action and observed outcome\n",
        "        if theta2_global == 0: # If O2 is still hidden globally\n",
        "            current_belief_global = calculate_joint_posterior(z_vector_global, p0_initial_joint, gamma1, gamma2)\n",
        "        else: # If O2 has been found globally\n",
        "            current_belief_global = calculate_conditional_posterior(z_vector_global, theta2_global - 1, global_p0_conditionals, gamma1)\n",
        "\n",
        "    # Phase 2: After T-k steps, switch to full DP for the remaining k steps\n",
        "    # The remaining time horizon for this DP problem is `k`\n",
        "    # The DP solvers operate on their own relative z_vector, taking `current_belief_global` as their starting prior.\n",
        "    final_z_for_dp = np.zeros(n, dtype=int) # z_vector for the DP solver starts from 0 relative to its own horizon\n",
        "    final_theta2_for_dp = theta2_global # O2 state is carried over to the DP phase\n",
        "    final_policy = None\n",
        "\n",
        "    if final_theta2_for_dp == 0: # O2 is still hidden, use TwoObjectDPSolver\n",
        "        dp_solver = TwoObjectDPSolver(n=n, T=k, p0=current_belief_global, gamma1=gamma1, gamma2=gamma2, c=c)\n",
        "        dp_solver.solve()\n",
        "        final_policy = dp_solver.Policy\n",
        "    else: # O2 has been found, use SingleObjectDPSolver\n",
        "        dp_solver = SingleObjectDPSolver(n=n, T=k, p0_marginal=current_belief_global, gamma1=gamma1, c=c)\n",
        "        dp_solver.solve()\n",
        "        final_policy = dp_solver.Policy\n",
        "\n",
        "    for t_dp in range(k): # Iterate for the remaining `k` time steps using the final DP policy\n",
        "        episode_length += 1\n",
        "\n",
        "        # Determine the state key for the DP policy based on theta2\n",
        "        state_for_dp = (tuple(final_z_for_dp), final_theta2_for_dp) if final_theta2_for_dp == 0 else tuple(final_z_for_dp)\n",
        "\n",
        "        if t_dp not in final_policy or state_for_dp not in final_policy[t_dp]:\n",
        "            # This case should ideally not happen if the DP policy is complete for the horizon `k`\n",
        "            # print(f\"Warning: DP Policy not found for state {state_for_dp} at time {t_dp}. Ending episode.\")\n",
        "            return False, detection_time, accumulated_reward, episode_length\n",
        "\n",
        "        action = final_policy[t_dp][state_for_dp]\n",
        "        accumulated_reward -= c[action]\n",
        "\n",
        "        # Simulate outcome of the action in the DP phase\n",
        "        if action == true_pos_o1 and (np.random.random() > gamma1[action]):\n",
        "            accumulated_reward += 1.0 # Reward for finding O1\n",
        "            detection_time = (T - k) + (t_dp + 1) # Absolute time of detection\n",
        "            return True, detection_time, accumulated_reward, episode_length # Mission Success!\n",
        "\n",
        "        # Check for O2 discovery during DP phase, if O2 was still hidden\n",
        "        if final_theta2_for_dp == 0 and action == true_pos_o2 and (np.random.random() > gamma2[action]):\n",
        "            # If O2 is found here, the state `final_theta2_for_dp` is updated, but the DP policy\n",
        "            # was computed assuming `final_theta2_for_dp` was `0` for the entire `k` steps.\n",
        "            # A more advanced adaptive approach would re-plan here with a new SingleObjectDPSolver.\n",
        "            # For this simplified model, we just update the state and continue with the same policy.\n",
        "            final_theta2_for_dp = action + 1\n",
        "\n",
        "        # Update z_vector for the DP solver's internal state\n",
        "        final_z_for_dp[action] += 1\n",
        "\n",
        "    return False, detection_time, accumulated_reward, episode_length # Mission Failed\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 2. MAIN EXPERIMENT SCRIPT (Modified for multiple seeds)\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Problem Definition ---\n",
        "    NUM_CELLS = 25\n",
        "    TIME_HORIZON = 20\n",
        "    LOOKAHEAD_K = 1 # Set your desired lookahead depth here. If LOOKAHEAD_K == T, it's pure KStepLookahead.\n",
        "                      # If LOOKAHEAD_K == 0, it's pure TwoObjectDPSolver (adaptive replanning).\n",
        "    NUM_EPISODES_PER_SEED = 10\n",
        "    ROLLOUT_SIMULATIONS = 10 # Number of simulations for rollout value estimation\n",
        "    NUM_SEEDS = 10 # Number of different seeds to run\n",
        "\n",
        "    \"\"\"# Problem parameters (same for all seeds)\n",
        "    prior = np.array([\n",
        "        [0.152,  0.0039, 0.003,  0.0108, 0.011],\n",
        "        [0.0038, 0.0052, 0.117,  0.0162, 0.165],\n",
        "        [0.0057, 0.195,  0.015,  0.009,  0.011],\n",
        "        [0.0038, 0.0091, 0.0075, 0.027,  0.011],\n",
        "        [0.0247, 0.0468, 0.0075, 0.117,  0.022]\n",
        "    ])\n",
        "\n",
        "    gammas1 = np.array([0.8, 0.65, 0.82, 0.75, 0.7])\n",
        "    gammas2 = np.array([0.2, 0.1, 0.25, 0.15, 0.2])\n",
        "    c= np.array([0.15,0.2,0.25,0.1,0.2])\"\"\"\n",
        "    # Example usage for testing the greedy policy\n",
        "\n",
        "\n",
        "    SEED = 42\n",
        "\n",
        "    prior, gammas1, gammas2, c = generate_problem_parameters(NUM_CELLS, SEED)\n",
        "\n",
        "    all_success_rates = []\n",
        "    all_detection_times = []\n",
        "    all_rewards = []\n",
        "    all_episode_lengths = []\n",
        "    total_experiment_start_time = time.time()\n",
        "\n",
        "    print(\"--- Running Empirical Experiment for k-step lookahead Policy over Multiple Seeds ---\")\n",
        "    print(f\"Number of seeds: {NUM_SEEDS}\")\n",
        "    print(f\"Number of evaluation episodes per seed: {NUM_EPISODES_PER_SEED}\")\n",
        "    print(f\"Lookahead depth (k): {LOOKAHEAD_K}\")\n",
        "    print(f\"Rollout simulations: {ROLLOUT_SIMULATIONS}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "\n",
        "    for seed in range(NUM_SEEDS):\n",
        "        print(f\"\\n--- Running with Seed: {seed} ---\")\n",
        "        np.random.seed(seed) # Set seed for main episode simulation\n",
        "\n",
        "        num_successes = 0\n",
        "        detection_times_this_seed = []\n",
        "        rewards_this_seed = []\n",
        "        episode_lengths_this_seed = []\n",
        "        start_time_this_seed = time.time()\n",
        "\n",
        "        for i in range(NUM_EPISODES_PER_SEED):\n",
        "            # print(f\"Episode : {i}\") # Uncomment for detailed per-episode output\n",
        "            success, detection_time, reward, episode_length = run_k_step_episode(NUM_CELLS, TIME_HORIZON, prior, gammas1, gammas2, c, LOOKAHEAD_K, ROLLOUT_SIMULATIONS, seed)\n",
        "            rewards_this_seed.append(reward)\n",
        "            episode_lengths_this_seed.append(episode_length)\n",
        "\n",
        "            if success:\n",
        "                num_successes += 1\n",
        "                detection_times_this_seed.append(detection_time)\n",
        "                # print(f\"    (Episode {i+1} Result) SUCCESS at time {detection_time} with reward {reward:.4f}\")\n",
        "            # else:\n",
        "                # print(f\"    (Episode {i+1} Result) FAILURE with reward {reward:.4f}\")\n",
        "\n",
        "\n",
        "        end_time_this_seed = time.time()\n",
        "        success_rate_this_seed = num_successes / NUM_EPISODES_PER_SEED\n",
        "        all_success_rates.append(success_rate_this_seed)\n",
        "        all_detection_times.extend(detection_times_this_seed)\n",
        "        all_rewards.extend(rewards_this_seed)\n",
        "        all_episode_lengths.extend(episode_lengths_this_seed)\n",
        "\n",
        "\n",
        "        print(f\"--- Seed {seed} Results ---\")\n",
        "        print(f\"Success Rate: {success_rate_this_seed:.4f} ({success_rate_this_seed*100:.2f}%)\")\n",
        "        if detection_times_this_seed:\n",
        "            print(f\"Average Detection Time (Successful Episodes): {np.mean(detection_times_this_seed):.2f}\")\n",
        "        else:\n",
        "            print(\"Average Detection Time (Successful Episodes): N/A (no successes)\")\n",
        "        print(f\"Average Episode Reward: {np.mean(rewards_this_seed):.4f}\")\n",
        "        print(f\"Average Episode Length: {np.mean(episode_lengths_this_seed):.2f}\")\n",
        "        print(f\"Time taken for this seed: {end_time_this_seed - start_time_this_seed:.2f} seconds\")\n",
        "\n",
        "\n",
        "    total_experiment_end_time = time.time()\n",
        "\n",
        "    # --- 3.4. Display Final Results ---\n",
        "    mean_success_rate = np.mean(all_success_rates)\n",
        "    std_success_rate = np.std(all_success_rates)\n",
        "    mean_detection_time = np.mean(all_detection_times) if all_detection_times else -1\n",
        "    std_detection_time = np.std(all_detection_times) if all_detection_times else -1\n",
        "    mean_reward = np.mean(all_rewards)\n",
        "    std_reward = np.std(all_rewards)\n",
        "    mean_episode_length = np.mean(all_episode_lengths)\n",
        "    std_episode_length = np.std(all_episode_lengths)\n",
        "\n",
        "\n",
        "    print(f\"\\n--- Overall {LOOKAHEAD_K}-step Lookahead Policy Results ---\")\n",
        "    print(f\"Total experiment time across {NUM_SEEDS} seeds: {total_experiment_end_time - total_experiment_start_time:.2f} seconds\")\n",
        "    print(f\"Average Success Rate over {NUM_SEEDS} seeds and {NUM_EPISODES_PER_SEED} episodes each: {mean_success_rate:.4f} ({mean_success_rate*100:.2f}%)\")\n",
        "    print(f\"Standard Deviation of Success Rate: {std_success_rate:.4f}\")\n",
        "    if all_detection_times:\n",
        "        print(f\"Average Detection Time (Successful Episodes) across all seeds: {mean_detection_time:.2f}\")\n",
        "        print(f\"Standard Deviation of Detection Time: {std_detection_time:.2f}\")\n",
        "    else:\n",
        "        print(\"Average Detection Time (Successful Episodes) across all seeds: N/A (no successes)\")\n",
        "\n",
        "    print(f\"Average Episode Reward across all seeds: {mean_reward:.4f}\")\n",
        "    print(f\"Standard Deviation of Reward: {std_reward:.4f}\")\n",
        "    print(f\"Average Episode Length across all seeds: {mean_episode_length:.2f}\")\n",
        "    print(f\"Standard Deviation of Episode Length: {std_episode_length:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    all_success_rates = []\n",
        "    all_detection_times = []\n",
        "    all_rewards = []\n",
        "    all_episode_lengths = []\n",
        "    total_experiment_start_time = time.time()\n",
        "\n",
        "    # Running multiple seeds for statistical robustness (similar to other policies)\n",
        "    NUM_SEEDS = 10\n",
        "    NUM_EPISODES_PER_SEED = 10 # Total episodes will be NUM_SEEDS * NUM_EPISODES_PER_SEED\n",
        "\n",
        "    for seed in range(NUM_SEEDS):\n",
        "        # Use a consistent seed for np.random in the greedy episode function\n",
        "        # to ensure reproducibility of the episode outcomes for a given seed.\n",
        "        # This seed is then used by the rng object inside run_greedy_episode.\n",
        "        # np.random.seed(seed) # Not needed as rng is passed per episode\n",
        "\n",
        "        num_successes = 0\n",
        "        detection_times_this_seed = []\n",
        "        rewards_this_seed = []\n",
        "        episode_lengths_this_seed = []\n",
        "        start_time_this_seed = time.time()\n",
        "\n",
        "        for i in range(NUM_EPISODES_PER_SEED):\n",
        "            episode_unique_seed = seed * NUM_EPISODES_PER_SEED + i\n",
        "            success, detection_time, reward, episode_length = run_greedy_episode(\n",
        "                NUM_CELLS, TIME_HORIZON, prior, gammas1, gammas2, c, episode_unique_seed\n",
        "            )\n",
        "            rewards_this_seed.append(reward)\n",
        "            episode_lengths_this_seed.append(episode_length)\n",
        "\n",
        "            if success:\n",
        "                num_successes += 1\n",
        "                detection_times_this_seed.append(detection_time)\n",
        "\n",
        "        end_time_this_seed = time.time()\n",
        "        success_rate_this_seed = num_successes / NUM_EPISODES_PER_SEED\n",
        "        all_success_rates.append(success_rate_this_seed)\n",
        "        all_detection_times.extend(detection_times_this_seed)\n",
        "        all_rewards.extend(rewards_this_seed)\n",
        "        all_episode_lengths.extend(episode_lengths_this_seed)\n",
        "\n",
        "        print(f\"--- Seed {seed} Results ---\")\n",
        "        print(f\"Success Rate: {success_rate_this_seed:.4f} ({success_rate_this_seed*100:.2f}%)\")\n",
        "        if detection_times_this_seed:\n",
        "            print(f\"Average Detection Time (Successful Episodes): {np.mean(detection_times_this_seed):.2f}\")\n",
        "        else:\n",
        "            print(\"Average Detection Time (Successful Episodes): N/A (no successes)\")\n",
        "        print(f\"Average Episode Reward: {np.mean(rewards_this_seed):.4f}\")\n",
        "        print(f\"Average Episode Length: {np.mean(episode_lengths_this_seed):.2f}\")\n",
        "        print(f\"Time taken for this seed: {end_time_this_seed - start_time_this_seed:.2f} seconds\")\n",
        "\n",
        "    total_experiment_end_time = time.time()\n",
        "\n",
        "    mean_success_rate = np.mean(all_success_rates)\n",
        "    std_success_rate = np.std(all_success_rates)\n",
        "    mean_detection_time = np.mean(all_detection_times) if all_detection_times else -1\n",
        "    std_detection_time = np.std(all_detection_times) if all_detection_times else -1\n",
        "    mean_reward = np.mean(all_rewards)\n",
        "    std_reward = np.std(all_rewards)\n",
        "    mean_episode_length = np.mean(all_episode_lengths)\n",
        "    std_episode_length = np.std(all_episode_lengths)\n",
        "\n",
        "    print(\"\\n--- Overall Greedy Policy Results ---\")\n",
        "    print(f\"Total experiment time across {NUM_SEEDS} seeds: {total_experiment_end_time - total_experiment_start_time:.2f} seconds\")\n",
        "    print(f\"Average Success Rate over {NUM_SEEDS} seeds and {NUM_EPISODES_PER_SEED} episodes each: {mean_success_rate:.4f} ({mean_success_rate*100:.2f}%)\")\n",
        "    print(f\"Standard Deviation of Success Rate: {std_success_rate:.4f}\")\n",
        "    if all_detection_times:\n",
        "        print(f\"Average Detection Time (Successful Episodes) across all seeds: {mean_detection_time:.2f}\")\n",
        "        print(f\"Standard Deviation of Detection Time: {std_detection_time:.2f}\")\n",
        "    else:\n",
        "        print(\"Average Detection Time (Successful Episodes) across all seeds: N/A (no successes)\")\n",
        "\n",
        "    print(f\"Average Episode Reward across all seeds: {mean_reward:.4f}\")\n",
        "    print(f\"Standard Deviation of Reward: {std_reward:.4f}\")\n",
        "    print(f\"Average Episode Length across all seeds: {mean_episode_length:.2f}\")\n",
        "    print(f\"Standard Deviation of Episode Length: {std_episode_length:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "tXg7ZC4RN973",
        "outputId": "150fdab7-67be-440d-8667-86b57a420b28"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'run_greedy_episode' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2278885419.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPISODES_PER_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mepisode_unique_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mNUM_EPISODES_PER_SEED\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             success, detection_time, reward, episode_length = run_greedy_episode(\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mNUM_CELLS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTIME_HORIZON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammas2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_unique_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'run_greedy_episode' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # --- Problem Definition ---\n",
        "    NUM_CELLS = 50\n",
        "    TIME_HORIZON = 30\n",
        "    LOOKAHEAD_K = 2 # Set your desired lookahead depth here. If LOOKAHEAD_K == T, it's pure KStepLookahead.\n",
        "                      # If LOOKAHEAD_K == 0, it's pure TwoObjectDPSolver (adaptive replanning).\n",
        "    NUM_EPISODES_PER_SEED = 10\n",
        "    ROLLOUT_SIMULATIONS = 10 # Number of simulations for rollout value estimation\n",
        "    NUM_SEEDS = 10 # Number of different seeds to run\n",
        "\n",
        "    \"\"\"# Problem parameters (same for all seeds)\n",
        "    prior = np.array([\n",
        "        [0.152,  0.0039, 0.003,  0.0108, 0.011],\n",
        "        [0.0038, 0.0052, 0.117,  0.0162, 0.165],\n",
        "        [0.0057, 0.195,  0.015,  0.009,  0.011],\n",
        "        [0.0038, 0.0091, 0.0075, 0.027,  0.011],\n",
        "        [0.0247, 0.0468, 0.0075, 0.117,  0.022]\n",
        "    ])\n",
        "\n",
        "    gammas1 = np.array([0.8, 0.65, 0.82, 0.75, 0.7])\n",
        "    gammas2 = np.array([0.2, 0.1, 0.25, 0.15, 0.2])\n",
        "    c= np.array([0.15,0.2,0.25,0.1,0.2])\"\"\"\n",
        "    # Example usage for testing the greedy policy\n",
        "\n",
        "\n",
        "    SEED = 42\n",
        "\n",
        "    #prior, gammas1, gammas2, c = generate_problem_parameters(NUM_CELLS, SEED)\n",
        "\n",
        "    all_success_rates = []\n",
        "    all_detection_times = []\n",
        "    all_rewards = []\n",
        "    all_episode_lengths = []\n",
        "    total_experiment_start_time = time.time()\n",
        "\n",
        "    print(\"--- Running Empirical Experiment for k-step lookahead Policy over Multiple Seeds ---\")\n",
        "    print(f\"Number of seeds: {NUM_SEEDS}\")\n",
        "    print(f\"Number of evaluation episodes per seed: {NUM_EPISODES_PER_SEED}\")\n",
        "    print(f\"Lookahead depth (k): {LOOKAHEAD_K}\")\n",
        "    print(f\"Rollout simulations: {ROLLOUT_SIMULATIONS}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "\n",
        "    for seed in range(NUM_SEEDS):\n",
        "        print(f\"\\n--- Running with Seed: {seed} ---\")\n",
        "        np.random.seed(seed) # Set seed for main episode simulation\n",
        "\n",
        "        num_successes = 0\n",
        "        detection_times_this_seed = []\n",
        "        rewards_this_seed = []\n",
        "        episode_lengths_this_seed = []\n",
        "        start_time_this_seed = time.time()\n",
        "\n",
        "        for i in range(NUM_EPISODES_PER_SEED):\n",
        "            # print(f\"Episode : {i}\") # Uncomment for detailed per-episode output\n",
        "            success, detection_time, reward, episode_length = run_k_step_episode(NUM_CELLS, TIME_HORIZON, prior, gammas1, gammas2, c, LOOKAHEAD_K, ROLLOUT_SIMULATIONS, seed)\n",
        "            rewards_this_seed.append(reward)\n",
        "            episode_lengths_this_seed.append(episode_length)\n",
        "\n",
        "            if success:\n",
        "                num_successes += 1\n",
        "                detection_times_this_seed.append(detection_time)\n",
        "                # print(f\"    (Episode {i+1} Result) SUCCESS at time {detection_time} with reward {reward:.4f}\")\n",
        "            # else:\n",
        "                # print(f\"    (Episode {i+1} Result) FAILURE with reward {reward:.4f}\")\n",
        "\n",
        "\n",
        "        end_time_this_seed = time.time()\n",
        "        success_rate_this_seed = num_successes / NUM_EPISODES_PER_SEED\n",
        "        all_success_rates.append(success_rate_this_seed)\n",
        "        all_detection_times.extend(detection_times_this_seed)\n",
        "        all_rewards.extend(rewards_this_seed)\n",
        "        all_episode_lengths.extend(episode_lengths_this_seed)\n",
        "\n",
        "\n",
        "        print(f\"--- Seed {seed} Results ---\")\n",
        "        print(f\"Success Rate: {success_rate_this_seed:.4f} ({success_rate_this_seed*100:.2f}%)\")\n",
        "        if detection_times_this_seed:\n",
        "            print(f\"Average Detection Time (Successful Episodes): {np.mean(detection_times_this_seed):.2f}\")\n",
        "        else:\n",
        "            print(\"Average Detection Time (Successful Episodes): N/A (no successes)\")\n",
        "        print(f\"Average Episode Reward: {np.mean(rewards_this_seed):.4f}\")\n",
        "        print(f\"Average Episode Length: {np.mean(episode_lengths_this_seed):.2f}\")\n",
        "        print(f\"Time taken for this seed: {end_time_this_seed - start_time_this_seed:.2f} seconds\")\n",
        "\n",
        "\n",
        "    total_experiment_end_time = time.time()\n",
        "\n",
        "    # --- 3.4. Display Final Results ---\n",
        "    mean_success_rate = np.mean(all_success_rates)\n",
        "    std_success_rate = np.std(all_success_rates)\n",
        "    mean_detection_time = np.mean(all_detection_times) if all_detection_times else -1\n",
        "    std_detection_time = np.std(all_detection_times) if all_detection_times else -1\n",
        "    mean_reward = np.mean(all_rewards)\n",
        "    std_reward = np.std(all_rewards)\n",
        "    mean_episode_length = np.mean(all_episode_lengths)\n",
        "    std_episode_length = np.std(all_episode_lengths)\n",
        "\n",
        "\n",
        "    print(f\"\\n--- Overall {LOOKAHEAD_K}-step Lookahead Policy Results ---\")\n",
        "    print(f\"Total experiment time across {NUM_SEEDS} seeds: {total_experiment_end_time - total_experiment_start_time:.2f} seconds\")\n",
        "    print(f\"Average Success Rate over {NUM_SEEDS} seeds and {NUM_EPISODES_PER_SEED} episodes each: {mean_success_rate:.4f} ({mean_success_rate*100:.2f}%)\")\n",
        "    print(f\"Standard Deviation of Success Rate: {std_success_rate:.4f}\")\n",
        "    if all_detection_times:\n",
        "        print(f\"Average Detection Time (Successful Episodes) across all seeds: {mean_detection_time:.2f}\")\n",
        "        print(f\"Standard Deviation of Detection Time: {std_detection_time:.2f}\")\n",
        "    else:\n",
        "        print(\"Average Detection Time (Successful Episodes) across all seeds: N/A (no successes)\")\n",
        "\n",
        "    print(f\"Average Episode Reward across all seeds: {mean_reward:.4f}\")\n",
        "    print(f\"Standard Deviation of Reward: {std_reward:.4f}\")\n",
        "    print(f\"Average Episode Length across all seeds: {mean_episode_length:.2f}\")\n",
        "    print(f\"Standard Deviation of Episode Length: {std_episode_length:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "6YwJ8L1U-cmV",
        "outputId": "ce49ed6b-1715-4019-8520-1d2767b11fdf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'time' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1270683305.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mall_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mall_episode_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtotal_experiment_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Running Empirical Experiment for k-step lookahead Policy over Multiple Seeds ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrW0nD6KbTXc"
      },
      "outputs": [],
      "source": [
        "``Good''\n",
        "--- Overall 1-step Lookahead Policy Results ---\n",
        "Total experiment time across 100 seeds: 20744.66 seconds\n",
        "Average Success Rate over 100 seeds and 100 episodes each: 0.5977 (59.77%)\n",
        "Standard Deviation of Success Rate: 0.0521\n",
        "Average Detection Time (Successful Episodes) across all seeds: 5.29\n",
        "Standard Deviation of Detection Time: 2.68\n",
        "Average Episode Reward across all seeds: -0.5723\n",
        "Standard Deviation of Reward: 0.9251\n",
        "Average Episode Length across all seeds: 7.18\n",
        "Standard Deviation of Episode Length: 3.11\n",
        "\n",
        "``After crsuh''\n",
        "--- Overall 1-step Lookahead Policy Results ---\n",
        "Total experiment time across 100 seeds: 20803.67 seconds\n",
        "Average Success Rate over 100 seeds and 100 episodes each: 0.5888 (58.88%)\n",
        "Standard Deviation of Success Rate: 0.0491\n",
        "Average Detection Time (Successful Episodes) across all seeds: 5.27\n",
        "Standard Deviation of Detection Time: 2.67\n",
        "Average Episode Reward across all seeds: -0.5870\n",
        "Standard Deviation of Reward: 0.9294\n",
        "Average Episode Length across all seeds: 7.21\n",
        "Standard Deviation of Episode Length: 3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9bDkMDhBOlQ"
      },
      "outputs": [],
      "source": [
        "--- Overall 2-Step Lookahead Policy Results with Adaptive DP Replanning ---\n",
        "Total experiment time across 100 seeds: 885.99 seconds\n",
        "Average Success Rate over 100 seeds and 100 episodes each: 0.4231 (42.31%)\n",
        "Standard Deviation of Success Rate: 0.0547\n",
        "Average Detection Time (Successful Episodes) across all seeds: 7.22\n",
        "Standard Deviation of Detection Time: 2.91\n",
        "Average Episode Reward across all seeds: -0.7875\n",
        "Standard Deviation of Reward: 0.7513\n",
        "Average Episode Length across all seeds: 9.63\n",
        "Standard Deviation of Episode Length: 2.69"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvueSPiyuoz3"
      },
      "outputs": [],
      "source": [
        "--- Overall 5-Step Lookahead Policy Results with Adaptive DP Replanning ---\n",
        "Total experiment time across 100 seeds: 11173.73 seconds\n",
        "Average Success Rate over 100 seeds and 100 episodes each: 0.5826 (58.26%)\n",
        "Standard Deviation of Success Rate: 0.0464\n",
        "Average Detection Time (Successful Episodes) across all seeds: 5.98\n",
        "Standard Deviation of Detection Time: 2.20\n",
        "Average Episode Reward across all seeds: -0.5695\n",
        "Standard Deviation of Reward: 0.8657\n",
        "Average Episode Length across all seeds: 8.46\n",
        "Standard Deviation of Episode Length: 2.92"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAnihn-h1LIM"
      },
      "outputs": [],
      "source": [
        "--- Two-Object Search Policy Results ---\n",
        "Total experiment time across 100 seeds: 486.24 seconds\n",
        "Average Success Rate over 100 seeds and 100 episodes each: 0.6191 (61.91%)\n",
        "Standard Deviation of Success Rate: 0.0466\n",
        "Average Detection Time (Successful Episodes) across all seeds: 4.35\n",
        "Standard Deviation of Detection Time: 2.45\n",
        "Average Episode Reward across all seeds: -0.4749\n",
        "Standard Deviation of Reward: 0.9713\n",
        "Average Episode Length across all seeds: 6.50\n",
        "Standard Deviation of Episode Length: 3.36"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EukiImBu8W41"
      },
      "outputs": [],
      "source": [
        "--- Overall 0-Step Lookahead Policy Results with Adaptive DP Replanning ---\n",
        "Total experiment time across 100 seeds: 3.36 seconds\n",
        "Average Success Rate over 100 seeds and 100 episodes each: 0.4830 (48.30%)\n",
        "Standard Deviation of Success Rate: 0.0443\n",
        "Average Detection Time (Successful Episodes) across all seeds: 4.32\n",
        "Standard Deviation of Detection Time: 2.37\n",
        "Average Episode Reward across all seeds: -0.5364\n",
        "Standard Deviation of Reward: 0.8706\n",
        "Average Episode Length across all seeds: 7.25\n",
        "Standard Deviation of Episode Length: 3.28"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_problem_parameters(n: int, seed: int = 42) -> tuple:\n",
        "    \"\"\"\n",
        "    Generates correlated joint prior, cost vector, and miss-detection rates\n",
        "    for a two-object search problem.\n",
        "\n",
        "    Args:\n",
        "        n (int): The number of cells.\n",
        "        seed (int): Seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (prior_joint, gamma1, gamma2, c)\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # 1. Generate gamma1 in a range that allows gamma2 to be smaller\n",
        "    gamma1 = np.random.uniform(low=0.6, high=0.85, size=n)\n",
        "\n",
        "    # 2. Generate gamma2 such that gamma2 < gamma1, and within its own desired range [0.15, 0.25]\n",
        "    gamma2 = np.zeros(n)\n",
        "    min_gamma2_val = 0.15\n",
        "    max_gamma2_val = 0.25\n",
        "    for i in range(n):\n",
        "        # Upper bound for gamma2 is min(max_gamma2_val, gamma1[i] - 0.02)\n",
        "        upper_bound_for_this_gamma2 = min(max_gamma2_val, gamma1[i] - 0.02) # Small buffer to ensure gamma2 < gamma1\n",
        "        if upper_bound_for_this_gamma2 < min_gamma2_val: # If buffer makes it too small, ensure min\n",
        "            gamma2[i] = np.random.uniform(low=min_gamma2_val, high=min_gamma2_val + 0.01) # Force a small valid range\n",
        "        else:\n",
        "            gamma2[i] = np.random.uniform(low=min_gamma2_val, high=upper_bound_for_this_gamma2)\n",
        "    gamma2 = np.clip(gamma2, None, max_gamma2_val) # Ensure max bound\n",
        "\n",
        "    # 3. Generate cost vector c\n",
        "    c = np.random.uniform(low=0.1, high=0.3, size=n)\n",
        "\n",
        "    # 4. Generate correlated joint prior (n x n matrix)\n",
        "    prior_joint = np.zeros((n, n))\n",
        "    correlation_strength = 2.0 # Adjust this value to control correlation strength\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            # Higher probability when O1 and O2 are in the same or adjacent cells\n",
        "            distance = abs(i - j)\n",
        "            prior_joint[i, j] = np.exp(-correlation_strength * distance) # Exponential decay with distance\n",
        "\n",
        "    # Add some random noise to ensure variety (but keep it small)\n",
        "    prior_joint += np.random.uniform(low=0.01, high=0.05, size=(n, n))\n",
        "    prior_joint[prior_joint < 0.001] = 0.001 # Ensure no zero probabilities\n",
        "\n",
        "    # Normalize the joint prior\n",
        "    prior_joint /= np.sum(prior_joint)\n",
        "\n",
        "    # Explicitly ensure the sum is exactly 1.0 by adjusting the last element\n",
        "    # This handles potential floating-point inaccuracies after normalization.\n",
        "    prior_joint[-1, -1] += (1.0 - np.sum(prior_joint))\n",
        "\n",
        "    return prior_joint, gamma1, gamma2, c\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example usage:\n",
        "    NUM_CELLS = 100\n",
        "    SEED = 42\n",
        "\n",
        "    prior_joint, gamma1, gamma2, c = generate_problem_parameters(NUM_CELLS, SEED)\n",
        "\n",
        "    print(f\"Generated Parameters (N={NUM_CELLS}, Seed={SEED}):\\n\")\n",
        "    print(\"Joint Prior (p(O1, O2)):\\n\", np.round(prior_joint, 4))\n",
        "    print(\"\\nSum of Joint Prior: \", np.sum(prior_joint))\n",
        "    print(\"\\nMiss-detection rates for O1 (gamma1):\", np.round(gamma1, 4))\n",
        "    print(\"Miss-detection rates for O2 (gamma2):\", np.round(gamma2, 4))\n",
        "    print(\"\\nCost vector (c):\", np.round(c, 4))\n",
        "\n",
        "    # Verify gamma1 > gamma2\n",
        "    print(\"\\nIs gamma1 > gamma2 for all elements?:\", np.all(gamma1 > gamma2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwK0IBwVrybK",
        "outputId": "ae7b051f-30cd-443a-c3fd-032aa241d1f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Parameters (N=100, Seed=42):\n",
            "\n",
            "Joint Prior (p(O1, O2)):\n",
            " [[0.0024 0.0004 0.0001 ... 0.0001 0.0001 0.0001]\n",
            " [0.0003 0.0024 0.0004 ... 0.     0.0001 0.0001]\n",
            " [0.0001 0.0004 0.0024 ... 0.     0.     0.    ]\n",
            " ...\n",
            " [0.0001 0.0001 0.     ... 0.0024 0.0004 0.0001]\n",
            " [0.0001 0.0001 0.0001 ... 0.0004 0.0024 0.0004]\n",
            " [0.0001 0.     0.0001 ... 0.0001 0.0003 0.0024]]\n",
            "\n",
            "Sum of Joint Prior:  1.0\n",
            "\n",
            "Miss-detection rates for O1 (gamma1): [0.6936 0.8377 0.783  0.7497 0.639  0.639  0.6145 0.8165 0.7503 0.777\n",
            " 0.6051 0.8425 0.8081 0.6531 0.6455 0.6459 0.6761 0.7312 0.708  0.6728\n",
            " 0.753  0.6349 0.673  0.6916 0.714  0.7963 0.6499 0.7286 0.7481 0.6116\n",
            " 0.7519 0.6426 0.6163 0.8372 0.8414 0.8021 0.6762 0.6244 0.7711 0.71\n",
            " 0.6305 0.7238 0.6086 0.8273 0.6647 0.7656 0.6779 0.73   0.7367 0.6462\n",
            " 0.8424 0.7938 0.8349 0.8237 0.7495 0.8305 0.6221 0.649  0.6113 0.6813\n",
            " 0.6972 0.6678 0.8072 0.6892 0.6702 0.7357 0.6352 0.8005 0.6186 0.8467\n",
            " 0.7931 0.6497 0.6014 0.8039 0.7767 0.7823 0.7928 0.6185 0.6896 0.629\n",
            " 0.8158 0.7558 0.6827 0.6159 0.6777 0.6813 0.7824 0.7594 0.8218 0.7181\n",
            " 0.6299 0.7783 0.7902 0.7403 0.7927 0.7234 0.7307 0.7069 0.6064 0.627 ]\n",
            "Miss-detection rates for O2 (gamma2): [0.1531 0.2136 0.1814 0.2009 0.2408 0.1749 0.191  0.2256 0.1729 0.1577\n",
            " 0.179  0.1661 0.243  0.2308 0.2133 0.2371 0.2304 0.1687 0.2393 0.2039\n",
            " 0.2307 0.2396 0.1818 0.161  0.1728 0.1927 0.2318 0.2361 0.1507 0.2011\n",
            " 0.1917 0.1722 0.162  0.1838 0.2443 0.1823 0.2019 0.2203 0.1864 0.2472\n",
            " 0.2462 0.1752 0.1997 0.1801 0.1785 0.1537 0.211  0.2003 0.1551 0.1779\n",
            " 0.2408 0.174  0.1645 0.1989 0.2486 0.1742 0.2172 0.2262 0.1738 0.2228\n",
            " 0.1868 0.2132 0.2134 0.2036 0.159  0.2335 0.1821 0.1687 0.1541 0.2091\n",
            " 0.2178 0.1517 0.2012 0.1726 0.2145 0.1674 0.2191 0.1887 0.2437 0.1638\n",
            " 0.1841 0.1613 0.2425 0.2377 0.1758 0.216  0.2317 0.2055 0.203  0.1742\n",
            " 0.1593 0.2397 0.24   0.2133 0.1839 0.1849 0.2226 0.2397 0.2387 0.228 ]\n",
            "\n",
            "Cost vector (c): [0.2284 0.1168 0.1323 0.2797 0.2213 0.1018 0.1203 0.2327 0.101  0.1322\n",
            " 0.2097 0.2384 0.2304 0.1449 0.2424 0.1474 0.1651 0.2493 0.2299 0.2698\n",
            " 0.2315 0.2137 0.1187 0.1735 0.153  0.1488 0.2946 0.1786 0.2784 0.2262\n",
            " 0.259  0.2005 0.2154 0.1985 0.139  0.2445 0.1562 0.1049 0.2291 0.1354\n",
            " 0.2881 0.2908 0.283  0.174  0.1031 0.2857 0.1856 0.2933 0.2927 0.2706\n",
            " 0.1589 0.177  0.2702 0.1634 0.1339 0.2114 0.2872 0.2392 0.214  0.1194\n",
            " 0.223  0.298  0.128  0.2037 0.2755 0.2482 0.2394 0.2405 0.1719 0.1587\n",
            " 0.2619 0.262  0.2734 0.2826 0.2023 0.2003 0.2597 0.23   0.2404 0.2592\n",
            " 0.278  0.1676 0.1751 0.1188 0.2157 0.1072 0.1931 0.2085 0.1573 0.2182\n",
            " 0.1061 0.1075 0.2645 0.172  0.1254 0.2044 0.254  0.1432 0.2246 0.1171]\n",
            "\n",
            "Is gamma1 > gamma2 for all elements?: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F2xpAfacuAJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e799c9bf"
      },
      "source": [
        "# Task\n",
        "Implement a greedy search policy for the two-object search problem. This involves creating a `run_greedy_episode` function that simulates an episode by always choosing the action that maximizes immediate expected reward based on current belief. Integrate the problem parameters (NUM_CELLS, TIME_HORIZON, prior_joint, gamma1, gamma2, c) from the `generate_problem_parameters` function in cell `gwK0IBwVrybK`. Conduct an experiment over 100 seeds, running 100 episodes per seed, and collect and display the average and standard deviation of success rates, detection times, episode rewards, and episode lengths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13cdd056"
      },
      "source": [
        "## Implement Greedy Search Episode Simulation\n",
        "\n",
        "### Subtask:\n",
        "Create a `run_greedy_episode` function that simulates a single episode using a purely greedy policy, which always selects the action maximizing immediate expected reward based on the current belief state. This function will also handle object location sampling and dynamic belief state updates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e32fa493"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to implement the `run_greedy_episode` function as described in the instructions, which involves setting up the simulation environment, making decisions based on a greedy policy, and updating the state and beliefs over time. I will also reuse the `calculate_joint_posterior`, `calculate_conditional_posterior`, and `precompute_p0_conditionals` helper functions that were defined in the previous code block (CNgLojO6pzq_).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7933ba94",
        "outputId": "3d5cd6d8-7abb-4c31-d6f4-1d4fd95a06f7"
      },
      "source": [
        "import numpy as np\n",
        "from typing import Tuple, Any\n",
        "\n",
        "# Helper function definitions (copied from cell CNgLojO6pzq_ for self-containment)\n",
        "# Note: These functions are expected to be available in the environment.\n",
        "# If they were not defined in a previous cell, they would need to be moved here.\n",
        "\n",
        "def calculate_joint_posterior(z_vector: np.ndarray, initial_prior_joint: np.ndarray,\n",
        "                              gamma1: np.ndarray, gamma2: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Calculates the joint posterior belief p(O1=i, O2=j | z) based on initial joint prior.\"\"\" #5\n",
        "    g1_z = np.power(gamma1, z_vector)\n",
        "    g2_z = np.power(gamma2, z_vector)\n",
        "    likelihood = np.outer(g1_z, g2_z)\n",
        "    numerator = initial_prior_joint * likelihood\n",
        "    norm = np.sum(numerator)\n",
        "    return numerator / norm if norm > 0 else numerator\n",
        "\n",
        "def calculate_conditional_posterior(z_vector: np.ndarray, o2_loc: int,\n",
        "                                    precomputed_conditionals: list, gamma1: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Calculates the conditional posterior belief p(O1=i | z, O2=k) based on precomputed conditionals and current z_vector.\"\"\"\n",
        "    p0_conditional = precomputed_conditionals[o2_loc]\n",
        "    g1_z = np.power(gamma1, z_vector)\n",
        "    numerator = g1_z * p0_conditional\n",
        "    norm = np.sum(numerator)\n",
        "    return numerator / norm if norm > 0 else numerator\n",
        "\n",
        "def precompute_p0_conditionals(n: int, p0_joint: np.ndarray) -> list:\n",
        "    \"\"\"Pre-calculates P(O1=i | O2=k) for all k from a joint prior matrix.\"\"\"\n",
        "    if p0_joint.ndim != 2: # Should be (n,n) for joint prior\n",
        "        raise ValueError(\"p0_joint must be a 2D joint probability matrix for precomputing conditionals.\")\n",
        "    conditionals = []\n",
        "    for k in range(n):\n",
        "        marginal_o2 = np.sum(p0_joint[:, k])\n",
        "        conditionals.append(p0_joint[:, k] / marginal_o2 if marginal_o2 > 0 else np.zeros(n))\n",
        "    return conditionals\n",
        "\n",
        "\n",
        "def run_greedy_episode(n: int, T: int, p0_joint: np.ndarray,\n",
        "                           gamma1: np.ndarray, gamma2: np.ndarray, c: np.ndarray, episode_seed: int) -> Tuple[bool, int, float, int]:\n",
        "    \"\"\"\n",
        "    Simulates a single episode using a purely greedy policy, which always selects the action\n",
        "    maximizing immediate expected reward based on the current belief state.\n",
        "    Returns success status, time of detection (or -1 if failed), accumulated reward, and episode length.\n",
        "    \"\"\"\n",
        "    # 1. Initialize a local random number generator\n",
        "    rng = np.random.default_rng(episode_seed)\n",
        "\n",
        "    # 2. Secretly determine the true locations of O1 and O2\n",
        "    flat_p0 = p0_joint.flatten()\n",
        "    if np.sum(flat_p0) == 0: # Handle case where prior is all zeros\n",
        "         true_pos_o1, true_pos_o2 = -1, -1 # Indicate no target\n",
        "    else:\n",
        "        choice_index = rng.choice(n * n, p=flat_p0 / np.sum(flat_p0)) # Normalize\n",
        "        true_pos_o1, true_pos_o2 = np.unravel_index(choice_index, (n, n))\n",
        "\n",
        "    # 3. Initialize state variables\n",
        "    z_vector = np.zeros(n, dtype=int)\n",
        "    theta2 = 0 # 0=hidden, >0=found at cell (theta2-1)\n",
        "    current_belief_joint = np.copy(p0_joint) # Initial joint belief\n",
        "\n",
        "    # 4. Precompute the conditional priors from p0_joint (used if O2 is found)\n",
        "    p0_conditionals = precompute_p0_conditionals(n, p0_joint)\n",
        "\n",
        "    # 5. Initialize reward and time tracking\n",
        "    accumulated_reward = 0.0\n",
        "    detection_time = -1\n",
        "    episode_length = 0\n",
        "\n",
        "    # 6. Loop for T steps\n",
        "    for t in range(T):\n",
        "        episode_length += 1\n",
        "\n",
        "        # 7. Calculate the immediate expected reward for each possible action\n",
        "        action_rewards = np.zeros(n)\n",
        "\n",
        "        if theta2 == 0: # O2 is hidden, use joint posterior\n",
        "            # Update current_belief_joint based on z_vector\n",
        "            current_posterior = calculate_joint_posterior(z_vector, p0_joint, gamma1, gamma2)\n",
        "            for a in range(n):\n",
        "                p_marginal_o1_at_a = np.sum(current_posterior[a, :])\n",
        "                action_rewards[a] = (1 - gamma1[a]) * p_marginal_o1_at_a - c[a]\n",
        "        else: # O2 is found at theta2 - 1, use conditional posterior for O1\n",
        "            o2_loc = theta2 - 1\n",
        "            # Update current_belief_conditional based on z_vector\n",
        "            current_posterior = calculate_conditional_posterior(z_vector, o2_loc, p0_conditionals, gamma1)\n",
        "            for a in range(n):\n",
        "                p_o1_at_a = current_posterior[a]\n",
        "                action_rewards[a] = (1 - gamma1[a]) * p_o1_at_a - c[a]\n",
        "\n",
        "        # 8. Select the action maximizing immediate expected reward\n",
        "        a_t = np.argmax(action_rewards)\n",
        "\n",
        "        # 9. Deduct cost for the action\n",
        "        accumulated_reward -= c[a_t]\n",
        "\n",
        "        # 10. Simulate the outcome of searching cell a_t\n",
        "        found_o1 = (a_t == true_pos_o1) and (rng.random() > gamma1[a_t])\n",
        "        if found_o1:\n",
        "            accumulated_reward += 1.0 # Reward for finding O1\n",
        "            detection_time = t + 1 # Absolute time of detection\n",
        "            return True, detection_time, accumulated_reward, episode_length # Mission Success!\n",
        "\n",
        "        # If O1 not found, check for O2 discovery (only if O2 was previously hidden)\n",
        "        if theta2 == 0 and (a_t == true_pos_o2) and (rng.random() > gamma2[a_t]):\n",
        "            theta2 = a_t + 1 # Update theta2 state (O2 found at a_t)\n",
        "\n",
        "        # 11. Update z_vector\n",
        "        z_vector[a_t] += 1\n",
        "\n",
        "        # 12. Update the belief state for the next time step\n",
        "        # The belief `current_posterior` calculated above is the *past* belief for decision making.\n",
        "        # The `calculate_posterior` functions directly use the current `z_vector` which has been updated.\n",
        "        # No need to explicitly re-assign `current_belief_joint` or `current_belief_conditional` here,\n",
        "        # as they are re-calculated at the start of the next loop iteration (step 7) based on `z_vector` and `theta2`.\n",
        "\n",
        "    # 13. If loop completes without finding O1\n",
        "    return False, detection_time, accumulated_reward, episode_length\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example usage for testing the greedy policy\n",
        "    NUM_CELLS = 25\n",
        "    TIME_HORIZON = 20\n",
        "    NUM_EPISODES = 10 # Number of episodes to run for this benchmark\n",
        "\n",
        "    \"\"\"# Problem parameters (same as in previous cells)\n",
        "    prior = np.array([\n",
        "        [0.152,  0.0039, 0.003,  0.0108, 0.011],\n",
        "        [0.0038, 0.0052, 0.117,  0.0162, 0.165],\n",
        "        [0.0057, 0.195,  0.015,  0.009,  0.011],\n",
        "        [0.0038, 0.0091, 0.0075, 0.027,  0.011],\n",
        "        [0.0247, 0.0468, 0.0075, 0.117,  0.022]\n",
        "    ])\n",
        "\n",
        "    gammas1 = np.array([0.8, 0.65, 0.82, 0.75, 0.7])\n",
        "    gammas2 = np.array([0.2, 0.1, 0.25, 0.15, 0.2])\n",
        "    c = np.array([0.15,0.2,0.25,0.1,0.2])\"\"\"\n",
        "\n",
        "    print(\"--- Running Empirical Experiment for Greedy Policy ---\")\n",
        "    print(f\"Number of episodes: {NUM_EPISODES}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "\n",
        "    SEED = 42\n",
        "\n",
        "    prior, gammas1, gammas2, c = generate_problem_parameters(NUM_CELLS, SEED)\n",
        "\n",
        "\n",
        "    assert (prior[0][0], prior[24][24]) == (0.020334334191265974, 0.020140323245187997)\n",
        "    assert (gammas1[0], gammas1[1]) == (0.6936350297118405, 0.837678576602479)\n",
        "    assert (gammas2[0], gammas2[1]) == (0.22851759613930137, 0.16996737821583596)\n",
        "    assert (c[0], c[1]) ==( 0.2939169255529117, 0.2550265646722229)\n",
        "\n",
        "\n",
        "\n",
        "    all_success_rates = []\n",
        "    all_detection_times = []\n",
        "    all_rewards = []\n",
        "    all_episode_lengths = []\n",
        "    total_experiment_start_time = time.time()\n",
        "\n",
        "    # Running multiple seeds for statistical robustness (similar to other policies)\n",
        "    NUM_SEEDS = 10\n",
        "    NUM_EPISODES_PER_SEED = 10 # Total episodes will be NUM_SEEDS * NUM_EPISODES_PER_SEED\n",
        "\n",
        "    for seed in range(NUM_SEEDS):\n",
        "        # Use a consistent seed for np.random in the greedy episode function\n",
        "        # to ensure reproducibility of the episode outcomes for a given seed.\n",
        "        # This seed is then used by the rng object inside run_greedy_episode.\n",
        "        # np.random.seed(seed) # Not needed as rng is passed per episode\n",
        "\n",
        "        num_successes = 0\n",
        "        detection_times_this_seed = []\n",
        "        rewards_this_seed = []\n",
        "        episode_lengths_this_seed = []\n",
        "        start_time_this_seed = time.time()\n",
        "\n",
        "        for i in range(NUM_EPISODES_PER_SEED):\n",
        "            episode_unique_seed = seed * NUM_EPISODES_PER_SEED + i\n",
        "            success, detection_time, reward, episode_length = run_greedy_episode(\n",
        "                NUM_CELLS, TIME_HORIZON, prior, gammas1, gammas2, c, episode_unique_seed\n",
        "            )\n",
        "            rewards_this_seed.append(reward)\n",
        "            episode_lengths_this_seed.append(episode_length)\n",
        "\n",
        "            if success:\n",
        "                num_successes += 1\n",
        "                detection_times_this_seed.append(detection_time)\n",
        "\n",
        "        end_time_this_seed = time.time()\n",
        "        success_rate_this_seed = num_successes / NUM_EPISODES_PER_SEED\n",
        "        all_success_rates.append(success_rate_this_seed)\n",
        "        all_detection_times.extend(detection_times_this_seed)\n",
        "        all_rewards.extend(rewards_this_seed)\n",
        "        all_episode_lengths.extend(episode_lengths_this_seed)\n",
        "\n",
        "        print(f\"--- Seed {seed} Results ---\")\n",
        "        print(f\"Success Rate: {success_rate_this_seed:.4f} ({success_rate_this_seed*100:.2f}%)\")\n",
        "        if detection_times_this_seed:\n",
        "            print(f\"Average Detection Time (Successful Episodes): {np.mean(detection_times_this_seed):.2f}\")\n",
        "        else:\n",
        "            print(\"Average Detection Time (Successful Episodes): N/A (no successes)\")\n",
        "        print(f\"Average Episode Reward: {np.mean(rewards_this_seed):.4f}\")\n",
        "        print(f\"Average Episode Length: {np.mean(episode_lengths_this_seed):.2f}\")\n",
        "        print(f\"Time taken for this seed: {end_time_this_seed - start_time_this_seed:.2f} seconds\")\n",
        "\n",
        "    total_experiment_end_time = time.time()\n",
        "\n",
        "    mean_success_rate = np.mean(all_success_rates)\n",
        "    std_success_rate = np.std(all_success_rates)\n",
        "    mean_detection_time = np.mean(all_detection_times) if all_detection_times else -1\n",
        "    std_detection_time = np.std(all_detection_times) if all_detection_times else -1\n",
        "    mean_reward = np.mean(all_rewards)\n",
        "    std_reward = np.std(all_rewards)\n",
        "    mean_episode_length = np.mean(all_episode_lengths)\n",
        "    std_episode_length = np.std(all_episode_lengths)\n",
        "\n",
        "    print(\"\\n--- Overall Greedy Policy Results ---\")\n",
        "    print(f\"Total experiment time across {NUM_SEEDS} seeds: {total_experiment_end_time - total_experiment_start_time:.2f} seconds\")\n",
        "    print(f\"Average Success Rate over {NUM_SEEDS} seeds and {NUM_EPISODES_PER_SEED} episodes each: {mean_success_rate:.4f} ({mean_success_rate*100:.2f}%)\")\n",
        "    print(f\"Standard Deviation of Success Rate: {std_success_rate:.4f}\")\n",
        "    if all_detection_times:\n",
        "        print(f\"Average Detection Time (Successful Episodes) across all seeds: {mean_detection_time:.2f}\")\n",
        "        print(f\"Standard Deviation of Detection Time: {std_detection_time:.2f}\")\n",
        "    else:\n",
        "        print(\"Average Detection Time (Successful Episodes) across all seeds: N/A (no successes)\")\n",
        "\n",
        "    print(f\"Average Episode Reward across all seeds: {mean_reward:.4f}\")\n",
        "    print(f\"Standard Deviation of Reward: {std_reward:.4f}\")\n",
        "    print(f\"Average Episode Length across all seeds: {mean_episode_length:.2f}\")\n",
        "    print(f\"Standard Deviation of Episode Length: {std_episode_length:.2f}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Empirical Experiment for Greedy Policy ---\n",
            "Number of episodes: 10\n",
            "--------------------\n",
            "--- Seed 0 Results ---\n",
            "Success Rate: 0.2000 (20.00%)\n",
            "Average Detection Time (Successful Episodes): 7.50\n",
            "Average Episode Reward: -1.5972\n",
            "Average Episode Length: 17.50\n",
            "Time taken for this seed: 0.03 seconds\n",
            "--- Seed 1 Results ---\n",
            "Success Rate: 0.1000 (10.00%)\n",
            "Average Detection Time (Successful Episodes): 8.00\n",
            "Average Episode Reward: -1.8410\n",
            "Average Episode Length: 18.80\n",
            "Time taken for this seed: 0.02 seconds\n",
            "--- Seed 2 Results ---\n",
            "Success Rate: 0.1000 (10.00%)\n",
            "Average Detection Time (Successful Episodes): 8.00\n",
            "Average Episode Reward: -1.8268\n",
            "Average Episode Length: 18.80\n",
            "Time taken for this seed: 0.03 seconds\n",
            "--- Seed 3 Results ---\n",
            "Success Rate: 0.2000 (20.00%)\n",
            "Average Detection Time (Successful Episodes): 4.00\n",
            "Average Episode Reward: -1.5267\n",
            "Average Episode Length: 16.80\n",
            "Time taken for this seed: 0.02 seconds\n",
            "--- Seed 4 Results ---\n",
            "Success Rate: 0.1000 (10.00%)\n",
            "Average Detection Time (Successful Episodes): 3.00\n",
            "Average Episode Reward: -1.7723\n",
            "Average Episode Length: 18.30\n",
            "Time taken for this seed: 0.03 seconds\n",
            "--- Seed 5 Results ---\n",
            "Success Rate: 0.0000 (0.00%)\n",
            "Average Detection Time (Successful Episodes): N/A (no successes)\n",
            "Average Episode Reward: -2.0554\n",
            "Average Episode Length: 20.00\n",
            "Time taken for this seed: 0.03 seconds\n",
            "--- Seed 6 Results ---\n",
            "Success Rate: 0.1000 (10.00%)\n",
            "Average Detection Time (Successful Episodes): 1.00\n",
            "Average Episode Reward: -1.7608\n",
            "Average Episode Length: 18.10\n",
            "Time taken for this seed: 0.02 seconds\n",
            "--- Seed 7 Results ---\n",
            "Success Rate: 0.3000 (30.00%)\n",
            "Average Detection Time (Successful Episodes): 8.67\n",
            "Average Episode Reward: -1.4051\n",
            "Average Episode Length: 16.60\n",
            "Time taken for this seed: 0.02 seconds\n",
            "--- Seed 8 Results ---\n",
            "Success Rate: 0.0000 (0.00%)\n",
            "Average Detection Time (Successful Episodes): N/A (no successes)\n",
            "Average Episode Reward: -2.0554\n",
            "Average Episode Length: 20.00\n",
            "Time taken for this seed: 0.03 seconds\n",
            "--- Seed 9 Results ---\n",
            "Success Rate: 0.1000 (10.00%)\n",
            "Average Detection Time (Successful Episodes): 8.00\n",
            "Average Episode Reward: -1.8436\n",
            "Average Episode Length: 18.80\n",
            "Time taken for this seed: 0.02 seconds\n",
            "\n",
            "--- Overall Greedy Policy Results ---\n",
            "Total experiment time across 10 seeds: 0.25 seconds\n",
            "Average Success Rate over 10 seeds and 10 episodes each: 0.1200 (12.00%)\n",
            "Standard Deviation of Success Rate: 0.0872\n",
            "Average Detection Time (Successful Episodes) across all seeds: 6.42\n",
            "Standard Deviation of Detection Time: 3.97\n",
            "Average Episode Reward across all seeds: -1.7684\n",
            "Standard Deviation of Reward: 0.7881\n",
            "Average Episode Length across all seeds: 18.37\n",
            "Standard Deviation of Episode Length: 4.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "--- Overall 1-step Lookahead Policy Results ---\n",
        "Total experiment time across 10 seeds: 1933.66 seconds\n",
        "Average Success Rate over 10 seeds and 10 episodes each: 0.2200 (22.00%)\n",
        "Standard Deviation of Success Rate: 0.0872\n",
        "Average Detection Time (Successful Episodes) across all seeds: 11.36\n",
        "Standard Deviation of Detection Time: 6.18\n",
        "Average Episode Reward across all seeds: -2.7383\n",
        "Standard Deviation of Reward: 1.0847\n",
        "Average Episode Length across all seeds: 18.10\n",
        "Standard Deviation of Episode Length: 4.61\n"
      ],
      "metadata": {
        "id": "Jevj8MMf-tTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(c[0],c[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWbw1VoD0DKX",
        "outputId": "b159b754-5603-417f-972e-3a72a5ffca9d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2939169255529117 0.2550265646722229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3iwsvav10EXt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}