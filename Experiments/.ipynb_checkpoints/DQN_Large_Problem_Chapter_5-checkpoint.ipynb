{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abadebe7-219a-42c4-b181-07c7c9a7ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3991dc-1254-4fdb-ad5e-8237079afff0",
   "metadata": {},
   "source": [
    "### Scrip for the large Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8043525f-c9c6-439b-aff3-23a18a04288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_problem_parameters(n: int, seed: int = 42) -> tuple:\n",
    "    \"\"\"\n",
    "    Generates correlated joint prior, cost vector, and miss-detection rates\n",
    "    for a two-object search problem.\n",
    "\n",
    "    Args:\n",
    "        n (int): The number of cells.\n",
    "        seed (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (prior_joint, gamma1, gamma2, c)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 1. Generate gamma1 in a range that allows gamma2 to be smaller\n",
    "    gamma1 = np.random.uniform(low=0.6, high=0.85, size=n)\n",
    "\n",
    "    # 2. Generate gamma2 such that gamma2 < gamma1, and within its own desired range [0.15, 0.25]\n",
    "    gamma2 = np.zeros(n)\n",
    "    min_gamma2_val = 0.15\n",
    "    max_gamma2_val = 0.25\n",
    "    for i in range(n):\n",
    "        # Upper bound for gamma2 is min(max_gamma2_val, gamma1[i] - 0.02)\n",
    "        upper_bound_for_this_gamma2 = min(max_gamma2_val, gamma1[i] - 0.02) # Small buffer to ensure gamma2 < gamma1\n",
    "        if upper_bound_for_this_gamma2 < min_gamma2_val: # If buffer makes it too small, ensure min\n",
    "            gamma2[i] = np.random.uniform(low=min_gamma2_val, high=min_gamma2_val + 0.01) # Force a small valid range\n",
    "        else:\n",
    "            gamma2[i] = np.random.uniform(low=min_gamma2_val, high=upper_bound_for_this_gamma2)\n",
    "    gamma2 = np.clip(gamma2, None, max_gamma2_val) # Ensure max bound\n",
    "\n",
    "    # 3. Generate cost vector c\n",
    "    c = np.random.uniform(low=0.1, high=0.3, size=n)\n",
    "\n",
    "    # 4. Generate correlated joint prior (n x n matrix)\n",
    "    prior_joint = np.zeros((n, n))\n",
    "    correlation_strength = 2.0 # Adjust this value to control correlation strength\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # Higher probability when O1 and O2 are in the same or adjacent cells\n",
    "            distance = abs(i - j)\n",
    "            prior_joint[i, j] = np.exp(-correlation_strength * distance) # Exponential decay with distance\n",
    "\n",
    "    # Add some random noise to ensure variety (but keep it small)\n",
    "    prior_joint += np.random.uniform(low=0.01, high=0.05, size=(n, n))\n",
    "    prior_joint[prior_joint < 0.001] = 0.001 # Ensure no zero probabilities\n",
    "\n",
    "    # Normalize the joint prior\n",
    "    prior_joint /= np.sum(prior_joint)\n",
    "\n",
    "    # Explicitly ensure the sum is exactly 1.0 by adjusting the last element\n",
    "    # This handles potential floating-point inaccuracies after normalization.\n",
    "    prior_joint[-1, -1] += (1.0 - np.sum(prior_joint))\n",
    "\n",
    "    return prior_joint, gamma1, gamma2, c\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage:\n",
    "    \"\"\"NUM_CELLS = 100\n",
    "    SEED = 42\n",
    "\n",
    "    prior_joint, gamma1, gamma2, c = generate_problem_parameters(NUM_CELLS, SEED)\n",
    "\n",
    "    print(f\"Generated Parameters (N={NUM_CELLS}, Seed={SEED}):\\n\")\n",
    "    print(\"Joint Prior (p(O1, O2)):\\n\", np.round(prior_joint, 4))\n",
    "    print(\"\\nSum of Joint Prior: \", np.sum(prior_joint))\n",
    "    print(\"\\nMiss-detection rates for O1 (gamma1):\", np.round(gamma1, 4))\n",
    "    print(\"Miss-detection rates for O2 (gamma2):\", np.round(gamma2, 4))\n",
    "    print(\"\\nCost vector (c):\", np.round(c, 4))\n",
    "\n",
    "    # Verify gamma1 > gamma2\n",
    "    print(\"\\nIs gamma1 > gamma2 for all elements?:\", np.all(gamma1 > gamma2))\"\"\"\n",
    "    print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7375e1cd-bd08-44df-8392-d0cd3266e819",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 14:52:27.337050: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-08 14:52:27.426187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-08 14:52:33.383380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== STARTING DQN RUN 0 with SEED 0 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jey/anaconda3/envs/dataanot/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=-2.32 +/- 0.97\n",
      "Episode length: 18.10 +/- 3.96\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-2.07 +/- 1.38\n",
      "Episode length: 16.60 +/- 6.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-2.57 +/- 1.19\n",
      "Episode length: 18.10 +/- 5.70\n",
      "Eval num_timesteps=40000, episode_reward=-1.76 +/- 0.79\n",
      "Episode length: 18.50 +/- 4.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-5.10 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-4.02 +/- 1.69\n",
      "Episode length: 18.10 +/- 5.70\n",
      "Eval num_timesteps=70000, episode_reward=-3.17 +/- 1.39\n",
      "Episode length: 18.10 +/- 5.70\n",
      "Eval num_timesteps=80000, episode_reward=-1.95 +/- 0.53\n",
      "Episode length: 19.40 +/- 1.80\n",
      "Eval num_timesteps=90000, episode_reward=-2.80 +/- 0.57\n",
      "Episode length: 19.40 +/- 1.80\n",
      "Eval num_timesteps=100000, episode_reward=-1.79 +/- 0.90\n",
      "Episode length: 18.20 +/- 5.40\n",
      "Eval num_timesteps=110000, episode_reward=-4.32 +/- 0.83\n",
      "Episode length: 19.20 +/- 2.40\n",
      "Eval num_timesteps=120000, episode_reward=-1.73 +/- 0.91\n",
      "Episode length: 18.10 +/- 5.70\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-1.86 +/- 0.64\n",
      "Episode length: 19.50 +/- 1.20\n",
      "Eval num_timesteps=140000, episode_reward=-1.53 +/- 1.13\n",
      "Episode length: 16.70 +/- 6.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-2.13 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-4.29 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-1.95 +/- 0.39\n",
      "Episode length: 19.80 +/- 0.60\n",
      "Eval num_timesteps=180000, episode_reward=-1.74 +/- 1.00\n",
      "Episode length: 17.70 +/- 5.44\n",
      "Eval num_timesteps=190000, episode_reward=-1.61 +/- 0.89\n",
      "Episode length: 17.80 +/- 4.94\n",
      "Eval num_timesteps=200000, episode_reward=-3.34 +/- 1.27\n",
      "Episode length: 18.10 +/- 4.53\n",
      "Evaluating best model for run 0...\n",
      "Run 0 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.0900 (9.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.22\n",
      "  Average Episode Reward: -1.8632\n",
      "  Average Episode Length: 18.58\n",
      "===== COMPLETED DQN RUN 0 ====\n",
      "\n",
      "===== STARTING DQN RUN 1 with SEED 1 ====\n",
      "Eval num_timesteps=10000, episode_reward=-2.86 +/- 0.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-2.45 +/- 0.75\n",
      "Episode length: 19.10 +/- 2.70\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-2.07 +/- 0.58\n",
      "Episode length: 19.60 +/- 1.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-1.39 +/- 1.41\n",
      "Episode length: 14.90 +/- 7.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-1.38 +/- 1.38\n",
      "Episode length: 14.90 +/- 7.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-1.43 +/- 1.29\n",
      "Episode length: 15.50 +/- 6.89\n",
      "Eval num_timesteps=70000, episode_reward=-3.24 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-2.12 +/- 0.95\n",
      "Episode length: 18.10 +/- 3.81\n",
      "Eval num_timesteps=90000, episode_reward=-2.48 +/- 0.12\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-2.20 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-1.90 +/- 0.79\n",
      "Episode length: 18.50 +/- 3.20\n",
      "Eval num_timesteps=120000, episode_reward=-2.21 +/- 1.21\n",
      "Episode length: 17.10 +/- 4.44\n",
      "Eval num_timesteps=130000, episode_reward=-1.89 +/- 0.93\n",
      "Episode length: 18.20 +/- 5.40\n",
      "Eval num_timesteps=140000, episode_reward=-3.01 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-2.80 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-3.14 +/- 1.31\n",
      "Episode length: 18.20 +/- 5.40\n",
      "Eval num_timesteps=170000, episode_reward=-2.19 +/- 0.71\n",
      "Episode length: 19.00 +/- 2.05\n",
      "Eval num_timesteps=180000, episode_reward=-1.82 +/- 1.32\n",
      "Episode length: 16.50 +/- 7.00\n",
      "Eval num_timesteps=190000, episode_reward=-1.55 +/- 1.09\n",
      "Episode length: 16.50 +/- 6.12\n",
      "Eval num_timesteps=200000, episode_reward=-2.75 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Evaluating best model for run 1...\n",
      "Run 1 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.1300 (13.00%)\n",
      "  Average Detection Time (Successful Episodes): 6.62\n",
      "  Average Episode Reward: -2.1126\n",
      "  Average Episode Length: 18.26\n",
      "===== COMPLETED DQN RUN 1 ====\n",
      "\n",
      "===== STARTING DQN RUN 2 with SEED 2 ====\n",
      "Eval num_timesteps=10000, episode_reward=-2.01 +/- 0.99\n",
      "Episode length: 18.20 +/- 3.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-4.20 +/- 0.34\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-2.24 +/- 0.65\n",
      "Episode length: 19.40 +/- 1.50\n",
      "Eval num_timesteps=40000, episode_reward=-2.14 +/- 1.11\n",
      "Episode length: 18.00 +/- 5.37\n",
      "Eval num_timesteps=50000, episode_reward=-1.83 +/- 1.07\n",
      "Episode length: 17.50 +/- 5.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-1.95 +/- 0.92\n",
      "Episode length: 18.00 +/- 4.22\n",
      "Eval num_timesteps=70000, episode_reward=-1.19 +/- 1.42\n",
      "Episode length: 14.10 +/- 7.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-2.22 +/- 0.44\n",
      "Episode length: 19.70 +/- 0.90\n",
      "Eval num_timesteps=90000, episode_reward=-1.72 +/- 1.23\n",
      "Episode length: 16.60 +/- 6.81\n",
      "Eval num_timesteps=100000, episode_reward=-2.08 +/- 0.48\n",
      "Episode length: 19.60 +/- 1.20\n",
      "Eval num_timesteps=110000, episode_reward=-2.40 +/- 1.05\n",
      "Episode length: 18.20 +/- 4.02\n",
      "Eval num_timesteps=120000, episode_reward=-2.25 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-2.18 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-1.74 +/- 0.99\n",
      "Episode length: 17.00 +/- 4.05\n",
      "Eval num_timesteps=150000, episode_reward=-2.05 +/- 0.90\n",
      "Episode length: 18.40 +/- 4.80\n",
      "Eval num_timesteps=160000, episode_reward=-2.21 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-2.19 +/- 0.52\n",
      "Episode length: 19.50 +/- 1.50\n",
      "Eval num_timesteps=180000, episode_reward=-2.04 +/- 0.53\n",
      "Episode length: 19.40 +/- 1.80\n",
      "Eval num_timesteps=190000, episode_reward=-1.94 +/- 0.78\n",
      "Episode length: 18.60 +/- 2.84\n",
      "Eval num_timesteps=200000, episode_reward=-2.90 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Evaluating best model for run 2...\n",
      "Run 2 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.2400 (24.00%)\n",
      "  Average Detection Time (Successful Episodes): 8.25\n",
      "  Average Episode Reward: -1.7679\n",
      "  Average Episode Length: 17.18\n",
      "===== COMPLETED DQN RUN 2 ====\n",
      "\n",
      "===== STARTING DQN RUN 3 with SEED 3 ====\n",
      "Eval num_timesteps=10000, episode_reward=-2.34 +/- 0.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-2.41 +/- 1.10\n",
      "Episode length: 18.20 +/- 5.40\n",
      "Eval num_timesteps=30000, episode_reward=-2.36 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1.91 +/- 0.79\n",
      "Episode length: 18.60 +/- 4.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-1.48 +/- 1.24\n",
      "Episode length: 16.00 +/- 6.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-1.80 +/- 1.01\n",
      "Episode length: 17.50 +/- 5.39\n",
      "Eval num_timesteps=70000, episode_reward=-1.96 +/- 0.60\n",
      "Episode length: 19.20 +/- 2.40\n",
      "Eval num_timesteps=80000, episode_reward=-2.22 +/- 0.78\n",
      "Episode length: 18.60 +/- 2.80\n",
      "Eval num_timesteps=90000, episode_reward=-1.82 +/- 0.76\n",
      "Episode length: 18.60 +/- 4.20\n",
      "Eval num_timesteps=100000, episode_reward=-1.91 +/- 0.97\n",
      "Episode length: 18.10 +/- 5.70\n",
      "Eval num_timesteps=110000, episode_reward=-2.60 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-1.99 +/- 0.96\n",
      "Episode length: 17.90 +/- 4.66\n",
      "Eval num_timesteps=130000, episode_reward=-1.96 +/- 0.65\n",
      "Episode length: 19.00 +/- 3.00\n",
      "Eval num_timesteps=140000, episode_reward=-2.01 +/- 0.91\n",
      "Episode length: 18.30 +/- 5.10\n",
      "Eval num_timesteps=150000, episode_reward=-2.17 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-2.19 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-2.57 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-2.79 +/- 1.03\n",
      "Episode length: 18.60 +/- 4.20\n",
      "Eval num_timesteps=190000, episode_reward=-2.18 +/- 0.95\n",
      "Episode length: 18.30 +/- 5.10\n",
      "Eval num_timesteps=200000, episode_reward=-2.09 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Evaluating best model for run 3...\n",
      "Run 3 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.1900 (19.00%)\n",
      "  Average Detection Time (Successful Episodes): 7.74\n",
      "  Average Episode Reward: -1.7842\n",
      "  Average Episode Length: 17.67\n",
      "===== COMPLETED DQN RUN 3 ====\n",
      "\n",
      "===== STARTING DQN RUN 4 with SEED 4 ====\n",
      "Eval num_timesteps=10000, episode_reward=-2.06 +/- 1.80\n",
      "Episode length: 15.20 +/- 7.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-2.79 +/- 0.37\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-2.55 +/- 0.63\n",
      "Episode length: 19.30 +/- 2.10\n",
      "Eval num_timesteps=40000, episode_reward=-2.04 +/- 0.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-2.43 +/- 0.58\n",
      "Episode length: 19.40 +/- 1.80\n",
      "Eval num_timesteps=60000, episode_reward=-2.89 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-2.13 +/- 0.70\n",
      "Episode length: 19.20 +/- 2.40\n",
      "Eval num_timesteps=80000, episode_reward=-1.78 +/- 1.01\n",
      "Episode length: 17.90 +/- 5.66\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-1.79 +/- 1.26\n",
      "Episode length: 16.70 +/- 6.69\n",
      "Eval num_timesteps=100000, episode_reward=-1.73 +/- 1.11\n",
      "Episode length: 17.10 +/- 5.91\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-2.14 +/- 0.02\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-1.98 +/- 0.33\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-2.59 +/- 0.79\n",
      "Episode length: 18.80 +/- 3.60\n",
      "Eval num_timesteps=140000, episode_reward=-1.36 +/- 1.35\n",
      "Episode length: 15.10 +/- 7.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-1.87 +/- 0.96\n",
      "Episode length: 18.10 +/- 5.70\n",
      "Eval num_timesteps=160000, episode_reward=-2.62 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-2.30 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-2.14 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-1.70 +/- 0.80\n",
      "Episode length: 18.50 +/- 3.88\n",
      "Eval num_timesteps=200000, episode_reward=-2.18 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Evaluating best model for run 4...\n",
      "Run 4 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.0600 (6.00%)\n",
      "  Average Detection Time (Successful Episodes): 2.00\n",
      "  Average Episode Reward: -2.0466\n",
      "  Average Episode Length: 18.92\n",
      "===== COMPLETED DQN RUN 4 ====\n",
      "\n",
      "===== STARTING DQN RUN 5 with SEED 5 ====\n",
      "Eval num_timesteps=10000, episode_reward=-2.02 +/- 1.15\n",
      "Episode length: 17.60 +/- 5.73\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-2.30 +/- 1.57\n",
      "Episode length: 16.20 +/- 6.21\n",
      "Eval num_timesteps=30000, episode_reward=-2.68 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-2.07 +/- 0.83\n",
      "Episode length: 18.60 +/- 4.20\n",
      "Eval num_timesteps=50000, episode_reward=-1.81 +/- 1.07\n",
      "Episode length: 17.40 +/- 5.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-2.13 +/- 0.03\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-2.14 +/- 0.76\n",
      "Episode length: 18.80 +/- 2.40\n",
      "Eval num_timesteps=80000, episode_reward=-1.74 +/- 0.91\n",
      "Episode length: 18.10 +/- 5.70\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-1.84 +/- 0.87\n",
      "Episode length: 18.30 +/- 5.10\n",
      "Eval num_timesteps=100000, episode_reward=-2.31 +/- 0.70\n",
      "Episode length: 19.10 +/- 2.70\n",
      "Eval num_timesteps=110000, episode_reward=-2.31 +/- 0.10\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-1.77 +/- 0.89\n",
      "Episode length: 18.20 +/- 5.40\n",
      "Eval num_timesteps=130000, episode_reward=-2.02 +/- 0.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-1.88 +/- 0.61\n",
      "Episode length: 19.10 +/- 2.70\n",
      "Eval num_timesteps=150000, episode_reward=-2.23 +/- 0.22\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-1.61 +/- 0.97\n",
      "Episode length: 17.10 +/- 4.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=-1.86 +/- 0.83\n",
      "Episode length: 18.40 +/- 4.80\n",
      "Eval num_timesteps=180000, episode_reward=-2.14 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-1.85 +/- 0.70\n",
      "Episode length: 18.80 +/- 3.60\n",
      "Eval num_timesteps=200000, episode_reward=-2.39 +/- 0.98\n",
      "Episode length: 18.50 +/- 4.50\n",
      "Evaluating best model for run 5...\n",
      "Run 5 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.1800 (18.00%)\n",
      "  Average Detection Time (Successful Episodes): 9.50\n",
      "  Average Episode Reward: -1.8574\n",
      "  Average Episode Length: 18.11\n",
      "===== COMPLETED DQN RUN 5 ====\n",
      "\n",
      "===== STARTING DQN RUN 6 with SEED 6 ====\n",
      "Eval num_timesteps=10000, episode_reward=-1.67 +/- 1.29\n",
      "Episode length: 16.10 +/- 5.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-2.87 +/- 1.22\n",
      "Episode length: 18.40 +/- 4.80\n",
      "Eval num_timesteps=30000, episode_reward=-2.41 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1.59 +/- 1.51\n",
      "Episode length: 15.10 +/- 7.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-1.65 +/- 1.30\n",
      "Episode length: 16.30 +/- 7.40\n",
      "Eval num_timesteps=60000, episode_reward=-2.13 +/- 0.58\n",
      "Episode length: 19.30 +/- 2.10\n",
      "Eval num_timesteps=70000, episode_reward=-2.28 +/- 0.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-2.01 +/- 0.91\n",
      "Episode length: 18.30 +/- 5.10\n",
      "Eval num_timesteps=90000, episode_reward=-3.48 +/- 2.52\n",
      "Episode length: 15.00 +/- 7.72\n",
      "Eval num_timesteps=100000, episode_reward=-2.76 +/- 1.15\n",
      "Episode length: 18.30 +/- 5.10\n",
      "Eval num_timesteps=110000, episode_reward=-2.28 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-1.87 +/- 0.97\n",
      "Episode length: 17.80 +/- 4.75\n",
      "Eval num_timesteps=130000, episode_reward=-1.93 +/- 0.75\n",
      "Episode length: 18.70 +/- 3.90\n",
      "Eval num_timesteps=140000, episode_reward=-2.56 +/- 0.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-2.96 +/- 1.11\n",
      "Episode length: 18.30 +/- 3.58\n",
      "Eval num_timesteps=160000, episode_reward=-2.12 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-1.70 +/- 1.07\n",
      "Episode length: 17.30 +/- 5.76\n",
      "Eval num_timesteps=180000, episode_reward=-2.40 +/- 0.01\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-1.87 +/- 0.76\n",
      "Episode length: 18.60 +/- 4.20\n",
      "Eval num_timesteps=200000, episode_reward=-1.90 +/- 0.77\n",
      "Episode length: 18.60 +/- 4.20\n",
      "Evaluating best model for run 6...\n",
      "Run 6 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.1200 (12.00%)\n",
      "  Average Detection Time (Successful Episodes): 8.58\n",
      "  Average Episode Reward: -2.1303\n",
      "  Average Episode Length: 18.63\n",
      "===== COMPLETED DQN RUN 6 ====\n",
      "\n",
      "===== STARTING DQN RUN 7 with SEED 7 ====\n",
      "Eval num_timesteps=10000, episode_reward=-3.03 +/- 0.75\n",
      "Episode length: 19.50 +/- 1.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-3.53 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-2.06 +/- 0.97\n",
      "Episode length: 18.20 +/- 5.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-2.31 +/- 0.41\n",
      "Episode length: 19.80 +/- 0.60\n",
      "Eval num_timesteps=50000, episode_reward=-4.32 +/- 1.53\n",
      "Episode length: 18.40 +/- 4.80\n",
      "Eval num_timesteps=60000, episode_reward=-3.88 +/- 0.08\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-2.18 +/- 0.70\n",
      "Episode length: 19.10 +/- 2.70\n",
      "Eval num_timesteps=80000, episode_reward=-2.74 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1.93 +/- 1.09\n",
      "Episode length: 17.40 +/- 5.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-1.96 +/- 0.77\n",
      "Episode length: 18.60 +/- 4.20\n",
      "Eval num_timesteps=110000, episode_reward=-2.71 +/- 0.49\n",
      "Episode length: 19.60 +/- 1.20\n",
      "Eval num_timesteps=120000, episode_reward=-1.52 +/- 1.51\n",
      "Episode length: 14.80 +/- 7.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-1.57 +/- 1.15\n",
      "Episode length: 16.70 +/- 6.60\n",
      "Eval num_timesteps=140000, episode_reward=-1.76 +/- 0.88\n",
      "Episode length: 18.20 +/- 5.40\n",
      "Eval num_timesteps=150000, episode_reward=-2.03 +/- 0.95\n",
      "Episode length: 18.40 +/- 4.80\n",
      "Eval num_timesteps=160000, episode_reward=-2.63 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-2.44 +/- 1.36\n",
      "Episode length: 17.00 +/- 6.15\n",
      "Eval num_timesteps=180000, episode_reward=-2.65 +/- 0.20\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-2.14 +/- 0.37\n",
      "Episode length: 19.80 +/- 0.60\n",
      "Eval num_timesteps=200000, episode_reward=-2.10 +/- 0.14\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Evaluating best model for run 7...\n",
      "Run 7 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.2100 (21.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.10\n",
      "  Average Episode Reward: -1.8590\n",
      "  Average Episode Length: 16.66\n",
      "===== COMPLETED DQN RUN 7 ====\n",
      "\n",
      "===== STARTING DQN RUN 8 with SEED 8 ====\n",
      "Eval num_timesteps=10000, episode_reward=-2.45 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-2.66 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-2.51 +/- 0.07\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-2.12 +/- 1.01\n",
      "Episode length: 18.20 +/- 4.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-1.81 +/- 1.23\n",
      "Episode length: 16.80 +/- 6.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-4.67 +/- 1.12\n",
      "Episode length: 19.10 +/- 2.70\n",
      "Eval num_timesteps=70000, episode_reward=-3.74 +/- 0.17\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-2.28 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1.79 +/- 1.20\n",
      "Episode length: 16.90 +/- 6.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-2.06 +/- 0.90\n",
      "Episode length: 18.40 +/- 4.80\n",
      "Eval num_timesteps=110000, episode_reward=-2.08 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-2.10 +/- 0.91\n",
      "Episode length: 18.40 +/- 4.80\n",
      "Eval num_timesteps=130000, episode_reward=-1.97 +/- 0.84\n",
      "Episode length: 18.50 +/- 4.50\n",
      "Eval num_timesteps=140000, episode_reward=-1.80 +/- 0.90\n",
      "Episode length: 18.20 +/- 5.40\n",
      "Eval num_timesteps=150000, episode_reward=-1.52 +/- 1.10\n",
      "Episode length: 16.50 +/- 5.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-1.89 +/- 1.21\n",
      "Episode length: 17.00 +/- 6.07\n",
      "Eval num_timesteps=170000, episode_reward=-2.22 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-1.75 +/- 0.97\n",
      "Episode length: 17.90 +/- 5.37\n",
      "Eval num_timesteps=190000, episode_reward=-2.32 +/- 1.07\n",
      "Episode length: 18.20 +/- 5.40\n",
      "Eval num_timesteps=200000, episode_reward=-2.02 +/- 0.90\n",
      "Episode length: 18.40 +/- 4.80\n",
      "Evaluating best model for run 8...\n",
      "Run 8 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.1400 (14.00%)\n",
      "  Average Detection Time (Successful Episodes): 8.57\n",
      "  Average Episode Reward: -1.9909\n",
      "  Average Episode Length: 18.40\n",
      "===== COMPLETED DQN RUN 8 ====\n",
      "\n",
      "===== STARTING DQN RUN 9 with SEED 9 ====\n",
      "Eval num_timesteps=10000, episode_reward=-2.51 +/- 0.06\n",
      "Episode length: 20.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-2.27 +/- 0.67\n",
      "Episode length: 19.00 +/- 3.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-1.74 +/- 0.92\n",
      "Episode length: 18.10 +/- 4.78\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-2.14 +/- 1.03\n",
      "Episode length: 17.90 +/- 4.87\n",
      "Eval num_timesteps=50000, episode_reward=-2.37 +/- 0.99\n",
      "Episode length: 18.40 +/- 4.80\n",
      "Eval num_timesteps=60000, episode_reward=-2.11 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1.51 +/- 1.54\n",
      "Episode length: 15.30 +/- 7.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-2.79 +/- 1.26\n",
      "Episode length: 18.10 +/- 5.70\n",
      "Eval num_timesteps=90000, episode_reward=-1.86 +/- 0.91\n",
      "Episode length: 18.20 +/- 4.49\n",
      "Eval num_timesteps=100000, episode_reward=-1.24 +/- 1.26\n",
      "Episode length: 14.40 +/- 6.96\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-1.82 +/- 0.73\n",
      "Episode length: 18.70 +/- 3.90\n",
      "Eval num_timesteps=120000, episode_reward=-2.23 +/- 0.38\n",
      "Episode length: 19.80 +/- 0.60\n",
      "Eval num_timesteps=130000, episode_reward=-2.18 +/- 0.05\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-1.81 +/- 1.11\n",
      "Episode length: 17.20 +/- 5.21\n",
      "Eval num_timesteps=150000, episode_reward=-1.73 +/- 0.88\n",
      "Episode length: 18.20 +/- 4.49\n",
      "Eval num_timesteps=160000, episode_reward=-2.36 +/- 0.35\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-2.21 +/- 0.04\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-2.72 +/- 0.85\n",
      "Episode length: 18.70 +/- 2.69\n",
      "Eval num_timesteps=190000, episode_reward=-2.21 +/- 0.09\n",
      "Episode length: 20.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-2.52 +/- 0.60\n",
      "Episode length: 19.30 +/- 2.10\n",
      "Evaluating best model for run 9...\n",
      "Run 9 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.1800 (18.00%)\n",
      "  Average Detection Time (Successful Episodes): 7.83\n",
      "  Average Episode Reward: -1.8444\n",
      "  Average Episode Length: 17.81\n",
      "===== COMPLETED DQN RUN 9 ====\n",
      "\n",
      "All 10 DQN training runs are complete.\n",
      "Log files are saved in './dqn_z_vector_logs/'.\n",
      "Total time for all runs: 13210.10 seconds\n",
      "\n",
      "--- Overall DQN Policy Results Across All Seeds ---\n",
      "Average Success Rate: 0.1540 (15.40%)\n",
      "Standard Deviation of Success Rate: 0.0530\n",
      "Average Detection Time (Successful Episodes): 6.74\n",
      "Standard Deviation of Detection Time: 2.34\n",
      "Average Episode Reward: -1.9256\n",
      "Standard Deviation of Episode Reward: 0.1266\n",
      "Average Episode Length: 18.02\n",
      "Standard Deviation of Episode Length: 0.67\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import os\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# =============================================================================\n",
    "# 1. THE (p0, z, theta) ENVIRONMENT\n",
    "# =============================================================================\n",
    "\n",
    "class SearchEnvVec(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom Gymnasium environment for the two-object search problem.\n",
    "\n",
    "    This version uses a flat state representation based on:\n",
    "    1. The search count vector (z_t)\n",
    "    2. The static prior (p_0)\n",
    "    3. The related object status (theta)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p0: np.ndarray, gamma1: np.ndarray, gamma2: np.ndarray, T: int, cost_vector: np.ndarray):\n",
    "        super(SearchEnvVec, self).__init__()\n",
    "\n",
    "        self.n = p0.shape[0]\n",
    "        self.T = T\n",
    "        self.p0 = p0.astype(np.float32)\n",
    "        self.p0_flat = self.p0.flatten()\n",
    "        self.gamma1 = gamma1.astype(np.float32)\n",
    "        self.gamma2 = gamma2.astype(np.float32)\n",
    "        self.cost_vector = cost_vector.astype(np.float32)\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.n)\n",
    "\n",
    "        # --- Define the observation space ---\n",
    "        # 1. z_vector: n elements\n",
    "        # 2. prior: n*n elements\n",
    "        # 3. theta: n+1 elements (one-hot)\n",
    "        obs_size = self.n + (self.n * self.n) + (self.n + 1)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0,\n",
    "            high=float(self.T),\n",
    "            shape=(obs_size,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self._precompute_conditionals()\n",
    "\n",
    "        # Environment state variables\n",
    "        self.z_vector = np.zeros(self.n, dtype=np.int32)\n",
    "        self.theta = 0\n",
    "        self.current_step = 0\n",
    "        self.true_pos_o1 = 0\n",
    "        self.true_pos_o2 = 0\n",
    "\n",
    "    def _precompute_conditionals(self):\n",
    "        self.conditionals = np.zeros_like(self.p0)\n",
    "        for j in range(self.n):\n",
    "            col_sum = np.sum(self.p0[:, j])\n",
    "            if col_sum > 0:\n",
    "                self.conditionals[:, j] = self.p0[:, j] / col_sum\n",
    "            else:\n",
    "                self.conditionals[:, j] = 1.0 / self.n\n",
    "\n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Constructs the observation vector from z_t, p_0, and theta.\n",
    "        \"\"\"\n",
    "        z_flat = self.z_vector.astype(np.float32)\n",
    "        theta_one_hot = np.zeros(self.n + 1, dtype=np.float32)\n",
    "        theta_one_hot[self.theta] = 1.0\n",
    "\n",
    "        # Concatenate all parts into a single flat vector\n",
    "        obs = np.concatenate([z_flat, self.p0_flat, theta_one_hot])\n",
    "        return obs\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.z_vector = np.zeros(self.n, dtype=np.int32)\n",
    "        self.theta = 0\n",
    "        self.current_step = 0\n",
    "\n",
    "        p_flat = self.p0.flatten()\n",
    "        # Handle case where prior is all zeros by adding a small epsilon or checking sum\n",
    "        prior_sum = np.sum(p_flat)\n",
    "        if prior_sum == 0:\n",
    "             true_idx = self.np_random.choice(self.n * self.n) # Choose randomly if prior is zero\n",
    "        else:\n",
    "             true_idx = self.np_random.choice(self.n * self.n, p=p_flat / prior_sum) # Normalize\n",
    "\n",
    "        self.true_pos_o1, self.true_pos_o2 = np.unravel_index(true_idx, (self.n, self.n))\n",
    "\n",
    "        info = {}\n",
    "        return self._get_obs(), info\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        if not self.action_space.contains(action):\n",
    "            raise ValueError(f\"Invalid action: {action}\")\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        reward = -float(self.cost_vector[action])\n",
    "        terminated = False\n",
    "        found_o1 = False\n",
    "        found_o2_this_step = False\n",
    "\n",
    "        # --- Check for O1 (Target) Detection ---\n",
    "        # O1 is present if its true location is the action cell AND\n",
    "        # (O2 is hidden AND O1 is at action cell) OR (O2 is found at its true location AND O1 is at action cell)\n",
    "        is_o1_present = (action == self.true_pos_o1)\n",
    "\n",
    "        if is_o1_present:\n",
    "            if self.np_random.random() > self.gamma1[action]:\n",
    "                found_o1 = True\n",
    "\n",
    "        if found_o1:\n",
    "            reward = 1.0\n",
    "            terminated = True\n",
    "        else:\n",
    "            # --- Check for O2 (Related) Detection ---\n",
    "            if self.theta == 0: # Only check for O2 if it's currently hidden\n",
    "                if action == self.true_pos_o2:\n",
    "                    if self.np_random.random() > self.gamma2[action]:\n",
    "                        self.theta = action + 1 # O2 found, update theta\n",
    "\n",
    "            self.z_vector[action] += 1 # Increment search count for the actioned cell\n",
    "\n",
    "\n",
    "        if self.current_step >= self.T:\n",
    "            terminated = True\n",
    "\n",
    "        info = {'found_o1': found_o1, 'found_o2': found_o2_this_step}\n",
    "        truncated = False # We use terminated for end of horizon\n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DQN TRAINING FUNCTION FOR A SINGLE RUN\n",
    "# =============================================================================\n",
    "\n",
    "def run_single_experiment(run_id: int, base_log_dir: str, seed: int, total_timesteps: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Runs a single DQN training and evaluation experiment.\n",
    "    Returns a dictionary of final evaluation metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n===== STARTING DQN RUN {run_id} with SEED {seed} ====\")\n",
    "\n",
    "    # --- 3.1. Problem and Environment Definition ---\n",
    "    NUM_CELLS = 25\n",
    "    TIME_HORIZON = 20\n",
    "\n",
    "    set_random_seed(seed)\n",
    "\n",
    "    \n",
    "\n",
    "    SEED = 42\n",
    "\n",
    "    prior, gammas1, gammas2, costs = generate_problem_parameters(NUM_CELLS, SEED)\n",
    "    \n",
    "\n",
    "    assert (prior[0][0], prior[24][24]) == (0.020334334191265974, 0.020140323245187997)\n",
    "    assert (gammas1[0], gammas1[1]) == (0.6936350297118405, 0.837678576602479)\n",
    "    assert (gammas2[0], gammas2[1]) == (0.22851759613930137, 0.16996737821583596)\n",
    "    assert (costs[0], costs[1]) ==( 0.2939169255529117, 0.2550265646722229)\n",
    "\n",
    "    # --- 3.2. Create Environments with unique seed for this run ---\n",
    "    env = SearchEnvVec(p0=prior, gamma1=gammas1, gamma2=gammas2, T=TIME_HORIZON, cost_vector=costs)\n",
    "    eval_env = SearchEnvVec(p0=prior, gamma1=gammas1, gamma2=gammas2, T=TIME_HORIZON, cost_vector=costs)\n",
    "\n",
    "    # --- 3.3. Setup Logging and Callbacks for this specific run ---\n",
    "    run_log_dir = os.path.join(base_log_dir, f\"run_{run_id}\")\n",
    "    best_model_dir = os.path.join(run_log_dir, \"best_model\")\n",
    "    os.makedirs(run_log_dir, exist_ok=True)\n",
    "    os.makedirs(best_model_dir, exist_ok=True)\n",
    "\n",
    "    new_logger = configure(run_log_dir, [\"csv\", \"tensorboard\"])\n",
    "\n",
    "    # Set eval_freq lower to get more frequent evaluation points\n",
    "    eval_callback = EvalCallback(eval_env,\n",
    "                                 best_model_save_path=best_model_dir,\n",
    "                                 log_path=run_log_dir, # Save eval results in run dir\n",
    "                                 eval_freq=10000, # Evaluate more frequently\n",
    "                                 n_eval_episodes=10, # Evaluate on 100 episodes during training\n",
    "                                 deterministic=True,\n",
    "                                 render=False)\n",
    "\n",
    "    # --- 3.4. DQN Model Training ---\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=0,\n",
    "        buffer_size=50000,\n",
    "        learning_starts=1000,\n",
    "        batch_size=64,\n",
    "        gamma=0.95, #\n",
    "        train_freq=(10, \"step\"),\n",
    "        gradient_steps=10,\n",
    "        target_update_interval=1000,\n",
    "        exploration_fraction=0.2,\n",
    "        exploration_final_eps=0.01,\n",
    "        seed=seed,\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "    model.set_logger(new_logger)\n",
    "\n",
    "    model.learn(total_timesteps=total_timesteps, callback=eval_callback)\n",
    "\n",
    "    # --- 3.5. Final Evaluation of the Best Model ---\n",
    "    print(f\"Evaluating best model for run {run_id}...\")\n",
    "    best_model_path = os.path.join(best_model_dir, \"best_model.zip\")\n",
    "    final_metrics = {}\n",
    "\n",
    "    if os.path.exists(best_model_path):\n",
    "        best_model = DQN.load(best_model_path, env=eval_env)\n",
    "        num_final_eval_episodes = 100\n",
    "        total_reward = 0\n",
    "        num_successes = 0\n",
    "        detection_times = []\n",
    "        episode_lengths = []\n",
    "\n",
    "        # Each episode will now have a unique seed based on the run_id and episode index\n",
    "        for i_episode in range(num_final_eval_episodes):\n",
    "            episode_seed = seed * num_final_eval_episodes + i_episode # Unique seed for each episode\n",
    "            obs, info = eval_env.reset(seed=episode_seed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            episode_len = 0\n",
    "\n",
    "            while not done:\n",
    "                action, _ = best_model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "                done = terminated or truncated\n",
    "                episode_reward += reward\n",
    "                episode_len += 1\n",
    "\n",
    "                if info.get('found_o1'):\n",
    "                    detection_times.append(episode_len)\n",
    "\n",
    "            total_reward += episode_reward\n",
    "            episode_lengths.append(episode_len)\n",
    "            if info.get('found_o1'):\n",
    "                 num_successes += 1\n",
    "\n",
    "        mean_reward = total_reward / num_final_eval_episodes\n",
    "        success_rate = num_successes / num_final_eval_episodes\n",
    "        mean_detection_time = np.mean(detection_times) if detection_times else -1\n",
    "        mean_episode_length = np.mean(episode_lengths)\n",
    "\n",
    "        final_metrics = {\n",
    "            'success_rate': success_rate,\n",
    "            'avg_detection_time': mean_detection_time,\n",
    "            'avg_episode_reward': mean_reward,\n",
    "            'avg_episode_length': mean_episode_length\n",
    "        }\n",
    "\n",
    "\n",
    "        print(f\"Run {run_id} Final Evaluation Results (Best Model):\")\n",
    "        print(f\"  Success Rate: {final_metrics['success_rate']:.4f} ({final_metrics['success_rate']*100:.2f}%)\")\n",
    "        print(f\"  Average Detection Time (Successful Episodes): {final_metrics['avg_detection_time']:.2f}\")\n",
    "        print(f\"  Average Episode Reward: {final_metrics['avg_episode_reward']:.4f}\")\n",
    "        print(f\"  Average Episode Length: {final_metrics['avg_episode_length']:.2f}\")\n",
    "    else:\n",
    "        print(f\"No best model found for run {run_id}.\")\n",
    "\n",
    "\n",
    "    print(f\"===== COMPLETED DQN RUN {run_id} ====\")\n",
    "    return final_metrics\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration for Multiple Runs ---\n",
    "    NUM_RUNS = 10 # Set to 10 seeds as requested\n",
    "    TOTAL_TIMESTEPS = 200_000\n",
    "    BASE_LOG_DIR = \"./dqn_z_vector_logs/\"\n",
    "\n",
    "    # --- Execute All Runs ---\n",
    "    all_final_success_rates = []\n",
    "    all_final_detection_times = []\n",
    "    all_final_rewards = []\n",
    "    all_final_episode_lengths = []\n",
    "\n",
    "    total_experiment_start_time = time.time()\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        run_seed = i # Seeds go from 0 to 99\n",
    "        final_metrics = run_single_experiment(\n",
    "            run_id=i,\n",
    "            base_log_dir=BASE_LOG_DIR,\n",
    "            seed=run_seed,\n",
    "            total_timesteps=TOTAL_TIMESTEPS\n",
    "        )\n",
    "\n",
    "        # Collect metrics from each run\n",
    "        if final_metrics: # Only collect if evaluation was successful\n",
    "            all_final_success_rates.append(final_metrics['success_rate'])\n",
    "            if final_metrics['avg_detection_time'] != -1:\n",
    "                 all_final_detection_times.append(final_metrics['avg_detection_time'])\n",
    "            all_final_rewards.append(final_metrics['avg_episode_reward'])\n",
    "            all_final_episode_lengths.append(final_metrics['avg_episode_length'])\n",
    "\n",
    "\n",
    "    total_experiment_end_time = time.time()\n",
    "\n",
    "    # --- Calculate and Display Overall Metrics ---\n",
    "    print(f\"\\nAll {NUM_RUNS} DQN training runs are complete.\")\n",
    "    print(f\"Log files are saved in '{BASE_LOG_DIR}'.\")\n",
    "    print(f\"Total time for all runs: {total_experiment_end_time - total_experiment_start_time:.2f} seconds\")\n",
    "\n",
    "    print(\"\\n--- Overall DQN Policy Results Across All Seeds ---\")\n",
    "    print(f\"Average Success Rate: {np.mean(all_final_success_rates):.4f} ({np.mean(all_final_success_rates)*100:.2f}%)\")\n",
    "    print(f\"Standard Deviation of Success Rate: {np.std(all_final_success_rates):.4f}\")\n",
    "\n",
    "    if all_final_detection_times:\n",
    "        print(f\"Average Detection Time (Successful Episodes): {np.mean(all_final_detection_times):.2f}\")\n",
    "        print(f\"Standard Deviation of Detection Time: {np.std(all_final_detection_times):.2f}\")\n",
    "    else:\n",
    "        print(\"Average Detection Time (Successful Episodes): N/A (no successful episodes across all runs)\")\n",
    "\n",
    "    print(f\"Average Episode Reward: {np.mean(all_final_rewards):.4f}\")\n",
    "    print(f\"Standard Deviation of Episode Reward: {np.std(all_final_rewards):.4f}\")\n",
    "\n",
    "    print(f\"Average Episode Length: {np.mean(all_final_episode_lengths):.2f}\")\n",
    "    print(f\"Standard Deviation of Episode Length: {np.std(all_final_episode_lengths):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e3ae6-81f2-4f3f-ae4c-bc40a0d8c1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
