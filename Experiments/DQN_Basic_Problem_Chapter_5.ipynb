{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abadebe7-219a-42c4-b181-07c7c9a7ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414cb2b8-2664-4f2e-9224-2f1c2bc737b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 06:09:53.880714: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-07 06:09:53.971525: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-07 06:09:55.597969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== STARTING DQN RUN 0 with SEED 0 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jey/anaconda3/envs/dataanot/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=-0.62 +/- 1.02\n",
      "Episode length: 7.45 +/- 3.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.30 +/- 1.05\n",
      "Episode length: 6.45 +/- 3.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.46 +/- 1.02\n",
      "Episode length: 7.16 +/- 2.91\n",
      "Eval num_timesteps=40000, episode_reward=-0.51 +/- 0.97\n",
      "Episode length: 7.63 +/- 2.46\n",
      "Eval num_timesteps=50000, episode_reward=-0.55 +/- 1.04\n",
      "Episode length: 7.20 +/- 3.19\n",
      "Eval num_timesteps=60000, episode_reward=-0.66 +/- 0.98\n",
      "Episode length: 7.98 +/- 2.77\n",
      "Eval num_timesteps=70000, episode_reward=-0.37 +/- 0.99\n",
      "Episode length: 6.98 +/- 3.20\n",
      "Eval num_timesteps=80000, episode_reward=-0.53 +/- 0.97\n",
      "Episode length: 7.43 +/- 3.14\n",
      "Eval num_timesteps=90000, episode_reward=-0.38 +/- 1.04\n",
      "Episode length: 6.71 +/- 3.31\n",
      "Eval num_timesteps=100000, episode_reward=-0.69 +/- 0.96\n",
      "Episode length: 8.00 +/- 2.84\n",
      "Eval num_timesteps=110000, episode_reward=-0.41 +/- 0.91\n",
      "Episode length: 7.35 +/- 3.07\n",
      "Eval num_timesteps=120000, episode_reward=-0.47 +/- 0.92\n",
      "Episode length: 7.58 +/- 3.08\n",
      "Eval num_timesteps=130000, episode_reward=-0.36 +/- 1.00\n",
      "Episode length: 6.89 +/- 3.09\n",
      "Eval num_timesteps=140000, episode_reward=-0.56 +/- 0.98\n",
      "Episode length: 7.55 +/- 2.82\n",
      "Eval num_timesteps=150000, episode_reward=-0.27 +/- 0.94\n",
      "Episode length: 6.92 +/- 2.77\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-0.43 +/- 0.94\n",
      "Episode length: 7.32 +/- 3.17\n",
      "Eval num_timesteps=170000, episode_reward=-0.52 +/- 0.96\n",
      "Episode length: 7.55 +/- 3.23\n",
      "Eval num_timesteps=180000, episode_reward=-0.51 +/- 0.98\n",
      "Episode length: 7.48 +/- 2.90\n",
      "Eval num_timesteps=190000, episode_reward=-0.45 +/- 0.98\n",
      "Episode length: 7.26 +/- 3.09\n",
      "Eval num_timesteps=200000, episode_reward=-0.40 +/- 0.94\n",
      "Episode length: 7.35 +/- 2.79\n",
      "Evaluating best model for run 0...\n",
      "Run 0 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5600 (56.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.20\n",
      "  Average Episode Reward: -0.4745\n",
      "  Average Episode Length: 7.31\n",
      "===== COMPLETED DQN RUN 0 ====\n",
      "\n",
      "===== STARTING DQN RUN 1 with SEED 1 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.54 +/- 1.02\n",
      "Episode length: 7.18 +/- 3.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.54 +/- 0.98\n",
      "Episode length: 7.36 +/- 2.96\n",
      "Eval num_timesteps=30000, episode_reward=-0.43 +/- 1.07\n",
      "Episode length: 6.71 +/- 3.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.48 +/- 0.94\n",
      "Episode length: 7.29 +/- 3.53\n",
      "Eval num_timesteps=50000, episode_reward=-0.31 +/- 0.97\n",
      "Episode length: 6.57 +/- 3.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.66 +/- 1.07\n",
      "Episode length: 7.44 +/- 3.13\n",
      "Eval num_timesteps=70000, episode_reward=-0.39 +/- 1.03\n",
      "Episode length: 6.97 +/- 3.15\n",
      "Eval num_timesteps=80000, episode_reward=-0.37 +/- 1.01\n",
      "Episode length: 6.91 +/- 3.11\n",
      "Eval num_timesteps=90000, episode_reward=-0.42 +/- 1.02\n",
      "Episode length: 7.06 +/- 3.21\n",
      "Eval num_timesteps=100000, episode_reward=-0.24 +/- 1.02\n",
      "Episode length: 6.33 +/- 3.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-0.47 +/- 0.93\n",
      "Episode length: 7.41 +/- 2.99\n",
      "Eval num_timesteps=120000, episode_reward=-0.23 +/- 1.00\n",
      "Episode length: 6.53 +/- 3.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.31 +/- 0.94\n",
      "Episode length: 6.93 +/- 3.27\n",
      "Eval num_timesteps=140000, episode_reward=-0.29 +/- 1.00\n",
      "Episode length: 6.67 +/- 3.13\n",
      "Eval num_timesteps=150000, episode_reward=-0.34 +/- 1.01\n",
      "Episode length: 6.85 +/- 3.19\n",
      "Eval num_timesteps=160000, episode_reward=-0.44 +/- 1.04\n",
      "Episode length: 7.07 +/- 3.03\n",
      "Eval num_timesteps=170000, episode_reward=-0.36 +/- 1.00\n",
      "Episode length: 6.99 +/- 2.99\n",
      "Eval num_timesteps=180000, episode_reward=-0.42 +/- 0.90\n",
      "Episode length: 7.56 +/- 2.94\n",
      "Eval num_timesteps=190000, episode_reward=-0.26 +/- 0.99\n",
      "Episode length: 6.53 +/- 2.99\n",
      "Eval num_timesteps=200000, episode_reward=-0.26 +/- 1.00\n",
      "Episode length: 6.49 +/- 3.19\n",
      "Evaluating best model for run 1...\n",
      "Run 1 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5300 (53.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.64\n",
      "  Average Episode Reward: -0.4480\n",
      "  Average Episode Length: 7.16\n",
      "===== COMPLETED DQN RUN 1 ====\n",
      "\n",
      "===== STARTING DQN RUN 2 with SEED 2 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.65 +/- 0.87\n",
      "Episode length: 8.41 +/- 2.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.52 +/- 0.92\n",
      "Episode length: 8.02 +/- 2.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.43 +/- 1.06\n",
      "Episode length: 6.77 +/- 3.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.36 +/- 0.99\n",
      "Episode length: 7.20 +/- 2.67\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.51 +/- 0.97\n",
      "Episode length: 7.53 +/- 2.81\n",
      "Eval num_timesteps=60000, episode_reward=-0.45 +/- 1.06\n",
      "Episode length: 6.99 +/- 3.04\n",
      "Eval num_timesteps=70000, episode_reward=-0.63 +/- 1.10\n",
      "Episode length: 7.23 +/- 3.20\n",
      "Eval num_timesteps=80000, episode_reward=-0.39 +/- 1.04\n",
      "Episode length: 6.86 +/- 3.22\n",
      "Eval num_timesteps=90000, episode_reward=-0.33 +/- 1.00\n",
      "Episode length: 6.98 +/- 2.79\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.61 +/- 1.00\n",
      "Episode length: 7.68 +/- 2.99\n",
      "Eval num_timesteps=110000, episode_reward=-0.46 +/- 1.09\n",
      "Episode length: 7.02 +/- 3.28\n",
      "Eval num_timesteps=120000, episode_reward=-0.41 +/- 1.02\n",
      "Episode length: 7.01 +/- 3.20\n",
      "Eval num_timesteps=130000, episode_reward=-0.50 +/- 1.01\n",
      "Episode length: 7.24 +/- 3.05\n",
      "Eval num_timesteps=140000, episode_reward=-0.58 +/- 1.07\n",
      "Episode length: 7.18 +/- 3.03\n",
      "Eval num_timesteps=150000, episode_reward=-0.46 +/- 1.03\n",
      "Episode length: 7.06 +/- 3.12\n",
      "Eval num_timesteps=160000, episode_reward=-0.46 +/- 0.99\n",
      "Episode length: 7.38 +/- 2.83\n",
      "Eval num_timesteps=170000, episode_reward=-0.39 +/- 1.03\n",
      "Episode length: 6.86 +/- 3.20\n",
      "Eval num_timesteps=180000, episode_reward=-0.39 +/- 0.97\n",
      "Episode length: 7.19 +/- 2.77\n",
      "Eval num_timesteps=190000, episode_reward=-0.36 +/- 0.98\n",
      "Episode length: 7.05 +/- 2.95\n",
      "Eval num_timesteps=200000, episode_reward=-0.65 +/- 1.02\n",
      "Episode length: 7.68 +/- 2.92\n",
      "Evaluating best model for run 2...\n",
      "Run 2 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5300 (53.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.55\n",
      "  Average Episode Reward: -0.4895\n",
      "  Average Episode Length: 7.11\n",
      "===== COMPLETED DQN RUN 2 ====\n",
      "\n",
      "===== STARTING DQN RUN 3 with SEED 3 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.22 +/- 1.04\n",
      "Episode length: 6.31 +/- 3.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.49 +/- 1.06\n",
      "Episode length: 6.95 +/- 3.33\n",
      "Eval num_timesteps=30000, episode_reward=-0.38 +/- 1.03\n",
      "Episode length: 7.09 +/- 2.93\n",
      "Eval num_timesteps=40000, episode_reward=-0.28 +/- 1.04\n",
      "Episode length: 6.63 +/- 3.26\n",
      "Eval num_timesteps=50000, episode_reward=-0.46 +/- 0.94\n",
      "Episode length: 7.47 +/- 3.02\n",
      "Eval num_timesteps=60000, episode_reward=-0.40 +/- 1.02\n",
      "Episode length: 6.94 +/- 2.89\n",
      "Eval num_timesteps=70000, episode_reward=-0.30 +/- 1.03\n",
      "Episode length: 6.74 +/- 3.21\n",
      "Eval num_timesteps=80000, episode_reward=-0.35 +/- 1.01\n",
      "Episode length: 6.96 +/- 3.25\n",
      "Eval num_timesteps=90000, episode_reward=-0.40 +/- 1.00\n",
      "Episode length: 6.93 +/- 3.12\n",
      "Eval num_timesteps=100000, episode_reward=-0.42 +/- 0.93\n",
      "Episode length: 7.37 +/- 3.09\n",
      "Eval num_timesteps=110000, episode_reward=-0.37 +/- 0.99\n",
      "Episode length: 7.01 +/- 3.23\n",
      "Eval num_timesteps=120000, episode_reward=-0.44 +/- 1.01\n",
      "Episode length: 7.13 +/- 3.12\n",
      "Eval num_timesteps=130000, episode_reward=-0.57 +/- 1.01\n",
      "Episode length: 7.50 +/- 2.88\n",
      "Eval num_timesteps=140000, episode_reward=-0.39 +/- 1.00\n",
      "Episode length: 6.83 +/- 3.20\n",
      "Eval num_timesteps=150000, episode_reward=-0.31 +/- 0.96\n",
      "Episode length: 6.96 +/- 3.21\n",
      "Eval num_timesteps=160000, episode_reward=-0.22 +/- 0.92\n",
      "Episode length: 6.83 +/- 3.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=-0.37 +/- 0.96\n",
      "Episode length: 7.15 +/- 3.11\n",
      "Eval num_timesteps=180000, episode_reward=-0.41 +/- 0.96\n",
      "Episode length: 7.22 +/- 3.14\n",
      "Eval num_timesteps=190000, episode_reward=-0.43 +/- 0.95\n",
      "Episode length: 7.42 +/- 2.95\n",
      "Eval num_timesteps=200000, episode_reward=-0.54 +/- 1.03\n",
      "Episode length: 7.33 +/- 3.11\n",
      "Evaluating best model for run 3...\n",
      "Run 3 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5600 (56.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.71\n",
      "  Average Episode Reward: -0.3250\n",
      "  Average Episode Length: 7.04\n",
      "===== COMPLETED DQN RUN 3 ====\n",
      "\n",
      "===== STARTING DQN RUN 4 with SEED 4 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.43 +/- 1.09\n",
      "Episode length: 6.67 +/- 3.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.46 +/- 1.04\n",
      "Episode length: 7.17 +/- 3.09\n",
      "Eval num_timesteps=30000, episode_reward=-0.41 +/- 1.00\n",
      "Episode length: 7.15 +/- 2.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.28 +/- 1.03\n",
      "Episode length: 6.43 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.39 +/- 1.02\n",
      "Episode length: 6.78 +/- 3.38\n",
      "Eval num_timesteps=60000, episode_reward=-0.57 +/- 0.97\n",
      "Episode length: 7.38 +/- 2.96\n",
      "Eval num_timesteps=70000, episode_reward=-0.56 +/- 0.93\n",
      "Episode length: 7.34 +/- 3.08\n",
      "Eval num_timesteps=80000, episode_reward=-0.68 +/- 0.95\n",
      "Episode length: 8.09 +/- 2.47\n",
      "Eval num_timesteps=90000, episode_reward=-0.33 +/- 1.11\n",
      "Episode length: 6.24 +/- 3.20\n",
      "Eval num_timesteps=100000, episode_reward=-0.45 +/- 1.03\n",
      "Episode length: 6.94 +/- 3.12\n",
      "Eval num_timesteps=110000, episode_reward=-0.56 +/- 0.99\n",
      "Episode length: 7.43 +/- 2.99\n",
      "Eval num_timesteps=120000, episode_reward=-0.42 +/- 1.00\n",
      "Episode length: 7.21 +/- 2.88\n",
      "Eval num_timesteps=130000, episode_reward=-0.61 +/- 0.95\n",
      "Episode length: 7.83 +/- 2.68\n",
      "Eval num_timesteps=140000, episode_reward=-0.36 +/- 0.94\n",
      "Episode length: 7.23 +/- 2.88\n",
      "Eval num_timesteps=150000, episode_reward=-0.44 +/- 0.91\n",
      "Episode length: 7.61 +/- 2.64\n",
      "Eval num_timesteps=160000, episode_reward=-0.28 +/- 1.00\n",
      "Episode length: 6.79 +/- 2.92\n",
      "Eval num_timesteps=170000, episode_reward=-0.39 +/- 0.90\n",
      "Episode length: 7.47 +/- 2.81\n",
      "Eval num_timesteps=180000, episode_reward=-0.44 +/- 0.96\n",
      "Episode length: 7.35 +/- 3.07\n",
      "Eval num_timesteps=190000, episode_reward=-0.29 +/- 0.94\n",
      "Episode length: 7.15 +/- 2.99\n",
      "Eval num_timesteps=200000, episode_reward=-0.21 +/- 0.93\n",
      "Episode length: 6.80 +/- 2.96\n",
      "New best mean reward!\n",
      "Evaluating best model for run 4...\n",
      "Run 4 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5500 (55.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.60\n",
      "  Average Episode Reward: -0.3075\n",
      "  Average Episode Length: 7.03\n",
      "===== COMPLETED DQN RUN 4 ====\n",
      "\n",
      "===== STARTING DQN RUN 5 with SEED 5 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.57 +/- 0.92\n",
      "Episode length: 8.00 +/- 2.64\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.44 +/- 1.09\n",
      "Episode length: 6.62 +/- 3.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.43 +/- 1.04\n",
      "Episode length: 6.75 +/- 3.41\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.46 +/- 1.04\n",
      "Episode length: 6.89 +/- 3.54\n",
      "Eval num_timesteps=50000, episode_reward=-0.61 +/- 1.05\n",
      "Episode length: 7.21 +/- 3.09\n",
      "Eval num_timesteps=60000, episode_reward=-0.49 +/- 1.08\n",
      "Episode length: 6.77 +/- 3.45\n",
      "Eval num_timesteps=70000, episode_reward=-0.46 +/- 0.97\n",
      "Episode length: 7.33 +/- 3.02\n",
      "Eval num_timesteps=80000, episode_reward=-0.42 +/- 1.01\n",
      "Episode length: 6.98 +/- 3.04\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-0.38 +/- 0.97\n",
      "Episode length: 7.12 +/- 3.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.56 +/- 0.93\n",
      "Episode length: 7.70 +/- 3.13\n",
      "Eval num_timesteps=110000, episode_reward=-0.36 +/- 0.97\n",
      "Episode length: 6.91 +/- 3.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.31 +/- 1.03\n",
      "Episode length: 6.70 +/- 3.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.49 +/- 0.98\n",
      "Episode length: 7.34 +/- 2.90\n",
      "Eval num_timesteps=140000, episode_reward=-0.54 +/- 1.00\n",
      "Episode length: 7.40 +/- 3.01\n",
      "Eval num_timesteps=150000, episode_reward=-0.45 +/- 0.99\n",
      "Episode length: 7.23 +/- 2.97\n",
      "Eval num_timesteps=160000, episode_reward=-0.43 +/- 0.95\n",
      "Episode length: 7.18 +/- 3.25\n",
      "Eval num_timesteps=170000, episode_reward=-0.39 +/- 1.01\n",
      "Episode length: 6.98 +/- 3.16\n",
      "Eval num_timesteps=180000, episode_reward=-0.25 +/- 1.01\n",
      "Episode length: 6.54 +/- 3.18\n",
      "New best mean reward!\n",
      "Eval num_timesteps=190000, episode_reward=-0.33 +/- 1.02\n",
      "Episode length: 6.78 +/- 3.23\n",
      "Eval num_timesteps=200000, episode_reward=-0.27 +/- 0.97\n",
      "Episode length: 6.66 +/- 3.38\n",
      "Evaluating best model for run 5...\n",
      "Run 5 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5600 (56.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.96\n",
      "  Average Episode Reward: -0.4730\n",
      "  Average Episode Length: 7.18\n",
      "===== COMPLETED DQN RUN 5 ====\n",
      "\n",
      "===== STARTING DQN RUN 6 with SEED 6 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.85 +/- 1.11\n",
      "Episode length: 7.82 +/- 2.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.44 +/- 1.03\n",
      "Episode length: 7.10 +/- 3.09\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.47 +/- 1.07\n",
      "Episode length: 7.05 +/- 3.28\n",
      "Eval num_timesteps=40000, episode_reward=-0.31 +/- 1.00\n",
      "Episode length: 6.77 +/- 2.88\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.34 +/- 1.03\n",
      "Episode length: 6.95 +/- 2.92\n",
      "Eval num_timesteps=60000, episode_reward=-0.44 +/- 1.04\n",
      "Episode length: 7.11 +/- 2.88\n",
      "Eval num_timesteps=70000, episode_reward=-0.25 +/- 1.00\n",
      "Episode length: 6.71 +/- 2.89\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.51 +/- 1.01\n",
      "Episode length: 7.49 +/- 2.85\n",
      "Eval num_timesteps=90000, episode_reward=-0.45 +/- 0.96\n",
      "Episode length: 7.37 +/- 3.10\n",
      "Eval num_timesteps=100000, episode_reward=-0.58 +/- 1.03\n",
      "Episode length: 7.42 +/- 3.34\n",
      "Eval num_timesteps=110000, episode_reward=-0.31 +/- 0.98\n",
      "Episode length: 6.95 +/- 2.96\n",
      "Eval num_timesteps=120000, episode_reward=-0.49 +/- 1.07\n",
      "Episode length: 7.22 +/- 2.96\n",
      "Eval num_timesteps=130000, episode_reward=-0.19 +/- 1.00\n",
      "Episode length: 6.52 +/- 2.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-0.49 +/- 0.98\n",
      "Episode length: 7.43 +/- 3.01\n",
      "Eval num_timesteps=150000, episode_reward=-0.30 +/- 0.99\n",
      "Episode length: 6.88 +/- 3.11\n",
      "Eval num_timesteps=160000, episode_reward=-0.41 +/- 1.10\n",
      "Episode length: 6.88 +/- 3.28\n",
      "Eval num_timesteps=170000, episode_reward=-0.41 +/- 1.04\n",
      "Episode length: 7.01 +/- 3.10\n",
      "Eval num_timesteps=180000, episode_reward=-0.42 +/- 0.94\n",
      "Episode length: 7.42 +/- 3.05\n",
      "Eval num_timesteps=190000, episode_reward=-0.31 +/- 1.05\n",
      "Episode length: 6.61 +/- 3.25\n",
      "Eval num_timesteps=200000, episode_reward=-0.42 +/- 1.02\n",
      "Episode length: 7.13 +/- 3.11\n",
      "Evaluating best model for run 6...\n",
      "Run 6 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6100 (61.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.28\n",
      "  Average Episode Reward: -0.4095\n",
      "  Average Episode Length: 7.12\n",
      "===== COMPLETED DQN RUN 6 ====\n",
      "\n",
      "===== STARTING DQN RUN 7 with SEED 7 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.59 +/- 1.04\n",
      "Episode length: 7.55 +/- 2.88\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.49 +/- 1.06\n",
      "Episode length: 7.14 +/- 3.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.54 +/- 1.13\n",
      "Episode length: 6.87 +/- 3.28\n",
      "Eval num_timesteps=40000, episode_reward=-0.34 +/- 0.99\n",
      "Episode length: 6.88 +/- 2.94\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.42 +/- 1.04\n",
      "Episode length: 7.17 +/- 2.74\n",
      "Eval num_timesteps=60000, episode_reward=-0.51 +/- 1.05\n",
      "Episode length: 7.40 +/- 2.77\n",
      "Eval num_timesteps=70000, episode_reward=-0.51 +/- 1.05\n",
      "Episode length: 7.10 +/- 3.04\n",
      "Eval num_timesteps=80000, episode_reward=-0.47 +/- 1.00\n",
      "Episode length: 7.38 +/- 2.80\n",
      "Eval num_timesteps=90000, episode_reward=-0.39 +/- 1.09\n",
      "Episode length: 6.73 +/- 3.13\n",
      "Eval num_timesteps=100000, episode_reward=-0.62 +/- 0.99\n",
      "Episode length: 7.70 +/- 2.79\n",
      "Eval num_timesteps=110000, episode_reward=-0.43 +/- 1.01\n",
      "Episode length: 7.00 +/- 3.31\n",
      "Eval num_timesteps=120000, episode_reward=-0.55 +/- 0.94\n",
      "Episode length: 7.59 +/- 3.05\n",
      "Eval num_timesteps=130000, episode_reward=-0.16 +/- 0.97\n",
      "Episode length: 6.34 +/- 3.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-0.47 +/- 0.98\n",
      "Episode length: 7.43 +/- 3.11\n",
      "Eval num_timesteps=150000, episode_reward=-0.31 +/- 1.00\n",
      "Episode length: 6.74 +/- 3.32\n",
      "Eval num_timesteps=160000, episode_reward=-0.30 +/- 0.98\n",
      "Episode length: 6.76 +/- 3.09\n",
      "Eval num_timesteps=170000, episode_reward=-0.41 +/- 0.99\n",
      "Episode length: 7.02 +/- 3.09\n",
      "Eval num_timesteps=180000, episode_reward=-0.57 +/- 0.96\n",
      "Episode length: 7.52 +/- 2.91\n",
      "Eval num_timesteps=190000, episode_reward=-0.50 +/- 0.99\n",
      "Episode length: 7.36 +/- 3.02\n",
      "Eval num_timesteps=200000, episode_reward=-0.41 +/- 1.01\n",
      "Episode length: 7.00 +/- 3.14\n",
      "Evaluating best model for run 7...\n",
      "Run 7 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6900 (69.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.51\n",
      "  Average Episode Reward: -0.1370\n",
      "  Average Episode Length: 6.21\n",
      "===== COMPLETED DQN RUN 7 ====\n",
      "\n",
      "===== STARTING DQN RUN 8 with SEED 8 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.47 +/- 0.91\n",
      "Episode length: 7.51 +/- 2.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.54 +/- 0.94\n",
      "Episode length: 7.68 +/- 3.09\n",
      "Eval num_timesteps=30000, episode_reward=-0.42 +/- 1.15\n",
      "Episode length: 6.39 +/- 3.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.39 +/- 0.98\n",
      "Episode length: 7.13 +/- 3.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.52 +/- 1.02\n",
      "Episode length: 7.40 +/- 3.22\n",
      "Eval num_timesteps=60000, episode_reward=-0.28 +/- 0.90\n",
      "Episode length: 7.15 +/- 3.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.35 +/- 0.97\n",
      "Episode length: 7.11 +/- 2.94\n",
      "Eval num_timesteps=80000, episode_reward=-0.67 +/- 0.95\n",
      "Episode length: 8.02 +/- 2.89\n",
      "Eval num_timesteps=90000, episode_reward=-0.36 +/- 0.97\n",
      "Episode length: 7.05 +/- 3.30\n",
      "Eval num_timesteps=100000, episode_reward=-0.47 +/- 0.93\n",
      "Episode length: 7.54 +/- 2.89\n",
      "Eval num_timesteps=110000, episode_reward=-0.31 +/- 0.95\n",
      "Episode length: 7.09 +/- 2.86\n",
      "Eval num_timesteps=120000, episode_reward=-0.38 +/- 0.99\n",
      "Episode length: 7.01 +/- 3.37\n",
      "Eval num_timesteps=130000, episode_reward=-0.59 +/- 1.02\n",
      "Episode length: 7.54 +/- 3.01\n",
      "Eval num_timesteps=140000, episode_reward=-0.28 +/- 0.98\n",
      "Episode length: 6.83 +/- 3.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-0.56 +/- 1.04\n",
      "Episode length: 7.47 +/- 3.13\n",
      "Eval num_timesteps=160000, episode_reward=-0.54 +/- 0.90\n",
      "Episode length: 7.85 +/- 2.75\n",
      "Eval num_timesteps=170000, episode_reward=-0.65 +/- 1.00\n",
      "Episode length: 7.82 +/- 2.79\n",
      "Eval num_timesteps=180000, episode_reward=-0.31 +/- 1.02\n",
      "Episode length: 6.76 +/- 3.16\n",
      "Eval num_timesteps=190000, episode_reward=-0.42 +/- 1.05\n",
      "Episode length: 7.05 +/- 3.03\n",
      "Eval num_timesteps=200000, episode_reward=-0.45 +/- 1.06\n",
      "Episode length: 7.13 +/- 3.32\n",
      "Evaluating best model for run 8...\n",
      "Run 8 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5500 (55.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.67\n",
      "  Average Episode Reward: -0.5015\n",
      "  Average Episode Length: 7.62\n",
      "===== COMPLETED DQN RUN 8 ====\n",
      "\n",
      "===== STARTING DQN RUN 9 with SEED 9 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.54 +/- 1.01\n",
      "Episode length: 7.37 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.64 +/- 0.93\n",
      "Episode length: 7.91 +/- 2.77\n",
      "Eval num_timesteps=30000, episode_reward=-0.63 +/- 0.99\n",
      "Episode length: 7.68 +/- 3.09\n",
      "Eval num_timesteps=40000, episode_reward=-0.40 +/- 1.10\n",
      "Episode length: 6.72 +/- 3.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.28 +/- 1.03\n",
      "Episode length: 6.59 +/- 3.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.57 +/- 0.95\n",
      "Episode length: 7.64 +/- 2.92\n",
      "Eval num_timesteps=70000, episode_reward=-0.13 +/- 0.96\n",
      "Episode length: 6.17 +/- 3.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.58 +/- 0.95\n",
      "Episode length: 7.77 +/- 2.98\n",
      "Eval num_timesteps=90000, episode_reward=-0.51 +/- 1.14\n",
      "Episode length: 6.97 +/- 2.96\n",
      "Eval num_timesteps=100000, episode_reward=-0.64 +/- 1.01\n",
      "Episode length: 7.54 +/- 3.16\n",
      "Eval num_timesteps=110000, episode_reward=-0.33 +/- 0.97\n",
      "Episode length: 6.92 +/- 3.22\n",
      "Eval num_timesteps=120000, episode_reward=-0.56 +/- 0.94\n",
      "Episode length: 7.59 +/- 3.25\n",
      "Eval num_timesteps=130000, episode_reward=-0.43 +/- 1.03\n",
      "Episode length: 6.95 +/- 2.94\n",
      "Eval num_timesteps=140000, episode_reward=-0.61 +/- 1.05\n",
      "Episode length: 7.50 +/- 3.16\n",
      "Eval num_timesteps=150000, episode_reward=-0.46 +/- 1.04\n",
      "Episode length: 6.96 +/- 3.26\n",
      "Eval num_timesteps=160000, episode_reward=-0.45 +/- 1.03\n",
      "Episode length: 7.02 +/- 3.02\n",
      "Eval num_timesteps=170000, episode_reward=-0.52 +/- 0.98\n",
      "Episode length: 7.42 +/- 3.34\n",
      "Eval num_timesteps=180000, episode_reward=-0.30 +/- 1.00\n",
      "Episode length: 6.66 +/- 3.29\n",
      "Eval num_timesteps=190000, episode_reward=-0.50 +/- 1.02\n",
      "Episode length: 7.28 +/- 2.87\n",
      "Eval num_timesteps=200000, episode_reward=-0.44 +/- 1.06\n",
      "Episode length: 6.92 +/- 2.99\n",
      "Evaluating best model for run 9...\n",
      "Run 9 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6300 (63.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.70\n",
      "  Average Episode Reward: -0.3120\n",
      "  Average Episode Length: 6.66\n",
      "===== COMPLETED DQN RUN 9 ====\n",
      "\n",
      "===== STARTING DQN RUN 10 with SEED 10 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.45 +/- 1.04\n",
      "Episode length: 7.00 +/- 3.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.42 +/- 1.06\n",
      "Episode length: 7.01 +/- 3.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.44 +/- 1.09\n",
      "Episode length: 6.87 +/- 3.26\n",
      "Eval num_timesteps=40000, episode_reward=-0.41 +/- 1.05\n",
      "Episode length: 6.82 +/- 3.35\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.37 +/- 1.06\n",
      "Episode length: 6.54 +/- 3.41\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.42 +/- 1.03\n",
      "Episode length: 7.01 +/- 3.10\n",
      "Eval num_timesteps=70000, episode_reward=-0.49 +/- 0.99\n",
      "Episode length: 7.42 +/- 2.78\n",
      "Eval num_timesteps=80000, episode_reward=-0.35 +/- 1.03\n",
      "Episode length: 6.64 +/- 3.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-0.51 +/- 1.00\n",
      "Episode length: 7.16 +/- 3.30\n",
      "Eval num_timesteps=100000, episode_reward=-0.58 +/- 1.07\n",
      "Episode length: 6.99 +/- 3.30\n",
      "Eval num_timesteps=110000, episode_reward=-0.40 +/- 0.98\n",
      "Episode length: 7.25 +/- 3.11\n",
      "Eval num_timesteps=120000, episode_reward=-0.33 +/- 0.99\n",
      "Episode length: 6.75 +/- 3.46\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.48 +/- 0.89\n",
      "Episode length: 7.79 +/- 2.82\n",
      "Eval num_timesteps=140000, episode_reward=-0.35 +/- 1.01\n",
      "Episode length: 7.03 +/- 2.87\n",
      "Eval num_timesteps=150000, episode_reward=-0.40 +/- 0.98\n",
      "Episode length: 7.29 +/- 2.93\n",
      "Eval num_timesteps=160000, episode_reward=-0.46 +/- 0.95\n",
      "Episode length: 7.16 +/- 3.29\n",
      "Eval num_timesteps=170000, episode_reward=-0.58 +/- 0.93\n",
      "Episode length: 7.92 +/- 2.52\n",
      "Eval num_timesteps=180000, episode_reward=-0.54 +/- 1.12\n",
      "Episode length: 6.73 +/- 3.48\n",
      "Eval num_timesteps=190000, episode_reward=-0.38 +/- 1.01\n",
      "Episode length: 6.94 +/- 3.42\n",
      "Eval num_timesteps=200000, episode_reward=-0.45 +/- 0.99\n",
      "Episode length: 7.25 +/- 3.00\n",
      "Evaluating best model for run 10...\n",
      "Run 10 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6200 (62.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.42\n",
      "  Average Episode Reward: -0.3280\n",
      "  Average Episode Length: 6.54\n",
      "===== COMPLETED DQN RUN 10 ====\n",
      "\n",
      "===== STARTING DQN RUN 11 with SEED 11 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.55 +/- 0.97\n",
      "Episode length: 7.59 +/- 3.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.56 +/- 1.14\n",
      "Episode length: 6.81 +/- 3.47\n",
      "Eval num_timesteps=30000, episode_reward=-0.46 +/- 1.03\n",
      "Episode length: 7.14 +/- 3.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.66 +/- 1.04\n",
      "Episode length: 7.45 +/- 3.31\n",
      "Eval num_timesteps=50000, episode_reward=-0.47 +/- 0.99\n",
      "Episode length: 7.11 +/- 3.54\n",
      "Eval num_timesteps=60000, episode_reward=-0.44 +/- 1.02\n",
      "Episode length: 6.90 +/- 3.35\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.45 +/- 1.02\n",
      "Episode length: 7.11 +/- 3.22\n",
      "Eval num_timesteps=80000, episode_reward=-0.46 +/- 0.98\n",
      "Episode length: 7.23 +/- 3.10\n",
      "Eval num_timesteps=90000, episode_reward=-0.50 +/- 1.01\n",
      "Episode length: 7.35 +/- 3.04\n",
      "Eval num_timesteps=100000, episode_reward=-0.51 +/- 1.07\n",
      "Episode length: 7.12 +/- 3.18\n",
      "Eval num_timesteps=110000, episode_reward=-0.46 +/- 1.04\n",
      "Episode length: 7.07 +/- 2.99\n",
      "Eval num_timesteps=120000, episode_reward=-0.42 +/- 0.95\n",
      "Episode length: 7.42 +/- 2.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.32 +/- 0.97\n",
      "Episode length: 7.02 +/- 2.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-0.54 +/- 0.92\n",
      "Episode length: 7.86 +/- 2.62\n",
      "Eval num_timesteps=150000, episode_reward=-0.44 +/- 1.01\n",
      "Episode length: 7.17 +/- 2.86\n",
      "Eval num_timesteps=160000, episode_reward=-0.35 +/- 0.95\n",
      "Episode length: 6.84 +/- 2.98\n",
      "Eval num_timesteps=170000, episode_reward=-0.41 +/- 1.05\n",
      "Episode length: 6.66 +/- 3.33\n",
      "Eval num_timesteps=180000, episode_reward=-0.41 +/- 0.98\n",
      "Episode length: 7.27 +/- 3.07\n",
      "Eval num_timesteps=190000, episode_reward=-0.32 +/- 1.02\n",
      "Episode length: 6.76 +/- 3.20\n",
      "Eval num_timesteps=200000, episode_reward=-0.46 +/- 0.98\n",
      "Episode length: 7.41 +/- 2.94\n",
      "Evaluating best model for run 11...\n",
      "Run 11 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5400 (54.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.81\n",
      "  Average Episode Reward: -0.4950\n",
      "  Average Episode Length: 7.74\n",
      "===== COMPLETED DQN RUN 11 ====\n",
      "\n",
      "===== STARTING DQN RUN 12 with SEED 12 ====\n",
      "Eval num_timesteps=10000, episode_reward=-1.04 +/- 1.25\n",
      "Episode length: 7.53 +/- 3.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.43 +/- 1.01\n",
      "Episode length: 7.07 +/- 3.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.37 +/- 1.02\n",
      "Episode length: 7.00 +/- 3.38\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.53 +/- 1.12\n",
      "Episode length: 7.01 +/- 3.58\n",
      "Eval num_timesteps=50000, episode_reward=-0.43 +/- 1.07\n",
      "Episode length: 6.91 +/- 3.44\n",
      "Eval num_timesteps=60000, episode_reward=-0.41 +/- 1.02\n",
      "Episode length: 7.14 +/- 2.98\n",
      "Eval num_timesteps=70000, episode_reward=-0.41 +/- 1.00\n",
      "Episode length: 7.30 +/- 3.04\n",
      "Eval num_timesteps=80000, episode_reward=-0.53 +/- 1.03\n",
      "Episode length: 7.02 +/- 3.36\n",
      "Eval num_timesteps=90000, episode_reward=-0.45 +/- 0.98\n",
      "Episode length: 7.07 +/- 3.66\n",
      "Eval num_timesteps=100000, episode_reward=-0.49 +/- 0.97\n",
      "Episode length: 7.34 +/- 3.43\n",
      "Eval num_timesteps=110000, episode_reward=-0.29 +/- 1.05\n",
      "Episode length: 6.46 +/- 3.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.45 +/- 0.99\n",
      "Episode length: 7.31 +/- 2.98\n",
      "Eval num_timesteps=130000, episode_reward=-0.42 +/- 0.94\n",
      "Episode length: 7.46 +/- 2.82\n",
      "Eval num_timesteps=140000, episode_reward=-0.45 +/- 0.99\n",
      "Episode length: 7.47 +/- 2.90\n",
      "Eval num_timesteps=150000, episode_reward=-0.36 +/- 0.93\n",
      "Episode length: 7.39 +/- 2.82\n",
      "Eval num_timesteps=160000, episode_reward=-0.40 +/- 1.02\n",
      "Episode length: 7.16 +/- 2.88\n",
      "Eval num_timesteps=170000, episode_reward=-0.45 +/- 0.94\n",
      "Episode length: 7.74 +/- 2.62\n",
      "Eval num_timesteps=180000, episode_reward=-0.52 +/- 1.03\n",
      "Episode length: 7.42 +/- 2.99\n",
      "Eval num_timesteps=190000, episode_reward=-0.36 +/- 1.11\n",
      "Episode length: 6.45 +/- 3.47\n",
      "Eval num_timesteps=200000, episode_reward=-0.45 +/- 0.92\n",
      "Episode length: 7.58 +/- 2.72\n",
      "Evaluating best model for run 12...\n",
      "Run 12 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5800 (58.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.36\n",
      "  Average Episode Reward: -0.3990\n",
      "  Average Episode Length: 6.73\n",
      "===== COMPLETED DQN RUN 12 ====\n",
      "\n",
      "===== STARTING DQN RUN 13 with SEED 13 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.82 +/- 1.19\n",
      "Episode length: 7.20 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.20 +/- 0.95\n",
      "Episode length: 6.51 +/- 3.46\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.52 +/- 1.01\n",
      "Episode length: 7.27 +/- 3.35\n",
      "Eval num_timesteps=40000, episode_reward=-0.27 +/- 1.05\n",
      "Episode length: 6.54 +/- 3.20\n",
      "Eval num_timesteps=50000, episode_reward=-0.58 +/- 0.96\n",
      "Episode length: 7.53 +/- 3.13\n",
      "Eval num_timesteps=60000, episode_reward=-0.48 +/- 1.02\n",
      "Episode length: 7.29 +/- 2.99\n",
      "Eval num_timesteps=70000, episode_reward=-0.57 +/- 1.02\n",
      "Episode length: 7.58 +/- 2.94\n",
      "Eval num_timesteps=80000, episode_reward=-0.43 +/- 1.03\n",
      "Episode length: 7.04 +/- 3.12\n",
      "Eval num_timesteps=90000, episode_reward=-0.50 +/- 0.98\n",
      "Episode length: 7.31 +/- 2.90\n",
      "Eval num_timesteps=100000, episode_reward=-0.30 +/- 0.98\n",
      "Episode length: 6.78 +/- 3.25\n",
      "Eval num_timesteps=110000, episode_reward=-0.47 +/- 1.07\n",
      "Episode length: 6.97 +/- 3.24\n",
      "Eval num_timesteps=120000, episode_reward=-0.56 +/- 1.06\n",
      "Episode length: 7.23 +/- 3.51\n",
      "Eval num_timesteps=130000, episode_reward=-0.52 +/- 0.95\n",
      "Episode length: 7.63 +/- 3.02\n",
      "Eval num_timesteps=140000, episode_reward=-0.45 +/- 0.91\n",
      "Episode length: 7.39 +/- 3.16\n",
      "Eval num_timesteps=150000, episode_reward=-0.44 +/- 0.89\n",
      "Episode length: 7.95 +/- 2.56\n",
      "Eval num_timesteps=160000, episode_reward=-0.19 +/- 1.04\n",
      "Episode length: 6.00 +/- 3.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=-0.37 +/- 1.00\n",
      "Episode length: 6.81 +/- 3.37\n",
      "Eval num_timesteps=180000, episode_reward=-0.54 +/- 0.92\n",
      "Episode length: 7.81 +/- 3.21\n",
      "Eval num_timesteps=190000, episode_reward=-0.53 +/- 1.03\n",
      "Episode length: 7.12 +/- 3.12\n",
      "Eval num_timesteps=200000, episode_reward=-0.38 +/- 1.03\n",
      "Episode length: 6.75 +/- 3.60\n",
      "Evaluating best model for run 13...\n",
      "Run 13 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6000 (60.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.57\n",
      "  Average Episode Reward: -0.4440\n",
      "  Average Episode Length: 6.74\n",
      "===== COMPLETED DQN RUN 13 ====\n",
      "\n",
      "===== STARTING DQN RUN 14 with SEED 14 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.72 +/- 0.91\n",
      "Episode length: 7.93 +/- 3.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.49 +/- 1.00\n",
      "Episode length: 7.35 +/- 3.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.33 +/- 1.04\n",
      "Episode length: 6.79 +/- 3.31\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.42 +/- 0.97\n",
      "Episode length: 7.20 +/- 3.24\n",
      "Eval num_timesteps=50000, episode_reward=-0.44 +/- 1.15\n",
      "Episode length: 6.46 +/- 3.53\n",
      "Eval num_timesteps=60000, episode_reward=-0.48 +/- 1.05\n",
      "Episode length: 7.18 +/- 2.98\n",
      "Eval num_timesteps=70000, episode_reward=-0.49 +/- 1.03\n",
      "Episode length: 6.98 +/- 3.27\n",
      "Eval num_timesteps=80000, episode_reward=-0.42 +/- 1.06\n",
      "Episode length: 6.72 +/- 3.41\n",
      "Eval num_timesteps=90000, episode_reward=-0.42 +/- 1.02\n",
      "Episode length: 6.93 +/- 3.44\n",
      "Eval num_timesteps=100000, episode_reward=-0.50 +/- 0.96\n",
      "Episode length: 7.40 +/- 3.30\n",
      "Eval num_timesteps=110000, episode_reward=-0.36 +/- 0.98\n",
      "Episode length: 6.97 +/- 3.19\n",
      "Eval num_timesteps=120000, episode_reward=-0.27 +/- 1.00\n",
      "Episode length: 6.61 +/- 3.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.63 +/- 1.02\n",
      "Episode length: 7.60 +/- 3.08\n",
      "Eval num_timesteps=140000, episode_reward=-0.42 +/- 1.03\n",
      "Episode length: 6.81 +/- 3.37\n",
      "Eval num_timesteps=150000, episode_reward=-0.27 +/- 1.04\n",
      "Episode length: 6.53 +/- 3.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-0.54 +/- 1.03\n",
      "Episode length: 7.30 +/- 3.09\n",
      "Eval num_timesteps=170000, episode_reward=-0.39 +/- 1.01\n",
      "Episode length: 6.99 +/- 3.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.30 +/- 1.05\n",
      "Episode length: 6.33 +/- 3.59\n",
      "Eval num_timesteps=190000, episode_reward=-0.27 +/- 1.03\n",
      "Episode length: 6.32 +/- 3.39\n",
      "Eval num_timesteps=200000, episode_reward=-0.56 +/- 1.04\n",
      "Episode length: 7.18 +/- 3.36\n",
      "Evaluating best model for run 14...\n",
      "Run 14 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6500 (65.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.78\n",
      "  Average Episode Reward: -0.2695\n",
      "  Average Episode Length: 6.61\n",
      "===== COMPLETED DQN RUN 14 ====\n",
      "\n",
      "===== STARTING DQN RUN 15 with SEED 15 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.38 +/- 1.13\n",
      "Episode length: 6.41 +/- 3.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.21 +/- 1.03\n",
      "Episode length: 5.92 +/- 3.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.50 +/- 0.94\n",
      "Episode length: 7.43 +/- 2.93\n",
      "Eval num_timesteps=40000, episode_reward=-0.47 +/- 1.14\n",
      "Episode length: 6.66 +/- 3.55\n",
      "Eval num_timesteps=50000, episode_reward=-0.51 +/- 1.08\n",
      "Episode length: 7.14 +/- 2.95\n",
      "Eval num_timesteps=60000, episode_reward=-0.36 +/- 0.99\n",
      "Episode length: 7.09 +/- 2.87\n",
      "Eval num_timesteps=70000, episode_reward=-0.24 +/- 0.99\n",
      "Episode length: 6.50 +/- 3.36\n",
      "Eval num_timesteps=80000, episode_reward=-0.69 +/- 0.99\n",
      "Episode length: 7.53 +/- 3.13\n",
      "Eval num_timesteps=90000, episode_reward=-0.36 +/- 0.99\n",
      "Episode length: 7.00 +/- 3.20\n",
      "Eval num_timesteps=100000, episode_reward=-0.49 +/- 0.96\n",
      "Episode length: 7.49 +/- 3.08\n",
      "Eval num_timesteps=110000, episode_reward=-0.19 +/- 1.02\n",
      "Episode length: 6.25 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.53 +/- 0.87\n",
      "Episode length: 8.09 +/- 2.57\n",
      "Eval num_timesteps=130000, episode_reward=-0.25 +/- 0.96\n",
      "Episode length: 6.79 +/- 3.34\n",
      "Eval num_timesteps=140000, episode_reward=-0.43 +/- 0.95\n",
      "Episode length: 7.40 +/- 2.81\n",
      "Eval num_timesteps=150000, episode_reward=-0.50 +/- 1.06\n",
      "Episode length: 7.24 +/- 3.11\n",
      "Eval num_timesteps=160000, episode_reward=-0.36 +/- 0.91\n",
      "Episode length: 7.25 +/- 3.06\n",
      "Eval num_timesteps=170000, episode_reward=-0.45 +/- 1.12\n",
      "Episode length: 6.75 +/- 3.56\n",
      "Eval num_timesteps=180000, episode_reward=-0.35 +/- 1.00\n",
      "Episode length: 6.92 +/- 3.56\n",
      "Eval num_timesteps=190000, episode_reward=-0.46 +/- 1.00\n",
      "Episode length: 7.31 +/- 3.34\n",
      "Eval num_timesteps=200000, episode_reward=-0.30 +/- 1.00\n",
      "Episode length: 6.76 +/- 3.14\n",
      "Evaluating best model for run 15...\n",
      "Run 15 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6300 (63.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.52\n",
      "  Average Episode Reward: -0.3610\n",
      "  Average Episode Length: 6.55\n",
      "===== COMPLETED DQN RUN 15 ====\n",
      "\n",
      "===== STARTING DQN RUN 16 with SEED 16 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.34 +/- 1.06\n",
      "Episode length: 6.69 +/- 3.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.41 +/- 1.08\n",
      "Episode length: 6.89 +/- 3.14\n",
      "Eval num_timesteps=30000, episode_reward=-0.35 +/- 1.03\n",
      "Episode length: 6.87 +/- 3.01\n",
      "Eval num_timesteps=40000, episode_reward=-0.51 +/- 1.02\n",
      "Episode length: 7.42 +/- 2.90\n",
      "Eval num_timesteps=50000, episode_reward=-0.53 +/- 1.02\n",
      "Episode length: 7.31 +/- 3.00\n",
      "Eval num_timesteps=60000, episode_reward=-0.58 +/- 1.04\n",
      "Episode length: 7.34 +/- 3.23\n",
      "Eval num_timesteps=70000, episode_reward=-0.39 +/- 1.03\n",
      "Episode length: 6.90 +/- 3.36\n",
      "Eval num_timesteps=80000, episode_reward=-0.32 +/- 1.05\n",
      "Episode length: 6.40 +/- 3.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-0.39 +/- 1.00\n",
      "Episode length: 7.16 +/- 2.81\n",
      "Eval num_timesteps=100000, episode_reward=-0.42 +/- 0.99\n",
      "Episode length: 7.19 +/- 3.28\n",
      "Eval num_timesteps=110000, episode_reward=-0.43 +/- 1.07\n",
      "Episode length: 6.88 +/- 3.03\n",
      "Eval num_timesteps=120000, episode_reward=-0.27 +/- 1.02\n",
      "Episode length: 6.61 +/- 3.09\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.48 +/- 1.08\n",
      "Episode length: 6.93 +/- 3.10\n",
      "Eval num_timesteps=140000, episode_reward=-0.44 +/- 0.98\n",
      "Episode length: 7.36 +/- 3.09\n",
      "Eval num_timesteps=150000, episode_reward=-0.38 +/- 1.00\n",
      "Episode length: 6.96 +/- 3.04\n",
      "Eval num_timesteps=160000, episode_reward=-0.48 +/- 1.03\n",
      "Episode length: 7.04 +/- 3.03\n",
      "Eval num_timesteps=170000, episode_reward=-0.44 +/- 1.03\n",
      "Episode length: 7.12 +/- 2.98\n",
      "Eval num_timesteps=180000, episode_reward=-0.27 +/- 0.99\n",
      "Episode length: 6.71 +/- 3.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=190000, episode_reward=-0.54 +/- 0.96\n",
      "Episode length: 7.63 +/- 3.17\n",
      "Eval num_timesteps=200000, episode_reward=-0.54 +/- 0.96\n",
      "Episode length: 7.68 +/- 3.07\n",
      "Evaluating best model for run 16...\n",
      "Run 16 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6300 (63.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.16\n",
      "  Average Episode Reward: -0.2205\n",
      "  Average Episode Length: 6.32\n",
      "===== COMPLETED DQN RUN 16 ====\n",
      "\n",
      "===== STARTING DQN RUN 17 with SEED 17 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.59 +/- 1.09\n",
      "Episode length: 7.06 +/- 3.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.46 +/- 1.06\n",
      "Episode length: 7.05 +/- 3.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.35 +/- 1.07\n",
      "Episode length: 6.69 +/- 3.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.49 +/- 1.04\n",
      "Episode length: 7.21 +/- 3.10\n",
      "Eval num_timesteps=50000, episode_reward=-0.39 +/- 1.04\n",
      "Episode length: 6.89 +/- 3.17\n",
      "Eval num_timesteps=60000, episode_reward=-0.44 +/- 0.98\n",
      "Episode length: 7.35 +/- 2.84\n",
      "Eval num_timesteps=70000, episode_reward=-0.61 +/- 0.90\n",
      "Episode length: 8.07 +/- 2.80\n",
      "Eval num_timesteps=80000, episode_reward=-0.35 +/- 0.97\n",
      "Episode length: 6.82 +/- 3.07\n",
      "Eval num_timesteps=90000, episode_reward=-0.49 +/- 1.06\n",
      "Episode length: 6.84 +/- 3.30\n",
      "Eval num_timesteps=100000, episode_reward=-0.36 +/- 0.91\n",
      "Episode length: 7.40 +/- 2.80\n",
      "Eval num_timesteps=110000, episode_reward=-0.60 +/- 0.93\n",
      "Episode length: 7.83 +/- 2.61\n",
      "Eval num_timesteps=120000, episode_reward=-0.36 +/- 0.98\n",
      "Episode length: 7.08 +/- 3.03\n",
      "Eval num_timesteps=130000, episode_reward=-0.52 +/- 0.92\n",
      "Episode length: 7.73 +/- 2.88\n",
      "Eval num_timesteps=140000, episode_reward=-0.41 +/- 0.94\n",
      "Episode length: 7.37 +/- 2.97\n",
      "Eval num_timesteps=150000, episode_reward=-0.21 +/- 0.98\n",
      "Episode length: 6.47 +/- 3.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-0.48 +/- 0.99\n",
      "Episode length: 7.14 +/- 3.13\n",
      "Eval num_timesteps=170000, episode_reward=-0.57 +/- 0.90\n",
      "Episode length: 7.95 +/- 2.87\n",
      "Eval num_timesteps=180000, episode_reward=-0.36 +/- 0.96\n",
      "Episode length: 6.97 +/- 3.21\n",
      "Eval num_timesteps=190000, episode_reward=-0.29 +/- 0.97\n",
      "Episode length: 6.64 +/- 3.09\n",
      "Eval num_timesteps=200000, episode_reward=-0.59 +/- 0.92\n",
      "Episode length: 7.55 +/- 2.71\n",
      "Evaluating best model for run 17...\n",
      "Run 17 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5000 (50.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.78\n",
      "  Average Episode Reward: -0.5230\n",
      "  Average Episode Length: 7.39\n",
      "===== COMPLETED DQN RUN 17 ====\n",
      "\n",
      "===== STARTING DQN RUN 18 with SEED 18 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.55 +/- 1.11\n",
      "Episode length: 6.83 +/- 3.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.37 +/- 1.01\n",
      "Episode length: 6.99 +/- 2.94\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.39 +/- 1.07\n",
      "Episode length: 6.73 +/- 3.34\n",
      "Eval num_timesteps=40000, episode_reward=-0.27 +/- 0.95\n",
      "Episode length: 6.79 +/- 3.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.43 +/- 1.04\n",
      "Episode length: 6.79 +/- 3.24\n",
      "Eval num_timesteps=60000, episode_reward=-0.37 +/- 1.03\n",
      "Episode length: 6.81 +/- 3.34\n",
      "Eval num_timesteps=70000, episode_reward=-0.24 +/- 1.00\n",
      "Episode length: 6.45 +/- 3.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.43 +/- 0.97\n",
      "Episode length: 7.27 +/- 2.77\n",
      "Eval num_timesteps=90000, episode_reward=-0.40 +/- 1.07\n",
      "Episode length: 6.78 +/- 3.14\n",
      "Eval num_timesteps=100000, episode_reward=-0.36 +/- 1.00\n",
      "Episode length: 6.83 +/- 3.27\n",
      "Eval num_timesteps=110000, episode_reward=-0.37 +/- 0.95\n",
      "Episode length: 7.14 +/- 2.88\n",
      "Eval num_timesteps=120000, episode_reward=-0.56 +/- 1.01\n",
      "Episode length: 7.43 +/- 3.09\n",
      "Eval num_timesteps=130000, episode_reward=-0.49 +/- 1.00\n",
      "Episode length: 7.31 +/- 3.09\n",
      "Eval num_timesteps=140000, episode_reward=-0.29 +/- 1.03\n",
      "Episode length: 6.61 +/- 3.15\n",
      "Eval num_timesteps=150000, episode_reward=-0.51 +/- 1.07\n",
      "Episode length: 7.25 +/- 3.17\n",
      "Eval num_timesteps=160000, episode_reward=-0.30 +/- 0.96\n",
      "Episode length: 6.95 +/- 2.92\n",
      "Eval num_timesteps=170000, episode_reward=-0.41 +/- 0.97\n",
      "Episode length: 7.51 +/- 2.87\n",
      "Eval num_timesteps=180000, episode_reward=-0.36 +/- 0.94\n",
      "Episode length: 7.33 +/- 2.66\n",
      "Eval num_timesteps=190000, episode_reward=-0.57 +/- 1.03\n",
      "Episode length: 7.39 +/- 3.18\n",
      "Eval num_timesteps=200000, episode_reward=-0.51 +/- 1.01\n",
      "Episode length: 7.54 +/- 2.87\n",
      "Evaluating best model for run 18...\n",
      "Run 18 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6100 (61.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.72\n",
      "  Average Episode Reward: -0.3910\n",
      "  Average Episode Length: 6.78\n",
      "===== COMPLETED DQN RUN 18 ====\n",
      "\n",
      "===== STARTING DQN RUN 19 with SEED 19 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.64 +/- 1.18\n",
      "Episode length: 6.92 +/- 3.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.34 +/- 1.08\n",
      "Episode length: 6.31 +/- 3.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.34 +/- 1.06\n",
      "Episode length: 6.51 +/- 3.21\n",
      "Eval num_timesteps=40000, episode_reward=-0.38 +/- 1.12\n",
      "Episode length: 6.63 +/- 3.14\n",
      "Eval num_timesteps=50000, episode_reward=-0.36 +/- 1.07\n",
      "Episode length: 6.62 +/- 3.17\n",
      "Eval num_timesteps=60000, episode_reward=-0.46 +/- 1.02\n",
      "Episode length: 6.95 +/- 3.10\n",
      "Eval num_timesteps=70000, episode_reward=-0.43 +/- 1.04\n",
      "Episode length: 6.89 +/- 3.15\n",
      "Eval num_timesteps=80000, episode_reward=-0.31 +/- 1.04\n",
      "Episode length: 6.62 +/- 3.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-0.20 +/- 0.97\n",
      "Episode length: 6.53 +/- 3.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.35 +/- 1.10\n",
      "Episode length: 6.62 +/- 3.33\n",
      "Eval num_timesteps=110000, episode_reward=-0.47 +/- 0.92\n",
      "Episode length: 7.64 +/- 3.10\n",
      "Eval num_timesteps=120000, episode_reward=-0.29 +/- 0.94\n",
      "Episode length: 6.91 +/- 2.96\n",
      "Eval num_timesteps=130000, episode_reward=-0.38 +/- 0.98\n",
      "Episode length: 7.11 +/- 3.20\n",
      "Eval num_timesteps=140000, episode_reward=-0.29 +/- 1.01\n",
      "Episode length: 6.60 +/- 3.22\n",
      "Eval num_timesteps=150000, episode_reward=-0.32 +/- 0.95\n",
      "Episode length: 6.87 +/- 3.08\n",
      "Eval num_timesteps=160000, episode_reward=-0.39 +/- 0.94\n",
      "Episode length: 7.35 +/- 3.01\n",
      "Eval num_timesteps=170000, episode_reward=-0.37 +/- 1.03\n",
      "Episode length: 6.84 +/- 3.23\n",
      "Eval num_timesteps=180000, episode_reward=-0.29 +/- 1.02\n",
      "Episode length: 6.75 +/- 2.97\n",
      "Eval num_timesteps=190000, episode_reward=-0.44 +/- 0.94\n",
      "Episode length: 7.47 +/- 2.94\n",
      "Eval num_timesteps=200000, episode_reward=-0.34 +/- 0.96\n",
      "Episode length: 7.05 +/- 3.00\n",
      "Evaluating best model for run 19...\n",
      "Run 19 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6100 (61.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.80\n",
      "  Average Episode Reward: -0.3435\n",
      "  Average Episode Length: 6.83\n",
      "===== COMPLETED DQN RUN 19 ====\n",
      "\n",
      "===== STARTING DQN RUN 20 with SEED 20 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.65 +/- 1.11\n",
      "Episode length: 7.24 +/- 3.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.21 +/- 1.05\n",
      "Episode length: 5.87 +/- 3.34\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.39 +/- 1.12\n",
      "Episode length: 6.58 +/- 3.25\n",
      "Eval num_timesteps=40000, episode_reward=-0.38 +/- 1.08\n",
      "Episode length: 6.73 +/- 3.24\n",
      "Eval num_timesteps=50000, episode_reward=-0.54 +/- 1.08\n",
      "Episode length: 7.28 +/- 3.12\n",
      "Eval num_timesteps=60000, episode_reward=-0.41 +/- 0.96\n",
      "Episode length: 7.15 +/- 3.01\n",
      "Eval num_timesteps=70000, episode_reward=-0.42 +/- 1.07\n",
      "Episode length: 6.75 +/- 3.22\n",
      "Eval num_timesteps=80000, episode_reward=-0.47 +/- 1.02\n",
      "Episode length: 7.13 +/- 3.19\n",
      "Eval num_timesteps=90000, episode_reward=-0.42 +/- 0.96\n",
      "Episode length: 7.18 +/- 3.36\n",
      "Eval num_timesteps=100000, episode_reward=-0.49 +/- 1.10\n",
      "Episode length: 6.71 +/- 3.42\n",
      "Eval num_timesteps=110000, episode_reward=-0.48 +/- 0.99\n",
      "Episode length: 7.27 +/- 3.00\n",
      "Eval num_timesteps=120000, episode_reward=-0.47 +/- 1.00\n",
      "Episode length: 7.11 +/- 2.91\n",
      "Eval num_timesteps=130000, episode_reward=-0.42 +/- 0.96\n",
      "Episode length: 7.05 +/- 3.01\n",
      "Eval num_timesteps=140000, episode_reward=-0.29 +/- 1.02\n",
      "Episode length: 6.56 +/- 3.12\n",
      "Eval num_timesteps=150000, episode_reward=-0.34 +/- 1.00\n",
      "Episode length: 6.85 +/- 3.10\n",
      "Eval num_timesteps=160000, episode_reward=-0.35 +/- 1.05\n",
      "Episode length: 6.65 +/- 3.47\n",
      "Eval num_timesteps=170000, episode_reward=-0.41 +/- 1.09\n",
      "Episode length: 6.73 +/- 3.16\n",
      "Eval num_timesteps=180000, episode_reward=-0.44 +/- 0.98\n",
      "Episode length: 7.21 +/- 2.86\n",
      "Eval num_timesteps=190000, episode_reward=-0.43 +/- 1.11\n",
      "Episode length: 6.79 +/- 3.15\n",
      "Eval num_timesteps=200000, episode_reward=-0.51 +/- 1.01\n",
      "Episode length: 7.14 +/- 3.33\n",
      "Evaluating best model for run 20...\n",
      "Run 20 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.7400 (74.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.49\n",
      "  Average Episode Reward: -0.1935\n",
      "  Average Episode Length: 5.92\n",
      "===== COMPLETED DQN RUN 20 ====\n",
      "\n",
      "===== STARTING DQN RUN 21 with SEED 21 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.48 +/- 1.10\n",
      "Episode length: 6.86 +/- 3.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.24 +/- 1.07\n",
      "Episode length: 6.10 +/- 3.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.44 +/- 1.06\n",
      "Episode length: 6.85 +/- 3.32\n",
      "Eval num_timesteps=40000, episode_reward=-0.33 +/- 0.97\n",
      "Episode length: 6.95 +/- 2.88\n",
      "Eval num_timesteps=50000, episode_reward=-0.44 +/- 0.95\n",
      "Episode length: 7.35 +/- 3.13\n",
      "Eval num_timesteps=60000, episode_reward=-0.43 +/- 1.03\n",
      "Episode length: 7.11 +/- 3.16\n",
      "Eval num_timesteps=70000, episode_reward=-0.37 +/- 0.97\n",
      "Episode length: 7.21 +/- 2.90\n",
      "Eval num_timesteps=80000, episode_reward=-0.37 +/- 1.01\n",
      "Episode length: 6.99 +/- 3.09\n",
      "Eval num_timesteps=90000, episode_reward=-0.51 +/- 0.92\n",
      "Episode length: 7.73 +/- 2.83\n",
      "Eval num_timesteps=100000, episode_reward=-0.44 +/- 0.98\n",
      "Episode length: 7.33 +/- 3.23\n",
      "Eval num_timesteps=110000, episode_reward=-0.43 +/- 1.04\n",
      "Episode length: 7.17 +/- 3.19\n",
      "Eval num_timesteps=120000, episode_reward=-0.49 +/- 1.04\n",
      "Episode length: 7.18 +/- 3.31\n",
      "Eval num_timesteps=130000, episode_reward=-0.32 +/- 1.03\n",
      "Episode length: 6.81 +/- 3.18\n",
      "Eval num_timesteps=140000, episode_reward=-0.43 +/- 1.01\n",
      "Episode length: 7.05 +/- 3.11\n",
      "Eval num_timesteps=150000, episode_reward=-0.27 +/- 1.01\n",
      "Episode length: 6.59 +/- 3.12\n",
      "Eval num_timesteps=160000, episode_reward=-0.38 +/- 0.96\n",
      "Episode length: 7.23 +/- 3.01\n",
      "Eval num_timesteps=170000, episode_reward=-0.57 +/- 0.86\n",
      "Episode length: 8.09 +/- 2.74\n",
      "Eval num_timesteps=180000, episode_reward=-0.23 +/- 1.00\n",
      "Episode length: 6.48 +/- 3.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=190000, episode_reward=-0.36 +/- 1.08\n",
      "Episode length: 6.63 +/- 3.33\n",
      "Eval num_timesteps=200000, episode_reward=-0.38 +/- 1.10\n",
      "Episode length: 6.66 +/- 3.26\n",
      "Evaluating best model for run 21...\n",
      "Run 21 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5700 (57.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.56\n",
      "  Average Episode Reward: -0.4375\n",
      "  Average Episode Length: 6.90\n",
      "===== COMPLETED DQN RUN 21 ====\n",
      "\n",
      "===== STARTING DQN RUN 22 with SEED 22 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.62 +/- 1.14\n",
      "Episode length: 7.05 +/- 3.46\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.47 +/- 1.02\n",
      "Episode length: 7.07 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.51 +/- 1.05\n",
      "Episode length: 7.24 +/- 3.22\n",
      "Eval num_timesteps=40000, episode_reward=-0.38 +/- 0.99\n",
      "Episode length: 7.06 +/- 3.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.42 +/- 0.83\n",
      "Episode length: 7.76 +/- 2.55\n",
      "Eval num_timesteps=60000, episode_reward=-0.20 +/- 1.00\n",
      "Episode length: 6.50 +/- 3.09\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.45 +/- 0.88\n",
      "Episode length: 7.79 +/- 2.65\n",
      "Eval num_timesteps=80000, episode_reward=-0.38 +/- 0.96\n",
      "Episode length: 7.20 +/- 3.12\n",
      "Eval num_timesteps=90000, episode_reward=-0.22 +/- 1.04\n",
      "Episode length: 6.19 +/- 3.16\n",
      "Eval num_timesteps=100000, episode_reward=-0.58 +/- 1.02\n",
      "Episode length: 7.69 +/- 2.80\n",
      "Eval num_timesteps=110000, episode_reward=-0.61 +/- 0.79\n",
      "Episode length: 8.34 +/- 2.78\n",
      "Eval num_timesteps=120000, episode_reward=-0.32 +/- 0.97\n",
      "Episode length: 7.06 +/- 2.72\n",
      "Eval num_timesteps=130000, episode_reward=-0.43 +/- 1.00\n",
      "Episode length: 7.36 +/- 3.01\n",
      "Eval num_timesteps=140000, episode_reward=-0.53 +/- 1.10\n",
      "Episode length: 7.08 +/- 3.38\n",
      "Eval num_timesteps=150000, episode_reward=-0.31 +/- 0.91\n",
      "Episode length: 7.37 +/- 2.64\n",
      "Eval num_timesteps=160000, episode_reward=-0.39 +/- 1.05\n",
      "Episode length: 6.79 +/- 3.32\n",
      "Eval num_timesteps=170000, episode_reward=-0.55 +/- 0.99\n",
      "Episode length: 7.59 +/- 2.95\n",
      "Eval num_timesteps=180000, episode_reward=-0.32 +/- 0.96\n",
      "Episode length: 6.88 +/- 3.26\n",
      "Eval num_timesteps=190000, episode_reward=-0.57 +/- 0.95\n",
      "Episode length: 7.63 +/- 3.19\n",
      "Eval num_timesteps=200000, episode_reward=-0.65 +/- 0.99\n",
      "Episode length: 7.78 +/- 3.17\n",
      "Evaluating best model for run 22...\n",
      "Run 22 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5400 (54.00%)\n",
      "  Average Detection Time (Successful Episodes): 6.02\n",
      "  Average Episode Reward: -0.5515\n",
      "  Average Episode Length: 7.85\n",
      "===== COMPLETED DQN RUN 22 ====\n",
      "\n",
      "===== STARTING DQN RUN 23 with SEED 23 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.44 +/- 1.03\n",
      "Episode length: 7.11 +/- 3.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.39 +/- 1.04\n",
      "Episode length: 7.08 +/- 2.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.39 +/- 1.02\n",
      "Episode length: 7.03 +/- 2.90\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.20 +/- 0.93\n",
      "Episode length: 6.66 +/- 2.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.63 +/- 1.05\n",
      "Episode length: 7.56 +/- 2.84\n",
      "Eval num_timesteps=60000, episode_reward=-0.38 +/- 1.01\n",
      "Episode length: 6.96 +/- 2.98\n",
      "Eval num_timesteps=70000, episode_reward=-0.52 +/- 0.99\n",
      "Episode length: 7.31 +/- 3.03\n",
      "Eval num_timesteps=80000, episode_reward=-0.71 +/- 1.00\n",
      "Episode length: 7.88 +/- 2.95\n",
      "Eval num_timesteps=90000, episode_reward=-0.34 +/- 0.99\n",
      "Episode length: 7.03 +/- 3.27\n",
      "Eval num_timesteps=100000, episode_reward=-0.41 +/- 1.00\n",
      "Episode length: 7.03 +/- 3.34\n",
      "Eval num_timesteps=110000, episode_reward=-0.51 +/- 0.92\n",
      "Episode length: 7.77 +/- 2.98\n",
      "Eval num_timesteps=120000, episode_reward=-0.37 +/- 1.01\n",
      "Episode length: 6.98 +/- 2.89\n",
      "Eval num_timesteps=130000, episode_reward=-0.53 +/- 0.98\n",
      "Episode length: 7.65 +/- 2.66\n",
      "Eval num_timesteps=140000, episode_reward=-0.51 +/- 1.06\n",
      "Episode length: 7.16 +/- 3.21\n",
      "Eval num_timesteps=150000, episode_reward=-0.31 +/- 1.00\n",
      "Episode length: 6.80 +/- 2.98\n",
      "Eval num_timesteps=160000, episode_reward=-0.34 +/- 1.00\n",
      "Episode length: 6.89 +/- 3.19\n",
      "Eval num_timesteps=170000, episode_reward=-0.42 +/- 0.99\n",
      "Episode length: 7.27 +/- 3.11\n",
      "Eval num_timesteps=180000, episode_reward=-0.55 +/- 0.99\n",
      "Episode length: 7.76 +/- 2.59\n",
      "Eval num_timesteps=190000, episode_reward=-0.57 +/- 0.97\n",
      "Episode length: 7.91 +/- 2.59\n",
      "Eval num_timesteps=200000, episode_reward=-0.35 +/- 0.98\n",
      "Episode length: 7.22 +/- 2.60\n",
      "Evaluating best model for run 23...\n",
      "Run 23 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5800 (58.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.69\n",
      "  Average Episode Reward: -0.5350\n",
      "  Average Episode Length: 7.50\n",
      "===== COMPLETED DQN RUN 23 ====\n",
      "\n",
      "===== STARTING DQN RUN 24 with SEED 24 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.54 +/- 1.07\n",
      "Episode length: 6.96 +/- 3.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.42 +/- 1.01\n",
      "Episode length: 6.97 +/- 3.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.62 +/- 0.88\n",
      "Episode length: 8.11 +/- 2.51\n",
      "Eval num_timesteps=40000, episode_reward=-0.53 +/- 0.94\n",
      "Episode length: 7.38 +/- 3.04\n",
      "Eval num_timesteps=50000, episode_reward=-0.31 +/- 1.07\n",
      "Episode length: 6.33 +/- 3.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.50 +/- 1.03\n",
      "Episode length: 7.16 +/- 3.16\n",
      "Eval num_timesteps=70000, episode_reward=-0.40 +/- 1.09\n",
      "Episode length: 6.41 +/- 3.26\n",
      "Eval num_timesteps=80000, episode_reward=-0.39 +/- 1.00\n",
      "Episode length: 6.94 +/- 3.30\n",
      "Eval num_timesteps=90000, episode_reward=-0.25 +/- 0.98\n",
      "Episode length: 6.51 +/- 3.41\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.31 +/- 1.01\n",
      "Episode length: 6.67 +/- 3.39\n",
      "Eval num_timesteps=110000, episode_reward=-0.56 +/- 1.00\n",
      "Episode length: 7.51 +/- 3.17\n",
      "Eval num_timesteps=120000, episode_reward=-0.56 +/- 1.03\n",
      "Episode length: 7.62 +/- 3.00\n",
      "Eval num_timesteps=130000, episode_reward=-0.44 +/- 1.03\n",
      "Episode length: 7.04 +/- 3.10\n",
      "Eval num_timesteps=140000, episode_reward=-0.48 +/- 0.91\n",
      "Episode length: 7.85 +/- 2.48\n",
      "Eval num_timesteps=150000, episode_reward=-0.30 +/- 0.97\n",
      "Episode length: 6.69 +/- 3.27\n",
      "Eval num_timesteps=160000, episode_reward=-0.48 +/- 0.94\n",
      "Episode length: 7.49 +/- 3.03\n",
      "Eval num_timesteps=170000, episode_reward=-0.61 +/- 1.03\n",
      "Episode length: 7.66 +/- 3.15\n",
      "Eval num_timesteps=180000, episode_reward=-0.32 +/- 1.04\n",
      "Episode length: 6.72 +/- 3.48\n",
      "Eval num_timesteps=190000, episode_reward=-0.19 +/- 1.07\n",
      "Episode length: 6.06 +/- 3.18\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-0.66 +/- 1.03\n",
      "Episode length: 7.74 +/- 2.95\n",
      "Evaluating best model for run 24...\n",
      "Run 24 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6500 (65.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.63\n",
      "  Average Episode Reward: -0.3805\n",
      "  Average Episode Length: 6.51\n",
      "===== COMPLETED DQN RUN 24 ====\n",
      "\n",
      "===== STARTING DQN RUN 25 with SEED 25 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.64 +/- 1.03\n",
      "Episode length: 7.61 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.52 +/- 1.04\n",
      "Episode length: 7.29 +/- 3.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.43 +/- 1.08\n",
      "Episode length: 6.96 +/- 3.31\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.41 +/- 0.99\n",
      "Episode length: 7.02 +/- 2.79\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.66 +/- 1.04\n",
      "Episode length: 7.66 +/- 2.70\n",
      "Eval num_timesteps=60000, episode_reward=-0.39 +/- 1.04\n",
      "Episode length: 6.91 +/- 3.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.45 +/- 1.00\n",
      "Episode length: 7.27 +/- 3.19\n",
      "Eval num_timesteps=80000, episode_reward=-0.56 +/- 1.08\n",
      "Episode length: 7.25 +/- 3.07\n",
      "Eval num_timesteps=90000, episode_reward=-0.32 +/- 1.03\n",
      "Episode length: 6.71 +/- 2.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.30 +/- 1.06\n",
      "Episode length: 6.44 +/- 3.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-0.40 +/- 1.09\n",
      "Episode length: 6.66 +/- 3.23\n",
      "Eval num_timesteps=120000, episode_reward=-0.41 +/- 1.06\n",
      "Episode length: 6.76 +/- 3.28\n",
      "Eval num_timesteps=130000, episode_reward=-0.57 +/- 0.96\n",
      "Episode length: 7.70 +/- 2.67\n",
      "Eval num_timesteps=140000, episode_reward=-0.09 +/- 0.97\n",
      "Episode length: 6.06 +/- 2.96\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-0.47 +/- 1.01\n",
      "Episode length: 7.24 +/- 3.20\n",
      "Eval num_timesteps=160000, episode_reward=-0.52 +/- 1.07\n",
      "Episode length: 7.24 +/- 3.04\n",
      "Eval num_timesteps=170000, episode_reward=-0.42 +/- 0.96\n",
      "Episode length: 7.38 +/- 2.69\n",
      "Eval num_timesteps=180000, episode_reward=-0.37 +/- 1.02\n",
      "Episode length: 6.88 +/- 2.93\n",
      "Eval num_timesteps=190000, episode_reward=-0.56 +/- 1.05\n",
      "Episode length: 7.34 +/- 3.11\n",
      "Eval num_timesteps=200000, episode_reward=-0.48 +/- 1.04\n",
      "Episode length: 7.05 +/- 3.26\n",
      "Evaluating best model for run 25...\n",
      "Run 25 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6100 (61.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.30\n",
      "  Average Episode Reward: -0.4315\n",
      "  Average Episode Length: 7.13\n",
      "===== COMPLETED DQN RUN 25 ====\n",
      "\n",
      "===== STARTING DQN RUN 26 with SEED 26 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.61 +/- 1.04\n",
      "Episode length: 7.46 +/- 2.91\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.41 +/- 1.03\n",
      "Episode length: 6.98 +/- 2.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.62 +/- 1.00\n",
      "Episode length: 7.63 +/- 3.12\n",
      "Eval num_timesteps=40000, episode_reward=-0.39 +/- 1.03\n",
      "Episode length: 7.02 +/- 2.86\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.56 +/- 1.03\n",
      "Episode length: 7.50 +/- 2.82\n",
      "Eval num_timesteps=60000, episode_reward=-0.42 +/- 0.99\n",
      "Episode length: 7.02 +/- 3.30\n",
      "Eval num_timesteps=70000, episode_reward=-0.45 +/- 1.01\n",
      "Episode length: 7.21 +/- 3.01\n",
      "Eval num_timesteps=80000, episode_reward=-0.47 +/- 0.98\n",
      "Episode length: 7.42 +/- 2.93\n",
      "Eval num_timesteps=90000, episode_reward=-0.56 +/- 1.05\n",
      "Episode length: 7.39 +/- 2.86\n",
      "Eval num_timesteps=100000, episode_reward=-0.51 +/- 1.09\n",
      "Episode length: 7.09 +/- 3.24\n",
      "Eval num_timesteps=110000, episode_reward=-0.50 +/- 1.07\n",
      "Episode length: 7.00 +/- 3.04\n",
      "Eval num_timesteps=120000, episode_reward=-0.38 +/- 1.10\n",
      "Episode length: 6.60 +/- 3.34\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.27 +/- 1.02\n",
      "Episode length: 6.51 +/- 3.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-0.58 +/- 1.00\n",
      "Episode length: 7.54 +/- 3.36\n",
      "Eval num_timesteps=150000, episode_reward=-0.41 +/- 1.01\n",
      "Episode length: 7.00 +/- 3.04\n",
      "Eval num_timesteps=160000, episode_reward=-0.29 +/- 1.01\n",
      "Episode length: 6.62 +/- 3.21\n",
      "Eval num_timesteps=170000, episode_reward=-0.33 +/- 1.01\n",
      "Episode length: 6.85 +/- 3.28\n",
      "Eval num_timesteps=180000, episode_reward=-0.46 +/- 1.05\n",
      "Episode length: 7.01 +/- 3.07\n",
      "Eval num_timesteps=190000, episode_reward=-0.54 +/- 0.98\n",
      "Episode length: 7.64 +/- 2.93\n",
      "Eval num_timesteps=200000, episode_reward=-0.43 +/- 1.01\n",
      "Episode length: 6.90 +/- 3.11\n",
      "Evaluating best model for run 26...\n",
      "Run 26 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5600 (56.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.66\n",
      "  Average Episode Reward: -0.4700\n",
      "  Average Episode Length: 7.57\n",
      "===== COMPLETED DQN RUN 26 ====\n",
      "\n",
      "===== STARTING DQN RUN 27 with SEED 27 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.41 +/- 1.01\n",
      "Episode length: 6.84 +/- 3.09\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.39 +/- 1.02\n",
      "Episode length: 7.05 +/- 3.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.53 +/- 0.89\n",
      "Episode length: 7.81 +/- 2.86\n",
      "Eval num_timesteps=40000, episode_reward=-0.31 +/- 1.05\n",
      "Episode length: 6.54 +/- 3.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.61 +/- 0.76\n",
      "Episode length: 8.49 +/- 2.77\n",
      "Eval num_timesteps=60000, episode_reward=-0.59 +/- 0.93\n",
      "Episode length: 7.84 +/- 2.82\n",
      "Eval num_timesteps=70000, episode_reward=-0.33 +/- 1.04\n",
      "Episode length: 6.57 +/- 3.41\n",
      "Eval num_timesteps=80000, episode_reward=-0.42 +/- 1.00\n",
      "Episode length: 7.04 +/- 3.09\n",
      "Eval num_timesteps=90000, episode_reward=-0.46 +/- 0.95\n",
      "Episode length: 7.52 +/- 3.01\n",
      "Eval num_timesteps=100000, episode_reward=-0.59 +/- 0.94\n",
      "Episode length: 7.80 +/- 3.00\n",
      "Eval num_timesteps=110000, episode_reward=-0.53 +/- 0.96\n",
      "Episode length: 7.42 +/- 3.12\n",
      "Eval num_timesteps=120000, episode_reward=-0.32 +/- 0.93\n",
      "Episode length: 7.19 +/- 3.04\n",
      "Eval num_timesteps=130000, episode_reward=-0.47 +/- 0.91\n",
      "Episode length: 7.72 +/- 2.77\n",
      "Eval num_timesteps=140000, episode_reward=-0.35 +/- 0.93\n",
      "Episode length: 7.35 +/- 3.08\n",
      "Eval num_timesteps=150000, episode_reward=-0.41 +/- 0.98\n",
      "Episode length: 7.18 +/- 3.12\n",
      "Eval num_timesteps=160000, episode_reward=-0.34 +/- 1.05\n",
      "Episode length: 6.81 +/- 3.43\n",
      "Eval num_timesteps=170000, episode_reward=-0.55 +/- 0.99\n",
      "Episode length: 7.50 +/- 3.04\n",
      "Eval num_timesteps=180000, episode_reward=-0.46 +/- 0.95\n",
      "Episode length: 7.73 +/- 2.65\n",
      "Eval num_timesteps=190000, episode_reward=-0.25 +/- 1.05\n",
      "Episode length: 6.43 +/- 3.52\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-0.40 +/- 1.01\n",
      "Episode length: 7.22 +/- 2.98\n",
      "Evaluating best model for run 27...\n",
      "Run 27 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5700 (57.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.14\n",
      "  Average Episode Reward: -0.4585\n",
      "  Average Episode Length: 7.23\n",
      "===== COMPLETED DQN RUN 27 ====\n",
      "\n",
      "===== STARTING DQN RUN 28 with SEED 28 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.72 +/- 0.82\n",
      "Episode length: 8.42 +/- 2.94\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.57 +/- 1.14\n",
      "Episode length: 7.08 +/- 3.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.35 +/- 1.04\n",
      "Episode length: 6.46 +/- 3.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.40 +/- 1.01\n",
      "Episode length: 6.87 +/- 3.31\n",
      "Eval num_timesteps=50000, episode_reward=-0.28 +/- 1.03\n",
      "Episode length: 6.60 +/- 3.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.60 +/- 0.91\n",
      "Episode length: 8.03 +/- 2.51\n",
      "Eval num_timesteps=70000, episode_reward=-0.46 +/- 1.03\n",
      "Episode length: 7.12 +/- 3.13\n",
      "Eval num_timesteps=80000, episode_reward=-0.52 +/- 1.00\n",
      "Episode length: 7.06 +/- 2.91\n",
      "Eval num_timesteps=90000, episode_reward=-0.49 +/- 0.99\n",
      "Episode length: 7.55 +/- 2.65\n",
      "Eval num_timesteps=100000, episode_reward=-0.42 +/- 0.92\n",
      "Episode length: 7.38 +/- 3.16\n",
      "Eval num_timesteps=110000, episode_reward=-0.26 +/- 1.02\n",
      "Episode length: 6.57 +/- 2.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.38 +/- 1.08\n",
      "Episode length: 6.68 +/- 3.33\n",
      "Eval num_timesteps=130000, episode_reward=-0.52 +/- 0.90\n",
      "Episode length: 7.89 +/- 2.79\n",
      "Eval num_timesteps=140000, episode_reward=-0.08 +/- 1.02\n",
      "Episode length: 5.78 +/- 3.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-0.41 +/- 1.08\n",
      "Episode length: 6.80 +/- 3.15\n",
      "Eval num_timesteps=160000, episode_reward=-0.28 +/- 0.98\n",
      "Episode length: 6.72 +/- 3.08\n",
      "Eval num_timesteps=170000, episode_reward=-0.46 +/- 0.97\n",
      "Episode length: 7.29 +/- 3.36\n",
      "Eval num_timesteps=180000, episode_reward=-0.35 +/- 0.91\n",
      "Episode length: 7.27 +/- 2.89\n",
      "Eval num_timesteps=190000, episode_reward=-0.30 +/- 0.97\n",
      "Episode length: 6.92 +/- 2.78\n",
      "Eval num_timesteps=200000, episode_reward=-0.45 +/- 0.97\n",
      "Episode length: 7.23 +/- 3.20\n",
      "Evaluating best model for run 28...\n",
      "Run 28 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6500 (65.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.55\n",
      "  Average Episode Reward: -0.4195\n",
      "  Average Episode Length: 7.11\n",
      "===== COMPLETED DQN RUN 28 ====\n",
      "\n",
      "===== STARTING DQN RUN 29 with SEED 29 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.54 +/- 1.02\n",
      "Episode length: 6.97 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.12 +/- 0.94\n",
      "Episode length: 6.50 +/- 3.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.38 +/- 1.04\n",
      "Episode length: 6.75 +/- 3.25\n",
      "Eval num_timesteps=40000, episode_reward=-0.49 +/- 1.06\n",
      "Episode length: 7.00 +/- 3.35\n",
      "Eval num_timesteps=50000, episode_reward=-0.39 +/- 1.09\n",
      "Episode length: 6.81 +/- 3.18\n",
      "Eval num_timesteps=60000, episode_reward=-0.30 +/- 1.08\n",
      "Episode length: 6.33 +/- 3.16\n",
      "Eval num_timesteps=70000, episode_reward=-0.33 +/- 1.02\n",
      "Episode length: 6.73 +/- 3.20\n",
      "Eval num_timesteps=80000, episode_reward=-0.31 +/- 1.08\n",
      "Episode length: 6.36 +/- 3.16\n",
      "Eval num_timesteps=90000, episode_reward=-0.27 +/- 1.03\n",
      "Episode length: 6.53 +/- 3.22\n",
      "Eval num_timesteps=100000, episode_reward=-0.25 +/- 0.97\n",
      "Episode length: 6.74 +/- 2.82\n",
      "Eval num_timesteps=110000, episode_reward=-0.33 +/- 1.02\n",
      "Episode length: 6.69 +/- 3.06\n",
      "Eval num_timesteps=120000, episode_reward=-0.49 +/- 0.95\n",
      "Episode length: 7.24 +/- 3.31\n",
      "Eval num_timesteps=130000, episode_reward=-0.29 +/- 0.96\n",
      "Episode length: 6.80 +/- 3.17\n",
      "Eval num_timesteps=140000, episode_reward=-0.44 +/- 1.05\n",
      "Episode length: 7.04 +/- 3.11\n",
      "Eval num_timesteps=150000, episode_reward=-0.30 +/- 0.97\n",
      "Episode length: 6.80 +/- 2.99\n",
      "Eval num_timesteps=160000, episode_reward=-0.55 +/- 1.01\n",
      "Episode length: 7.46 +/- 3.12\n",
      "Eval num_timesteps=170000, episode_reward=-0.43 +/- 1.01\n",
      "Episode length: 7.12 +/- 3.01\n",
      "Eval num_timesteps=180000, episode_reward=-0.52 +/- 1.01\n",
      "Episode length: 7.36 +/- 3.14\n",
      "Eval num_timesteps=190000, episode_reward=-0.37 +/- 1.04\n",
      "Episode length: 6.88 +/- 3.10\n",
      "Eval num_timesteps=200000, episode_reward=-0.24 +/- 0.99\n",
      "Episode length: 6.58 +/- 3.24\n",
      "Evaluating best model for run 29...\n",
      "Run 29 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5500 (55.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.85\n",
      "  Average Episode Reward: -0.6365\n",
      "  Average Episode Length: 7.72\n",
      "===== COMPLETED DQN RUN 29 ====\n",
      "\n",
      "===== STARTING DQN RUN 30 with SEED 30 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.61 +/- 1.07\n",
      "Episode length: 7.51 +/- 3.12\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.50 +/- 0.97\n",
      "Episode length: 7.70 +/- 2.73\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.40 +/- 1.01\n",
      "Episode length: 7.00 +/- 2.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.71 +/- 1.04\n",
      "Episode length: 7.71 +/- 2.83\n",
      "Eval num_timesteps=50000, episode_reward=-0.48 +/- 1.01\n",
      "Episode length: 7.21 +/- 3.33\n",
      "Eval num_timesteps=60000, episode_reward=-0.41 +/- 0.98\n",
      "Episode length: 7.08 +/- 3.21\n",
      "Eval num_timesteps=70000, episode_reward=-0.39 +/- 1.03\n",
      "Episode length: 6.73 +/- 3.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.41 +/- 0.96\n",
      "Episode length: 7.26 +/- 2.82\n",
      "Eval num_timesteps=90000, episode_reward=-0.43 +/- 1.01\n",
      "Episode length: 6.90 +/- 3.27\n",
      "Eval num_timesteps=100000, episode_reward=-0.32 +/- 0.96\n",
      "Episode length: 6.97 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-0.53 +/- 0.99\n",
      "Episode length: 7.48 +/- 3.04\n",
      "Eval num_timesteps=120000, episode_reward=-0.31 +/- 1.00\n",
      "Episode length: 6.70 +/- 3.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.68 +/- 1.03\n",
      "Episode length: 7.72 +/- 2.87\n",
      "Eval num_timesteps=140000, episode_reward=-0.40 +/- 1.00\n",
      "Episode length: 7.07 +/- 3.07\n",
      "Eval num_timesteps=150000, episode_reward=-0.39 +/- 1.08\n",
      "Episode length: 6.80 +/- 3.25\n",
      "Eval num_timesteps=160000, episode_reward=-0.45 +/- 0.95\n",
      "Episode length: 7.42 +/- 3.00\n",
      "Eval num_timesteps=170000, episode_reward=-0.44 +/- 0.96\n",
      "Episode length: 7.35 +/- 3.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.57 +/- 0.91\n",
      "Episode length: 7.90 +/- 2.71\n",
      "Eval num_timesteps=190000, episode_reward=-0.61 +/- 0.90\n",
      "Episode length: 8.01 +/- 2.85\n",
      "Eval num_timesteps=200000, episode_reward=-0.62 +/- 0.93\n",
      "Episode length: 7.96 +/- 2.88\n",
      "Evaluating best model for run 30...\n",
      "Run 30 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5700 (57.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.37\n",
      "  Average Episode Reward: -0.3520\n",
      "  Average Episode Length: 6.79\n",
      "===== COMPLETED DQN RUN 30 ====\n",
      "\n",
      "===== STARTING DQN RUN 31 with SEED 31 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.46 +/- 1.00\n",
      "Episode length: 7.26 +/- 3.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.48 +/- 0.99\n",
      "Episode length: 7.34 +/- 3.38\n",
      "Eval num_timesteps=30000, episode_reward=-0.54 +/- 1.01\n",
      "Episode length: 7.37 +/- 2.91\n",
      "Eval num_timesteps=40000, episode_reward=-0.60 +/- 1.02\n",
      "Episode length: 7.53 +/- 3.26\n",
      "Eval num_timesteps=50000, episode_reward=-0.55 +/- 0.90\n",
      "Episode length: 8.10 +/- 2.58\n",
      "Eval num_timesteps=60000, episode_reward=-0.37 +/- 0.97\n",
      "Episode length: 6.98 +/- 3.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.52 +/- 0.95\n",
      "Episode length: 7.75 +/- 3.17\n",
      "Eval num_timesteps=80000, episode_reward=-0.50 +/- 0.97\n",
      "Episode length: 7.35 +/- 3.36\n",
      "Eval num_timesteps=90000, episode_reward=-0.34 +/- 0.99\n",
      "Episode length: 6.86 +/- 3.38\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.51 +/- 1.01\n",
      "Episode length: 7.10 +/- 3.24\n",
      "Eval num_timesteps=110000, episode_reward=-0.46 +/- 0.95\n",
      "Episode length: 7.41 +/- 3.11\n",
      "Eval num_timesteps=120000, episode_reward=-0.51 +/- 0.97\n",
      "Episode length: 7.52 +/- 3.20\n",
      "Eval num_timesteps=130000, episode_reward=-0.49 +/- 1.09\n",
      "Episode length: 6.86 +/- 3.18\n",
      "Eval num_timesteps=140000, episode_reward=-0.47 +/- 1.05\n",
      "Episode length: 7.09 +/- 3.36\n",
      "Eval num_timesteps=150000, episode_reward=-0.65 +/- 0.95\n",
      "Episode length: 7.97 +/- 2.78\n",
      "Eval num_timesteps=160000, episode_reward=-0.29 +/- 0.88\n",
      "Episode length: 7.29 +/- 2.66\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=-0.56 +/- 0.95\n",
      "Episode length: 7.69 +/- 2.90\n",
      "Eval num_timesteps=180000, episode_reward=-0.44 +/- 0.96\n",
      "Episode length: 7.28 +/- 3.30\n",
      "Eval num_timesteps=190000, episode_reward=-0.48 +/- 0.99\n",
      "Episode length: 7.38 +/- 3.07\n",
      "Eval num_timesteps=200000, episode_reward=-0.42 +/- 0.95\n",
      "Episode length: 7.25 +/- 3.31\n",
      "Evaluating best model for run 31...\n",
      "Run 31 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5500 (55.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.98\n",
      "  Average Episode Reward: -0.3895\n",
      "  Average Episode Length: 7.24\n",
      "===== COMPLETED DQN RUN 31 ====\n",
      "\n",
      "===== STARTING DQN RUN 32 with SEED 32 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.48 +/- 1.17\n",
      "Episode length: 6.59 +/- 3.32\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.44 +/- 1.06\n",
      "Episode length: 6.96 +/- 2.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.52 +/- 1.06\n",
      "Episode length: 7.16 +/- 3.07\n",
      "Eval num_timesteps=40000, episode_reward=-0.40 +/- 1.01\n",
      "Episode length: 6.96 +/- 2.94\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.50 +/- 0.97\n",
      "Episode length: 7.52 +/- 3.19\n",
      "Eval num_timesteps=60000, episode_reward=-0.48 +/- 0.97\n",
      "Episode length: 7.52 +/- 3.02\n",
      "Eval num_timesteps=70000, episode_reward=-0.48 +/- 1.03\n",
      "Episode length: 7.00 +/- 3.24\n",
      "Eval num_timesteps=80000, episode_reward=-0.49 +/- 1.03\n",
      "Episode length: 7.26 +/- 3.09\n",
      "Eval num_timesteps=90000, episode_reward=-0.25 +/- 1.07\n",
      "Episode length: 6.42 +/- 3.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.58 +/- 1.08\n",
      "Episode length: 7.22 +/- 3.12\n",
      "Eval num_timesteps=110000, episode_reward=-0.56 +/- 0.94\n",
      "Episode length: 7.56 +/- 3.32\n",
      "Eval num_timesteps=120000, episode_reward=-0.31 +/- 1.04\n",
      "Episode length: 6.63 +/- 3.27\n",
      "Eval num_timesteps=130000, episode_reward=-0.50 +/- 0.94\n",
      "Episode length: 7.41 +/- 3.30\n",
      "Eval num_timesteps=140000, episode_reward=-0.49 +/- 1.09\n",
      "Episode length: 6.90 +/- 3.09\n",
      "Eval num_timesteps=150000, episode_reward=-0.48 +/- 1.00\n",
      "Episode length: 7.38 +/- 3.17\n",
      "Eval num_timesteps=160000, episode_reward=-0.45 +/- 0.96\n",
      "Episode length: 7.48 +/- 2.90\n",
      "Eval num_timesteps=170000, episode_reward=-0.37 +/- 1.01\n",
      "Episode length: 7.04 +/- 3.07\n",
      "Eval num_timesteps=180000, episode_reward=-0.22 +/- 0.99\n",
      "Episode length: 6.52 +/- 3.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=190000, episode_reward=-0.46 +/- 1.02\n",
      "Episode length: 7.18 +/- 3.13\n",
      "Eval num_timesteps=200000, episode_reward=-0.48 +/- 1.05\n",
      "Episode length: 7.15 +/- 3.18\n",
      "Evaluating best model for run 32...\n",
      "Run 32 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5000 (50.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.36\n",
      "  Average Episode Reward: -0.5200\n",
      "  Average Episode Length: 7.18\n",
      "===== COMPLETED DQN RUN 32 ====\n",
      "\n",
      "===== STARTING DQN RUN 33 with SEED 33 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.49 +/- 1.12\n",
      "Episode length: 6.87 +/- 3.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.26 +/- 1.04\n",
      "Episode length: 6.44 +/- 3.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.48 +/- 1.04\n",
      "Episode length: 7.14 +/- 3.13\n",
      "Eval num_timesteps=40000, episode_reward=-0.47 +/- 1.00\n",
      "Episode length: 7.18 +/- 3.02\n",
      "Eval num_timesteps=50000, episode_reward=-0.42 +/- 1.07\n",
      "Episode length: 6.65 +/- 3.29\n",
      "Eval num_timesteps=60000, episode_reward=-0.21 +/- 0.99\n",
      "Episode length: 6.11 +/- 3.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.42 +/- 1.05\n",
      "Episode length: 6.91 +/- 3.01\n",
      "Eval num_timesteps=80000, episode_reward=-0.60 +/- 0.98\n",
      "Episode length: 7.61 +/- 3.06\n",
      "Eval num_timesteps=90000, episode_reward=-0.40 +/- 1.02\n",
      "Episode length: 7.02 +/- 3.12\n",
      "Eval num_timesteps=100000, episode_reward=-0.48 +/- 1.00\n",
      "Episode length: 7.35 +/- 3.01\n",
      "Eval num_timesteps=110000, episode_reward=-0.51 +/- 1.07\n",
      "Episode length: 6.93 +/- 3.10\n",
      "Eval num_timesteps=120000, episode_reward=-0.37 +/- 1.03\n",
      "Episode length: 6.98 +/- 3.03\n",
      "Eval num_timesteps=130000, episode_reward=-0.45 +/- 1.08\n",
      "Episode length: 7.00 +/- 3.51\n",
      "Eval num_timesteps=140000, episode_reward=-0.37 +/- 0.99\n",
      "Episode length: 7.20 +/- 2.81\n",
      "Eval num_timesteps=150000, episode_reward=-0.36 +/- 1.04\n",
      "Episode length: 6.85 +/- 2.92\n",
      "Eval num_timesteps=160000, episode_reward=-0.56 +/- 1.03\n",
      "Episode length: 7.31 +/- 3.45\n",
      "Eval num_timesteps=170000, episode_reward=-0.32 +/- 0.88\n",
      "Episode length: 7.48 +/- 2.66\n",
      "Eval num_timesteps=180000, episode_reward=-0.35 +/- 1.03\n",
      "Episode length: 6.95 +/- 2.92\n",
      "Eval num_timesteps=190000, episode_reward=-0.30 +/- 0.96\n",
      "Episode length: 6.98 +/- 3.01\n",
      "Eval num_timesteps=200000, episode_reward=-0.43 +/- 0.95\n",
      "Episode length: 7.37 +/- 2.95\n",
      "Evaluating best model for run 33...\n",
      "Run 33 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6200 (62.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.19\n",
      "  Average Episode Reward: -0.4630\n",
      "  Average Episode Length: 7.02\n",
      "===== COMPLETED DQN RUN 33 ====\n",
      "\n",
      "===== STARTING DQN RUN 34 with SEED 34 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.20 +/- 1.02\n",
      "Episode length: 6.32 +/- 3.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.59 +/- 1.04\n",
      "Episode length: 7.51 +/- 2.90\n",
      "Eval num_timesteps=30000, episode_reward=-0.56 +/- 1.04\n",
      "Episode length: 7.37 +/- 3.08\n",
      "Eval num_timesteps=40000, episode_reward=-0.52 +/- 0.98\n",
      "Episode length: 7.34 +/- 2.90\n",
      "Eval num_timesteps=50000, episode_reward=-0.52 +/- 0.94\n",
      "Episode length: 7.65 +/- 2.76\n",
      "Eval num_timesteps=60000, episode_reward=-0.56 +/- 1.14\n",
      "Episode length: 7.07 +/- 2.98\n",
      "Eval num_timesteps=70000, episode_reward=-0.41 +/- 0.98\n",
      "Episode length: 6.97 +/- 3.16\n",
      "Eval num_timesteps=80000, episode_reward=-0.34 +/- 0.98\n",
      "Episode length: 6.93 +/- 3.02\n",
      "Eval num_timesteps=90000, episode_reward=-0.45 +/- 0.97\n",
      "Episode length: 7.37 +/- 2.73\n",
      "Eval num_timesteps=100000, episode_reward=-0.32 +/- 1.03\n",
      "Episode length: 6.78 +/- 3.25\n",
      "Eval num_timesteps=110000, episode_reward=-0.39 +/- 1.02\n",
      "Episode length: 7.04 +/- 2.99\n",
      "Eval num_timesteps=120000, episode_reward=-0.31 +/- 0.87\n",
      "Episode length: 7.23 +/- 3.01\n",
      "Eval num_timesteps=130000, episode_reward=-0.49 +/- 1.00\n",
      "Episode length: 7.34 +/- 3.11\n",
      "Eval num_timesteps=140000, episode_reward=-0.65 +/- 0.99\n",
      "Episode length: 7.81 +/- 2.87\n",
      "Eval num_timesteps=150000, episode_reward=-0.47 +/- 0.97\n",
      "Episode length: 7.37 +/- 3.32\n",
      "Eval num_timesteps=160000, episode_reward=-0.31 +/- 1.01\n",
      "Episode length: 6.71 +/- 3.26\n",
      "Eval num_timesteps=170000, episode_reward=-0.41 +/- 0.98\n",
      "Episode length: 7.26 +/- 2.76\n",
      "Eval num_timesteps=180000, episode_reward=-0.30 +/- 0.88\n",
      "Episode length: 7.17 +/- 2.86\n",
      "Eval num_timesteps=190000, episode_reward=-0.44 +/- 0.99\n",
      "Episode length: 7.41 +/- 3.15\n",
      "Eval num_timesteps=200000, episode_reward=-0.51 +/- 0.94\n",
      "Episode length: 7.71 +/- 2.67\n",
      "Evaluating best model for run 34...\n",
      "Run 34 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6800 (68.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.62\n",
      "  Average Episode Reward: -0.2360\n",
      "  Average Episode Length: 6.34\n",
      "===== COMPLETED DQN RUN 34 ====\n",
      "\n",
      "===== STARTING DQN RUN 35 with SEED 35 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.50 +/- 1.14\n",
      "Episode length: 6.77 +/- 3.35\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.33 +/- 1.03\n",
      "Episode length: 6.83 +/- 2.82\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.70 +/- 0.97\n",
      "Episode length: 8.00 +/- 2.70\n",
      "Eval num_timesteps=40000, episode_reward=-0.54 +/- 1.11\n",
      "Episode length: 6.81 +/- 3.17\n",
      "Eval num_timesteps=50000, episode_reward=-0.53 +/- 0.97\n",
      "Episode length: 7.37 +/- 3.02\n",
      "Eval num_timesteps=60000, episode_reward=-0.24 +/- 1.05\n",
      "Episode length: 6.20 +/- 3.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.56 +/- 1.03\n",
      "Episode length: 7.38 +/- 3.32\n",
      "Eval num_timesteps=80000, episode_reward=-0.46 +/- 0.96\n",
      "Episode length: 7.08 +/- 3.10\n",
      "Eval num_timesteps=90000, episode_reward=-0.56 +/- 0.96\n",
      "Episode length: 7.52 +/- 3.01\n",
      "Eval num_timesteps=100000, episode_reward=-0.33 +/- 1.10\n",
      "Episode length: 6.32 +/- 3.27\n",
      "Eval num_timesteps=110000, episode_reward=-0.65 +/- 1.01\n",
      "Episode length: 7.83 +/- 2.80\n",
      "Eval num_timesteps=120000, episode_reward=-0.48 +/- 1.07\n",
      "Episode length: 6.92 +/- 3.48\n",
      "Eval num_timesteps=130000, episode_reward=-0.36 +/- 1.07\n",
      "Episode length: 6.47 +/- 3.28\n",
      "Eval num_timesteps=140000, episode_reward=-0.17 +/- 0.97\n",
      "Episode length: 6.18 +/- 3.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-0.25 +/- 1.03\n",
      "Episode length: 6.35 +/- 3.55\n",
      "Eval num_timesteps=160000, episode_reward=-0.48 +/- 0.99\n",
      "Episode length: 7.47 +/- 2.86\n",
      "Eval num_timesteps=170000, episode_reward=-0.41 +/- 1.05\n",
      "Episode length: 6.87 +/- 3.45\n",
      "Eval num_timesteps=180000, episode_reward=-0.38 +/- 1.00\n",
      "Episode length: 6.98 +/- 3.13\n",
      "Eval num_timesteps=190000, episode_reward=-0.39 +/- 1.05\n",
      "Episode length: 6.89 +/- 3.13\n",
      "Eval num_timesteps=200000, episode_reward=-0.58 +/- 1.10\n",
      "Episode length: 7.26 +/- 3.08\n",
      "Evaluating best model for run 35...\n",
      "Run 35 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6100 (61.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.11\n",
      "  Average Episode Reward: -0.2445\n",
      "  Average Episode Length: 6.41\n",
      "===== COMPLETED DQN RUN 35 ====\n",
      "\n",
      "===== STARTING DQN RUN 36 with SEED 36 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.53 +/- 1.06\n",
      "Episode length: 7.10 +/- 3.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.39 +/- 1.03\n",
      "Episode length: 7.10 +/- 2.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.39 +/- 1.10\n",
      "Episode length: 6.51 +/- 3.45\n",
      "Eval num_timesteps=40000, episode_reward=-0.53 +/- 1.02\n",
      "Episode length: 7.37 +/- 2.88\n",
      "Eval num_timesteps=50000, episode_reward=-0.37 +/- 1.02\n",
      "Episode length: 6.66 +/- 3.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.34 +/- 0.99\n",
      "Episode length: 6.85 +/- 3.43\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.56 +/- 1.02\n",
      "Episode length: 7.48 +/- 2.93\n",
      "Eval num_timesteps=80000, episode_reward=-0.49 +/- 0.95\n",
      "Episode length: 7.44 +/- 3.02\n",
      "Eval num_timesteps=90000, episode_reward=-0.37 +/- 1.06\n",
      "Episode length: 6.69 +/- 3.04\n",
      "Eval num_timesteps=100000, episode_reward=-0.62 +/- 1.04\n",
      "Episode length: 7.39 +/- 3.12\n",
      "Eval num_timesteps=110000, episode_reward=-0.31 +/- 0.94\n",
      "Episode length: 6.94 +/- 3.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.49 +/- 0.94\n",
      "Episode length: 7.54 +/- 3.02\n",
      "Eval num_timesteps=130000, episode_reward=-0.33 +/- 1.03\n",
      "Episode length: 6.81 +/- 3.25\n",
      "Eval num_timesteps=140000, episode_reward=-0.36 +/- 1.00\n",
      "Episode length: 7.00 +/- 3.21\n",
      "Eval num_timesteps=150000, episode_reward=-0.37 +/- 1.05\n",
      "Episode length: 6.91 +/- 3.20\n",
      "Eval num_timesteps=160000, episode_reward=-0.47 +/- 1.04\n",
      "Episode length: 7.28 +/- 3.28\n",
      "Eval num_timesteps=170000, episode_reward=-0.35 +/- 1.03\n",
      "Episode length: 6.68 +/- 3.36\n",
      "Eval num_timesteps=180000, episode_reward=-0.36 +/- 1.04\n",
      "Episode length: 6.78 +/- 3.34\n",
      "Eval num_timesteps=190000, episode_reward=-0.21 +/- 1.02\n",
      "Episode length: 6.30 +/- 3.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-0.41 +/- 1.04\n",
      "Episode length: 6.94 +/- 3.29\n",
      "Evaluating best model for run 36...\n",
      "Run 36 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5000 (50.00%)\n",
      "  Average Detection Time (Successful Episodes): 3.88\n",
      "  Average Episode Reward: -0.4115\n",
      "  Average Episode Length: 6.94\n",
      "===== COMPLETED DQN RUN 36 ====\n",
      "\n",
      "===== STARTING DQN RUN 37 with SEED 37 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.64 +/- 0.95\n",
      "Episode length: 8.14 +/- 2.66\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.55 +/- 1.15\n",
      "Episode length: 6.86 +/- 3.46\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.45 +/- 0.97\n",
      "Episode length: 7.48 +/- 2.82\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.41 +/- 1.02\n",
      "Episode length: 7.22 +/- 3.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.57 +/- 0.99\n",
      "Episode length: 7.59 +/- 2.77\n",
      "Eval num_timesteps=60000, episode_reward=-0.42 +/- 1.04\n",
      "Episode length: 7.03 +/- 3.08\n",
      "Eval num_timesteps=70000, episode_reward=-0.37 +/- 1.04\n",
      "Episode length: 6.84 +/- 3.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.46 +/- 0.98\n",
      "Episode length: 7.38 +/- 2.93\n",
      "Eval num_timesteps=90000, episode_reward=-0.34 +/- 1.01\n",
      "Episode length: 6.84 +/- 3.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.56 +/- 0.98\n",
      "Episode length: 7.38 +/- 2.68\n",
      "Eval num_timesteps=110000, episode_reward=-0.45 +/- 1.10\n",
      "Episode length: 6.82 +/- 3.23\n",
      "Eval num_timesteps=120000, episode_reward=-0.49 +/- 1.03\n",
      "Episode length: 7.32 +/- 2.87\n",
      "Eval num_timesteps=130000, episode_reward=-0.52 +/- 0.98\n",
      "Episode length: 7.57 +/- 2.91\n",
      "Eval num_timesteps=140000, episode_reward=-0.45 +/- 0.97\n",
      "Episode length: 7.61 +/- 2.47\n",
      "Eval num_timesteps=150000, episode_reward=-0.43 +/- 0.95\n",
      "Episode length: 7.54 +/- 2.85\n",
      "Eval num_timesteps=160000, episode_reward=-0.40 +/- 1.03\n",
      "Episode length: 6.97 +/- 3.02\n",
      "Eval num_timesteps=170000, episode_reward=-0.45 +/- 1.03\n",
      "Episode length: 7.28 +/- 2.99\n",
      "Eval num_timesteps=180000, episode_reward=-0.43 +/- 1.02\n",
      "Episode length: 7.16 +/- 3.01\n",
      "Eval num_timesteps=190000, episode_reward=-0.30 +/- 1.01\n",
      "Episode length: 6.69 +/- 2.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-0.49 +/- 1.04\n",
      "Episode length: 7.26 +/- 3.03\n",
      "Evaluating best model for run 37...\n",
      "Run 37 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5700 (57.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.07\n",
      "  Average Episode Reward: -0.5170\n",
      "  Average Episode Length: 7.19\n",
      "===== COMPLETED DQN RUN 37 ====\n",
      "\n",
      "===== STARTING DQN RUN 38 with SEED 38 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.79 +/- 0.98\n",
      "Episode length: 7.96 +/- 2.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.52 +/- 1.10\n",
      "Episode length: 7.06 +/- 2.96\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.62 +/- 0.98\n",
      "Episode length: 7.75 +/- 2.88\n",
      "Eval num_timesteps=40000, episode_reward=-0.60 +/- 1.02\n",
      "Episode length: 7.39 +/- 2.94\n",
      "Eval num_timesteps=50000, episode_reward=-0.39 +/- 0.96\n",
      "Episode length: 7.22 +/- 2.78\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.37 +/- 0.98\n",
      "Episode length: 7.14 +/- 2.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.46 +/- 1.03\n",
      "Episode length: 7.09 +/- 2.90\n",
      "Eval num_timesteps=80000, episode_reward=-0.48 +/- 0.96\n",
      "Episode length: 7.37 +/- 2.87\n",
      "Eval num_timesteps=90000, episode_reward=-0.55 +/- 0.93\n",
      "Episode length: 7.77 +/- 3.05\n",
      "Eval num_timesteps=100000, episode_reward=-0.26 +/- 0.97\n",
      "Episode length: 6.76 +/- 2.99\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-0.36 +/- 1.00\n",
      "Episode length: 6.93 +/- 3.39\n",
      "Eval num_timesteps=120000, episode_reward=-0.49 +/- 0.87\n",
      "Episode length: 7.85 +/- 2.89\n",
      "Eval num_timesteps=130000, episode_reward=-0.47 +/- 0.92\n",
      "Episode length: 7.62 +/- 3.07\n",
      "Eval num_timesteps=140000, episode_reward=-0.58 +/- 0.98\n",
      "Episode length: 7.66 +/- 3.05\n",
      "Eval num_timesteps=150000, episode_reward=-0.39 +/- 0.96\n",
      "Episode length: 7.13 +/- 2.93\n",
      "Eval num_timesteps=160000, episode_reward=-0.39 +/- 0.98\n",
      "Episode length: 6.95 +/- 3.15\n",
      "Eval num_timesteps=170000, episode_reward=-0.33 +/- 1.00\n",
      "Episode length: 6.88 +/- 2.99\n",
      "Eval num_timesteps=180000, episode_reward=-0.42 +/- 1.00\n",
      "Episode length: 7.08 +/- 3.23\n",
      "Eval num_timesteps=190000, episode_reward=-0.57 +/- 0.99\n",
      "Episode length: 7.70 +/- 2.72\n",
      "Eval num_timesteps=200000, episode_reward=-0.47 +/- 0.97\n",
      "Episode length: 7.54 +/- 2.95\n",
      "Evaluating best model for run 38...\n",
      "Run 38 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5800 (58.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.40\n",
      "  Average Episode Reward: -0.4185\n",
      "  Average Episode Length: 7.33\n",
      "===== COMPLETED DQN RUN 38 ====\n",
      "\n",
      "===== STARTING DQN RUN 39 with SEED 39 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.81 +/- 1.13\n",
      "Episode length: 7.50 +/- 3.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.82 +/- 0.80\n",
      "Episode length: 8.76 +/- 2.52\n",
      "Eval num_timesteps=30000, episode_reward=-0.69 +/- 0.98\n",
      "Episode length: 7.78 +/- 2.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.39 +/- 0.97\n",
      "Episode length: 7.04 +/- 3.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.58 +/- 0.93\n",
      "Episode length: 8.21 +/- 2.73\n",
      "Eval num_timesteps=60000, episode_reward=-0.41 +/- 1.01\n",
      "Episode length: 6.94 +/- 3.27\n",
      "Eval num_timesteps=70000, episode_reward=-0.56 +/- 1.10\n",
      "Episode length: 7.13 +/- 2.89\n",
      "Eval num_timesteps=80000, episode_reward=-0.46 +/- 0.97\n",
      "Episode length: 7.27 +/- 3.30\n",
      "Eval num_timesteps=90000, episode_reward=-0.48 +/- 0.94\n",
      "Episode length: 7.63 +/- 2.62\n",
      "Eval num_timesteps=100000, episode_reward=-0.43 +/- 1.00\n",
      "Episode length: 7.16 +/- 3.30\n",
      "Eval num_timesteps=110000, episode_reward=-0.29 +/- 1.01\n",
      "Episode length: 6.57 +/- 3.31\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.41 +/- 0.93\n",
      "Episode length: 7.33 +/- 3.26\n",
      "Eval num_timesteps=130000, episode_reward=-0.39 +/- 1.00\n",
      "Episode length: 7.02 +/- 3.56\n",
      "Eval num_timesteps=140000, episode_reward=-0.65 +/- 0.93\n",
      "Episode length: 8.12 +/- 2.47\n",
      "Eval num_timesteps=150000, episode_reward=-0.51 +/- 0.96\n",
      "Episode length: 7.50 +/- 3.12\n",
      "Eval num_timesteps=160000, episode_reward=-0.66 +/- 0.88\n",
      "Episode length: 8.18 +/- 2.39\n",
      "Eval num_timesteps=170000, episode_reward=-0.61 +/- 1.02\n",
      "Episode length: 7.62 +/- 2.83\n",
      "Eval num_timesteps=180000, episode_reward=-0.42 +/- 0.98\n",
      "Episode length: 7.38 +/- 2.86\n",
      "Eval num_timesteps=190000, episode_reward=-0.58 +/- 0.97\n",
      "Episode length: 7.69 +/- 2.93\n",
      "Eval num_timesteps=200000, episode_reward=-0.35 +/- 0.98\n",
      "Episode length: 7.06 +/- 2.96\n",
      "Evaluating best model for run 39...\n",
      "Run 39 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6400 (64.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.72\n",
      "  Average Episode Reward: -0.2930\n",
      "  Average Episode Length: 6.62\n",
      "===== COMPLETED DQN RUN 39 ====\n",
      "\n",
      "===== STARTING DQN RUN 40 with SEED 40 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.48 +/- 0.98\n",
      "Episode length: 7.50 +/- 2.90\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.52 +/- 1.18\n",
      "Episode length: 6.72 +/- 3.43\n",
      "Eval num_timesteps=30000, episode_reward=-0.60 +/- 1.09\n",
      "Episode length: 7.24 +/- 3.00\n",
      "Eval num_timesteps=40000, episode_reward=-0.43 +/- 0.96\n",
      "Episode length: 7.09 +/- 3.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.53 +/- 1.12\n",
      "Episode length: 6.95 +/- 3.60\n",
      "Eval num_timesteps=60000, episode_reward=-0.52 +/- 0.96\n",
      "Episode length: 7.39 +/- 3.13\n",
      "Eval num_timesteps=70000, episode_reward=-0.30 +/- 1.05\n",
      "Episode length: 6.43 +/- 3.44\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.48 +/- 1.04\n",
      "Episode length: 6.99 +/- 3.03\n",
      "Eval num_timesteps=90000, episode_reward=-0.36 +/- 1.01\n",
      "Episode length: 6.73 +/- 3.15\n",
      "Eval num_timesteps=100000, episode_reward=-0.25 +/- 1.07\n",
      "Episode length: 6.16 +/- 3.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-0.41 +/- 1.06\n",
      "Episode length: 6.73 +/- 3.30\n",
      "Eval num_timesteps=120000, episode_reward=-0.37 +/- 1.02\n",
      "Episode length: 7.03 +/- 2.94\n",
      "Eval num_timesteps=130000, episode_reward=-0.24 +/- 1.02\n",
      "Episode length: 6.49 +/- 3.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-0.44 +/- 1.06\n",
      "Episode length: 7.09 +/- 2.84\n",
      "Eval num_timesteps=150000, episode_reward=-0.33 +/- 1.04\n",
      "Episode length: 6.74 +/- 3.13\n",
      "Eval num_timesteps=160000, episode_reward=-0.39 +/- 0.96\n",
      "Episode length: 7.32 +/- 3.00\n",
      "Eval num_timesteps=170000, episode_reward=-0.49 +/- 1.05\n",
      "Episode length: 7.09 +/- 3.52\n",
      "Eval num_timesteps=180000, episode_reward=-0.43 +/- 0.95\n",
      "Episode length: 7.24 +/- 2.99\n",
      "Eval num_timesteps=190000, episode_reward=-0.49 +/- 1.01\n",
      "Episode length: 7.43 +/- 2.67\n",
      "Eval num_timesteps=200000, episode_reward=-0.60 +/- 1.00\n",
      "Episode length: 7.58 +/- 3.09\n",
      "Evaluating best model for run 40...\n",
      "Run 40 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6400 (64.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.56\n",
      "  Average Episode Reward: -0.3050\n",
      "  Average Episode Length: 6.52\n",
      "===== COMPLETED DQN RUN 40 ====\n",
      "\n",
      "===== STARTING DQN RUN 41 with SEED 41 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.44 +/- 1.10\n",
      "Episode length: 7.04 +/- 2.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.51 +/- 1.03\n",
      "Episode length: 7.15 +/- 2.91\n",
      "Eval num_timesteps=30000, episode_reward=-0.26 +/- 1.04\n",
      "Episode length: 6.49 +/- 3.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.42 +/- 1.02\n",
      "Episode length: 6.85 +/- 3.02\n",
      "Eval num_timesteps=50000, episode_reward=-0.53 +/- 0.98\n",
      "Episode length: 7.34 +/- 3.09\n",
      "Eval num_timesteps=60000, episode_reward=-0.40 +/- 0.93\n",
      "Episode length: 7.53 +/- 2.79\n",
      "Eval num_timesteps=70000, episode_reward=-0.56 +/- 0.89\n",
      "Episode length: 8.01 +/- 2.62\n",
      "Eval num_timesteps=80000, episode_reward=-0.41 +/- 0.99\n",
      "Episode length: 7.28 +/- 2.98\n",
      "Eval num_timesteps=90000, episode_reward=-0.50 +/- 1.06\n",
      "Episode length: 7.18 +/- 3.45\n",
      "Eval num_timesteps=100000, episode_reward=-0.36 +/- 0.95\n",
      "Episode length: 7.27 +/- 2.94\n",
      "Eval num_timesteps=110000, episode_reward=-0.38 +/- 0.97\n",
      "Episode length: 7.12 +/- 3.08\n",
      "Eval num_timesteps=120000, episode_reward=-0.32 +/- 0.95\n",
      "Episode length: 7.25 +/- 2.92\n",
      "Eval num_timesteps=130000, episode_reward=-0.35 +/- 1.03\n",
      "Episode length: 7.10 +/- 3.02\n",
      "Eval num_timesteps=140000, episode_reward=-0.33 +/- 1.05\n",
      "Episode length: 6.72 +/- 3.48\n",
      "Eval num_timesteps=150000, episode_reward=-0.45 +/- 0.95\n",
      "Episode length: 7.47 +/- 2.98\n",
      "Eval num_timesteps=160000, episode_reward=-0.50 +/- 1.02\n",
      "Episode length: 7.49 +/- 2.81\n",
      "Eval num_timesteps=170000, episode_reward=-0.57 +/- 1.02\n",
      "Episode length: 7.55 +/- 2.97\n",
      "Eval num_timesteps=180000, episode_reward=-0.49 +/- 0.96\n",
      "Episode length: 7.43 +/- 2.91\n",
      "Eval num_timesteps=190000, episode_reward=-0.37 +/- 1.01\n",
      "Episode length: 6.98 +/- 3.04\n",
      "Eval num_timesteps=200000, episode_reward=-0.49 +/- 0.95\n",
      "Episode length: 7.73 +/- 2.53\n",
      "Evaluating best model for run 41...\n",
      "Run 41 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5800 (58.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.52\n",
      "  Average Episode Reward: -0.3705\n",
      "  Average Episode Length: 6.82\n",
      "===== COMPLETED DQN RUN 41 ====\n",
      "\n",
      "===== STARTING DQN RUN 42 with SEED 42 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.60 +/- 1.15\n",
      "Episode length: 6.93 +/- 3.32\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.49 +/- 1.04\n",
      "Episode length: 6.86 +/- 3.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.62 +/- 1.10\n",
      "Episode length: 7.19 +/- 3.25\n",
      "Eval num_timesteps=40000, episode_reward=-0.43 +/- 1.02\n",
      "Episode length: 7.12 +/- 3.18\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.46 +/- 0.95\n",
      "Episode length: 7.40 +/- 3.15\n",
      "Eval num_timesteps=60000, episode_reward=-0.30 +/- 1.04\n",
      "Episode length: 6.47 +/- 3.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.25 +/- 1.02\n",
      "Episode length: 6.42 +/- 3.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.33 +/- 0.97\n",
      "Episode length: 6.85 +/- 3.12\n",
      "Eval num_timesteps=90000, episode_reward=-0.33 +/- 1.10\n",
      "Episode length: 6.43 +/- 3.60\n",
      "Eval num_timesteps=100000, episode_reward=-0.48 +/- 0.99\n",
      "Episode length: 7.43 +/- 2.59\n",
      "Eval num_timesteps=110000, episode_reward=-0.53 +/- 0.98\n",
      "Episode length: 7.62 +/- 2.90\n",
      "Eval num_timesteps=120000, episode_reward=-0.23 +/- 0.95\n",
      "Episode length: 6.96 +/- 2.79\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.38 +/- 0.99\n",
      "Episode length: 6.98 +/- 3.45\n",
      "Eval num_timesteps=140000, episode_reward=-0.48 +/- 0.88\n",
      "Episode length: 8.07 +/- 2.43\n",
      "Eval num_timesteps=150000, episode_reward=-0.47 +/- 1.03\n",
      "Episode length: 7.16 +/- 3.04\n",
      "Eval num_timesteps=160000, episode_reward=-0.38 +/- 0.90\n",
      "Episode length: 7.41 +/- 2.95\n",
      "Eval num_timesteps=170000, episode_reward=-0.24 +/- 1.05\n",
      "Episode length: 6.26 +/- 3.39\n",
      "Eval num_timesteps=180000, episode_reward=-0.50 +/- 1.02\n",
      "Episode length: 7.28 +/- 3.11\n",
      "Eval num_timesteps=190000, episode_reward=-0.58 +/- 0.99\n",
      "Episode length: 7.58 +/- 3.13\n",
      "Eval num_timesteps=200000, episode_reward=-0.41 +/- 0.97\n",
      "Episode length: 7.26 +/- 3.05\n",
      "Evaluating best model for run 42...\n",
      "Run 42 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6000 (60.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.92\n",
      "  Average Episode Reward: -0.4180\n",
      "  Average Episode Length: 7.55\n",
      "===== COMPLETED DQN RUN 42 ====\n",
      "\n",
      "===== STARTING DQN RUN 43 with SEED 43 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.56 +/- 0.92\n",
      "Episode length: 7.73 +/- 2.79\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.43 +/- 0.93\n",
      "Episode length: 7.51 +/- 2.82\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.36 +/- 1.11\n",
      "Episode length: 6.39 +/- 3.67\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.52 +/- 1.00\n",
      "Episode length: 7.39 +/- 3.13\n",
      "Eval num_timesteps=50000, episode_reward=-0.58 +/- 0.96\n",
      "Episode length: 7.66 +/- 3.09\n",
      "Eval num_timesteps=60000, episode_reward=-0.41 +/- 1.06\n",
      "Episode length: 6.82 +/- 3.12\n",
      "Eval num_timesteps=70000, episode_reward=-0.29 +/- 1.06\n",
      "Episode length: 6.29 +/- 3.31\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.45 +/- 1.11\n",
      "Episode length: 6.51 +/- 3.42\n",
      "Eval num_timesteps=90000, episode_reward=-0.42 +/- 1.02\n",
      "Episode length: 6.95 +/- 3.14\n",
      "Eval num_timesteps=100000, episode_reward=-0.33 +/- 0.98\n",
      "Episode length: 6.83 +/- 3.11\n",
      "Eval num_timesteps=110000, episode_reward=-0.48 +/- 0.99\n",
      "Episode length: 7.28 +/- 3.11\n",
      "Eval num_timesteps=120000, episode_reward=-0.42 +/- 0.98\n",
      "Episode length: 7.09 +/- 3.06\n",
      "Eval num_timesteps=130000, episode_reward=-0.55 +/- 1.01\n",
      "Episode length: 7.49 +/- 3.22\n",
      "Eval num_timesteps=140000, episode_reward=-0.51 +/- 1.07\n",
      "Episode length: 7.11 +/- 3.07\n",
      "Eval num_timesteps=150000, episode_reward=-0.40 +/- 1.02\n",
      "Episode length: 7.10 +/- 3.06\n",
      "Eval num_timesteps=160000, episode_reward=-0.57 +/- 0.91\n",
      "Episode length: 7.77 +/- 2.86\n",
      "Eval num_timesteps=170000, episode_reward=-0.62 +/- 0.86\n",
      "Episode length: 8.15 +/- 2.77\n",
      "Eval num_timesteps=180000, episode_reward=-0.37 +/- 1.00\n",
      "Episode length: 6.95 +/- 3.04\n",
      "Eval num_timesteps=190000, episode_reward=-0.46 +/- 1.03\n",
      "Episode length: 7.11 +/- 3.40\n",
      "Eval num_timesteps=200000, episode_reward=-0.44 +/- 0.92\n",
      "Episode length: 7.40 +/- 3.08\n",
      "Evaluating best model for run 43...\n",
      "Run 43 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6500 (65.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.71\n",
      "  Average Episode Reward: -0.3885\n",
      "  Average Episode Length: 6.56\n",
      "===== COMPLETED DQN RUN 43 ====\n",
      "\n",
      "===== STARTING DQN RUN 44 with SEED 44 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.43 +/- 1.06\n",
      "Episode length: 6.86 +/- 3.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.51 +/- 1.03\n",
      "Episode length: 7.11 +/- 3.32\n",
      "Eval num_timesteps=30000, episode_reward=-0.62 +/- 1.02\n",
      "Episode length: 7.29 +/- 3.08\n",
      "Eval num_timesteps=40000, episode_reward=-0.37 +/- 1.03\n",
      "Episode length: 6.72 +/- 3.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.48 +/- 1.04\n",
      "Episode length: 7.08 +/- 3.24\n",
      "Eval num_timesteps=60000, episode_reward=-0.51 +/- 1.02\n",
      "Episode length: 7.32 +/- 3.17\n",
      "Eval num_timesteps=70000, episode_reward=-0.46 +/- 1.02\n",
      "Episode length: 7.15 +/- 3.31\n",
      "Eval num_timesteps=80000, episode_reward=-0.59 +/- 1.12\n",
      "Episode length: 7.10 +/- 3.38\n",
      "Eval num_timesteps=90000, episode_reward=-0.65 +/- 1.07\n",
      "Episode length: 7.65 +/- 2.82\n",
      "Eval num_timesteps=100000, episode_reward=-0.24 +/- 1.02\n",
      "Episode length: 6.19 +/- 3.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-0.50 +/- 1.01\n",
      "Episode length: 7.26 +/- 3.34\n",
      "Eval num_timesteps=120000, episode_reward=-0.48 +/- 1.01\n",
      "Episode length: 7.08 +/- 3.07\n",
      "Eval num_timesteps=130000, episode_reward=-0.69 +/- 0.91\n",
      "Episode length: 7.96 +/- 2.79\n",
      "Eval num_timesteps=140000, episode_reward=-0.42 +/- 1.00\n",
      "Episode length: 6.93 +/- 2.84\n",
      "Eval num_timesteps=150000, episode_reward=-0.40 +/- 1.00\n",
      "Episode length: 7.05 +/- 3.06\n",
      "Eval num_timesteps=160000, episode_reward=-0.64 +/- 1.02\n",
      "Episode length: 7.43 +/- 3.28\n",
      "Eval num_timesteps=170000, episode_reward=-0.17 +/- 0.91\n",
      "Episode length: 6.28 +/- 3.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-0.37 +/- 1.02\n",
      "Episode length: 6.75 +/- 2.82\n",
      "Eval num_timesteps=190000, episode_reward=-0.41 +/- 1.01\n",
      "Episode length: 6.89 +/- 3.12\n",
      "Eval num_timesteps=200000, episode_reward=-0.45 +/- 0.96\n",
      "Episode length: 7.32 +/- 2.99\n",
      "Evaluating best model for run 44...\n",
      "Run 44 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6200 (62.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.35\n",
      "  Average Episode Reward: -0.3645\n",
      "  Average Episode Length: 6.50\n",
      "===== COMPLETED DQN RUN 44 ====\n",
      "\n",
      "===== STARTING DQN RUN 45 with SEED 45 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.47 +/- 1.04\n",
      "Episode length: 7.09 +/- 3.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.35 +/- 1.07\n",
      "Episode length: 6.60 +/- 3.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.27 +/- 1.06\n",
      "Episode length: 6.34 +/- 3.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.48 +/- 1.12\n",
      "Episode length: 6.94 +/- 3.30\n",
      "Eval num_timesteps=50000, episode_reward=-0.53 +/- 0.99\n",
      "Episode length: 7.43 +/- 3.08\n",
      "Eval num_timesteps=60000, episode_reward=-0.42 +/- 1.01\n",
      "Episode length: 7.00 +/- 3.17\n",
      "Eval num_timesteps=70000, episode_reward=-0.33 +/- 1.02\n",
      "Episode length: 6.74 +/- 3.08\n",
      "Eval num_timesteps=80000, episode_reward=-0.24 +/- 1.00\n",
      "Episode length: 6.35 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-0.45 +/- 1.05\n",
      "Episode length: 7.02 +/- 3.05\n",
      "Eval num_timesteps=100000, episode_reward=-0.42 +/- 1.03\n",
      "Episode length: 6.97 +/- 3.26\n",
      "Eval num_timesteps=110000, episode_reward=-0.41 +/- 0.99\n",
      "Episode length: 7.11 +/- 3.31\n",
      "Eval num_timesteps=120000, episode_reward=-0.40 +/- 1.00\n",
      "Episode length: 6.99 +/- 2.81\n",
      "Eval num_timesteps=130000, episode_reward=-0.47 +/- 0.94\n",
      "Episode length: 7.51 +/- 3.07\n",
      "Eval num_timesteps=140000, episode_reward=-0.27 +/- 1.02\n",
      "Episode length: 6.16 +/- 3.28\n",
      "Eval num_timesteps=150000, episode_reward=-0.22 +/- 0.94\n",
      "Episode length: 6.67 +/- 3.10\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-0.30 +/- 1.00\n",
      "Episode length: 6.60 +/- 3.07\n",
      "Eval num_timesteps=170000, episode_reward=-0.48 +/- 0.95\n",
      "Episode length: 7.45 +/- 2.86\n",
      "Eval num_timesteps=180000, episode_reward=-0.23 +/- 1.00\n",
      "Episode length: 6.33 +/- 3.05\n",
      "Eval num_timesteps=190000, episode_reward=-0.48 +/- 1.00\n",
      "Episode length: 7.17 +/- 3.01\n",
      "Eval num_timesteps=200000, episode_reward=-0.29 +/- 0.99\n",
      "Episode length: 6.68 +/- 3.09\n",
      "Evaluating best model for run 45...\n",
      "Run 45 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6300 (63.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.68\n",
      "  Average Episode Reward: -0.4430\n",
      "  Average Episode Length: 7.28\n",
      "===== COMPLETED DQN RUN 45 ====\n",
      "\n",
      "===== STARTING DQN RUN 46 with SEED 46 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.64 +/- 0.93\n",
      "Episode length: 8.06 +/- 2.67\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.55 +/- 1.06\n",
      "Episode length: 7.41 +/- 2.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.48 +/- 1.04\n",
      "Episode length: 7.17 +/- 3.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.48 +/- 0.97\n",
      "Episode length: 7.65 +/- 2.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.57 +/- 1.05\n",
      "Episode length: 7.22 +/- 3.33\n",
      "Eval num_timesteps=60000, episode_reward=-0.36 +/- 0.97\n",
      "Episode length: 7.04 +/- 2.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.42 +/- 1.08\n",
      "Episode length: 6.91 +/- 3.05\n",
      "Eval num_timesteps=80000, episode_reward=-0.50 +/- 0.95\n",
      "Episode length: 7.57 +/- 2.92\n",
      "Eval num_timesteps=90000, episode_reward=-0.29 +/- 1.06\n",
      "Episode length: 6.52 +/- 3.31\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.44 +/- 1.04\n",
      "Episode length: 6.94 +/- 3.20\n",
      "Eval num_timesteps=110000, episode_reward=-0.33 +/- 0.95\n",
      "Episode length: 7.01 +/- 3.12\n",
      "Eval num_timesteps=120000, episode_reward=-0.53 +/- 1.04\n",
      "Episode length: 7.40 +/- 3.14\n",
      "Eval num_timesteps=130000, episode_reward=-0.30 +/- 0.95\n",
      "Episode length: 7.01 +/- 3.00\n",
      "Eval num_timesteps=140000, episode_reward=-0.31 +/- 0.93\n",
      "Episode length: 7.00 +/- 3.10\n",
      "Eval num_timesteps=150000, episode_reward=-0.32 +/- 0.92\n",
      "Episode length: 7.09 +/- 3.12\n",
      "Eval num_timesteps=160000, episode_reward=-0.32 +/- 0.94\n",
      "Episode length: 7.02 +/- 2.95\n",
      "Eval num_timesteps=170000, episode_reward=-0.45 +/- 1.03\n",
      "Episode length: 7.27 +/- 3.11\n",
      "Eval num_timesteps=180000, episode_reward=-0.57 +/- 1.01\n",
      "Episode length: 7.54 +/- 2.99\n",
      "Eval num_timesteps=190000, episode_reward=-0.32 +/- 1.05\n",
      "Episode length: 6.68 +/- 3.16\n",
      "Eval num_timesteps=200000, episode_reward=-0.30 +/- 1.08\n",
      "Episode length: 6.39 +/- 3.17\n",
      "Evaluating best model for run 46...\n",
      "Run 46 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5200 (52.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.04\n",
      "  Average Episode Reward: -0.5540\n",
      "  Average Episode Length: 7.42\n",
      "===== COMPLETED DQN RUN 46 ====\n",
      "\n",
      "===== STARTING DQN RUN 47 with SEED 47 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.48 +/- 1.09\n",
      "Episode length: 7.08 +/- 3.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.75 +/- 0.97\n",
      "Episode length: 8.17 +/- 2.63\n",
      "Eval num_timesteps=30000, episode_reward=-0.29 +/- 0.98\n",
      "Episode length: 6.38 +/- 3.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.49 +/- 1.02\n",
      "Episode length: 7.22 +/- 3.18\n",
      "Eval num_timesteps=50000, episode_reward=-0.42 +/- 1.07\n",
      "Episode length: 6.52 +/- 3.23\n",
      "Eval num_timesteps=60000, episode_reward=-0.32 +/- 1.04\n",
      "Episode length: 6.45 +/- 3.24\n",
      "Eval num_timesteps=70000, episode_reward=-0.76 +/- 1.00\n",
      "Episode length: 7.95 +/- 2.67\n",
      "Eval num_timesteps=80000, episode_reward=-0.36 +/- 0.99\n",
      "Episode length: 6.96 +/- 3.04\n",
      "Eval num_timesteps=90000, episode_reward=-0.50 +/- 1.09\n",
      "Episode length: 6.93 +/- 3.38\n",
      "Eval num_timesteps=100000, episode_reward=-0.55 +/- 1.00\n",
      "Episode length: 7.35 +/- 3.21\n",
      "Eval num_timesteps=110000, episode_reward=-0.37 +/- 1.01\n",
      "Episode length: 6.82 +/- 3.39\n",
      "Eval num_timesteps=120000, episode_reward=-0.49 +/- 1.05\n",
      "Episode length: 6.96 +/- 3.30\n",
      "Eval num_timesteps=130000, episode_reward=-0.41 +/- 1.01\n",
      "Episode length: 6.96 +/- 3.15\n",
      "Eval num_timesteps=140000, episode_reward=-0.17 +/- 0.97\n",
      "Episode length: 6.42 +/- 3.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-0.52 +/- 1.00\n",
      "Episode length: 7.37 +/- 3.05\n",
      "Eval num_timesteps=160000, episode_reward=-0.66 +/- 0.96\n",
      "Episode length: 7.82 +/- 2.73\n",
      "Eval num_timesteps=170000, episode_reward=-0.29 +/- 0.97\n",
      "Episode length: 6.74 +/- 3.08\n",
      "Eval num_timesteps=180000, episode_reward=-0.40 +/- 0.97\n",
      "Episode length: 7.43 +/- 2.87\n",
      "Eval num_timesteps=190000, episode_reward=-0.45 +/- 1.01\n",
      "Episode length: 7.22 +/- 3.13\n",
      "Eval num_timesteps=200000, episode_reward=-0.62 +/- 0.96\n",
      "Episode length: 7.80 +/- 2.95\n",
      "Evaluating best model for run 47...\n",
      "Run 47 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6800 (68.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.06\n",
      "  Average Episode Reward: -0.2410\n",
      "  Average Episode Length: 6.64\n",
      "===== COMPLETED DQN RUN 47 ====\n",
      "\n",
      "===== STARTING DQN RUN 48 with SEED 48 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.25 +/- 1.12\n",
      "Episode length: 6.05 +/- 3.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.50 +/- 1.07\n",
      "Episode length: 7.11 +/- 3.22\n",
      "Eval num_timesteps=30000, episode_reward=-0.48 +/- 0.95\n",
      "Episode length: 7.53 +/- 2.93\n",
      "Eval num_timesteps=40000, episode_reward=-0.34 +/- 1.02\n",
      "Episode length: 6.71 +/- 3.08\n",
      "Eval num_timesteps=50000, episode_reward=-0.38 +/- 1.07\n",
      "Episode length: 6.70 +/- 3.39\n",
      "Eval num_timesteps=60000, episode_reward=-0.64 +/- 1.05\n",
      "Episode length: 7.48 +/- 3.02\n",
      "Eval num_timesteps=70000, episode_reward=-0.28 +/- 0.92\n",
      "Episode length: 7.00 +/- 3.08\n",
      "Eval num_timesteps=80000, episode_reward=-0.57 +/- 1.00\n",
      "Episode length: 7.58 +/- 3.13\n",
      "Eval num_timesteps=90000, episode_reward=-0.46 +/- 1.08\n",
      "Episode length: 6.79 +/- 3.36\n",
      "Eval num_timesteps=100000, episode_reward=-0.49 +/- 1.04\n",
      "Episode length: 7.21 +/- 3.25\n",
      "Eval num_timesteps=110000, episode_reward=-0.51 +/- 0.83\n",
      "Episode length: 7.99 +/- 2.66\n",
      "Eval num_timesteps=120000, episode_reward=-0.39 +/- 0.94\n",
      "Episode length: 7.34 +/- 2.97\n",
      "Eval num_timesteps=130000, episode_reward=-0.61 +/- 0.98\n",
      "Episode length: 7.71 +/- 2.95\n",
      "Eval num_timesteps=140000, episode_reward=-0.19 +/- 1.04\n",
      "Episode length: 6.16 +/- 3.34\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-0.42 +/- 0.92\n",
      "Episode length: 7.32 +/- 3.01\n",
      "Eval num_timesteps=160000, episode_reward=-0.54 +/- 0.99\n",
      "Episode length: 7.56 +/- 3.06\n",
      "Eval num_timesteps=170000, episode_reward=-0.60 +/- 0.95\n",
      "Episode length: 7.82 +/- 3.18\n",
      "Eval num_timesteps=180000, episode_reward=-0.51 +/- 1.00\n",
      "Episode length: 7.37 +/- 3.04\n",
      "Eval num_timesteps=190000, episode_reward=-0.51 +/- 0.96\n",
      "Episode length: 7.53 +/- 3.06\n",
      "Eval num_timesteps=200000, episode_reward=-0.35 +/- 0.94\n",
      "Episode length: 6.96 +/- 2.86\n",
      "Evaluating best model for run 48...\n",
      "Run 48 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5900 (59.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.44\n",
      "  Average Episode Reward: -0.4335\n",
      "  Average Episode Length: 6.72\n",
      "===== COMPLETED DQN RUN 48 ====\n",
      "\n",
      "===== STARTING DQN RUN 49 with SEED 49 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.41 +/- 1.07\n",
      "Episode length: 6.99 +/- 3.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.49 +/- 1.06\n",
      "Episode length: 7.38 +/- 2.77\n",
      "Eval num_timesteps=30000, episode_reward=-0.23 +/- 1.05\n",
      "Episode length: 6.31 +/- 3.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.30 +/- 0.95\n",
      "Episode length: 7.09 +/- 2.99\n",
      "Eval num_timesteps=50000, episode_reward=-0.39 +/- 1.01\n",
      "Episode length: 6.98 +/- 2.95\n",
      "Eval num_timesteps=60000, episode_reward=-0.54 +/- 1.03\n",
      "Episode length: 7.25 +/- 3.22\n",
      "Eval num_timesteps=70000, episode_reward=-0.43 +/- 1.12\n",
      "Episode length: 6.62 +/- 3.45\n",
      "Eval num_timesteps=80000, episode_reward=-0.49 +/- 1.07\n",
      "Episode length: 7.23 +/- 2.98\n",
      "Eval num_timesteps=90000, episode_reward=-0.41 +/- 1.07\n",
      "Episode length: 6.94 +/- 3.03\n",
      "Eval num_timesteps=100000, episode_reward=-0.38 +/- 1.07\n",
      "Episode length: 6.98 +/- 2.90\n",
      "Eval num_timesteps=110000, episode_reward=-0.85 +/- 0.64\n",
      "Episode length: 9.15 +/- 2.12\n",
      "Eval num_timesteps=120000, episode_reward=-0.47 +/- 0.96\n",
      "Episode length: 7.27 +/- 3.25\n",
      "Eval num_timesteps=130000, episode_reward=-0.43 +/- 1.01\n",
      "Episode length: 7.17 +/- 3.16\n",
      "Eval num_timesteps=140000, episode_reward=-0.37 +/- 1.01\n",
      "Episode length: 7.00 +/- 3.13\n",
      "Eval num_timesteps=150000, episode_reward=-0.40 +/- 1.03\n",
      "Episode length: 7.10 +/- 3.29\n",
      "Eval num_timesteps=160000, episode_reward=-0.40 +/- 1.07\n",
      "Episode length: 6.61 +/- 3.36\n",
      "Eval num_timesteps=170000, episode_reward=-0.59 +/- 1.02\n",
      "Episode length: 7.39 +/- 3.14\n",
      "Eval num_timesteps=180000, episode_reward=-0.60 +/- 1.03\n",
      "Episode length: 7.73 +/- 2.78\n",
      "Eval num_timesteps=190000, episode_reward=-0.26 +/- 1.07\n",
      "Episode length: 6.09 +/- 3.36\n",
      "Eval num_timesteps=200000, episode_reward=-0.46 +/- 1.03\n",
      "Episode length: 7.13 +/- 3.28\n",
      "Evaluating best model for run 49...\n",
      "Run 49 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6100 (61.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.84\n",
      "  Average Episode Reward: -0.4065\n",
      "  Average Episode Length: 6.85\n",
      "===== COMPLETED DQN RUN 49 ====\n",
      "\n",
      "===== STARTING DQN RUN 50 with SEED 50 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.48 +/- 1.12\n",
      "Episode length: 6.67 +/- 3.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.38 +/- 1.13\n",
      "Episode length: 6.29 +/- 3.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.42 +/- 1.01\n",
      "Episode length: 7.09 +/- 2.99\n",
      "Eval num_timesteps=40000, episode_reward=-0.57 +/- 0.92\n",
      "Episode length: 7.95 +/- 2.58\n",
      "Eval num_timesteps=50000, episode_reward=-0.45 +/- 0.97\n",
      "Episode length: 7.25 +/- 3.07\n",
      "Eval num_timesteps=60000, episode_reward=-0.32 +/- 1.04\n",
      "Episode length: 6.61 +/- 3.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.45 +/- 1.05\n",
      "Episode length: 7.14 +/- 3.15\n",
      "Eval num_timesteps=80000, episode_reward=-0.55 +/- 1.05\n",
      "Episode length: 7.11 +/- 3.30\n",
      "Eval num_timesteps=90000, episode_reward=-0.33 +/- 1.04\n",
      "Episode length: 6.73 +/- 3.21\n",
      "Eval num_timesteps=100000, episode_reward=-0.43 +/- 1.06\n",
      "Episode length: 6.90 +/- 3.45\n",
      "Eval num_timesteps=110000, episode_reward=-0.47 +/- 0.97\n",
      "Episode length: 7.31 +/- 3.21\n",
      "Eval num_timesteps=120000, episode_reward=-0.50 +/- 0.99\n",
      "Episode length: 7.36 +/- 3.30\n",
      "Eval num_timesteps=130000, episode_reward=-0.22 +/- 0.91\n",
      "Episode length: 6.85 +/- 2.82\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-0.43 +/- 0.95\n",
      "Episode length: 7.42 +/- 2.92\n",
      "Eval num_timesteps=150000, episode_reward=-0.53 +/- 0.90\n",
      "Episode length: 7.87 +/- 2.98\n",
      "Eval num_timesteps=160000, episode_reward=-0.52 +/- 1.02\n",
      "Episode length: 7.50 +/- 2.85\n",
      "Eval num_timesteps=170000, episode_reward=-0.67 +/- 0.93\n",
      "Episode length: 7.92 +/- 3.15\n",
      "Eval num_timesteps=180000, episode_reward=-0.46 +/- 1.01\n",
      "Episode length: 7.11 +/- 3.42\n",
      "Eval num_timesteps=190000, episode_reward=-0.56 +/- 0.95\n",
      "Episode length: 7.78 +/- 3.06\n",
      "Eval num_timesteps=200000, episode_reward=-0.48 +/- 1.07\n",
      "Episode length: 7.12 +/- 3.23\n",
      "Evaluating best model for run 50...\n",
      "Run 50 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.4600 (46.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.35\n",
      "  Average Episode Reward: -0.5295\n",
      "  Average Episode Length: 7.86\n",
      "===== COMPLETED DQN RUN 50 ====\n",
      "\n",
      "===== STARTING DQN RUN 51 with SEED 51 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.52 +/- 1.05\n",
      "Episode length: 7.19 +/- 2.87\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.40 +/- 1.10\n",
      "Episode length: 6.55 +/- 3.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.39 +/- 1.09\n",
      "Episode length: 6.69 +/- 3.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.55 +/- 0.97\n",
      "Episode length: 7.70 +/- 2.77\n",
      "Eval num_timesteps=50000, episode_reward=-0.32 +/- 0.98\n",
      "Episode length: 6.90 +/- 3.12\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.28 +/- 1.03\n",
      "Episode length: 6.54 +/- 2.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.41 +/- 1.08\n",
      "Episode length: 6.93 +/- 3.23\n",
      "Eval num_timesteps=80000, episode_reward=-0.45 +/- 0.99\n",
      "Episode length: 7.40 +/- 2.86\n",
      "Eval num_timesteps=90000, episode_reward=-0.40 +/- 1.12\n",
      "Episode length: 6.51 +/- 3.53\n",
      "Eval num_timesteps=100000, episode_reward=-0.34 +/- 1.13\n",
      "Episode length: 6.28 +/- 3.39\n",
      "Eval num_timesteps=110000, episode_reward=-0.48 +/- 1.04\n",
      "Episode length: 7.09 +/- 3.27\n",
      "Eval num_timesteps=120000, episode_reward=-0.51 +/- 0.93\n",
      "Episode length: 7.69 +/- 2.73\n",
      "Eval num_timesteps=130000, episode_reward=-0.43 +/- 0.94\n",
      "Episode length: 7.36 +/- 3.00\n",
      "Eval num_timesteps=140000, episode_reward=-0.25 +/- 1.01\n",
      "Episode length: 6.55 +/- 3.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-0.50 +/- 1.02\n",
      "Episode length: 7.24 +/- 3.43\n",
      "Eval num_timesteps=160000, episode_reward=-0.36 +/- 1.10\n",
      "Episode length: 6.61 +/- 3.43\n",
      "Eval num_timesteps=170000, episode_reward=-0.30 +/- 0.97\n",
      "Episode length: 6.91 +/- 3.39\n",
      "Eval num_timesteps=180000, episode_reward=-0.42 +/- 1.01\n",
      "Episode length: 7.04 +/- 3.47\n",
      "Eval num_timesteps=190000, episode_reward=-0.44 +/- 0.92\n",
      "Episode length: 7.45 +/- 3.33\n",
      "Eval num_timesteps=200000, episode_reward=-0.57 +/- 0.91\n",
      "Episode length: 7.83 +/- 2.91\n",
      "Evaluating best model for run 51...\n",
      "Run 51 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5600 (56.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.82\n",
      "  Average Episode Reward: -0.4300\n",
      "  Average Episode Length: 7.10\n",
      "===== COMPLETED DQN RUN 51 ====\n",
      "\n",
      "===== STARTING DQN RUN 52 with SEED 52 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.29 +/- 1.03\n",
      "Episode length: 6.35 +/- 3.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.55 +/- 1.03\n",
      "Episode length: 7.31 +/- 3.51\n",
      "Eval num_timesteps=30000, episode_reward=-0.35 +/- 1.11\n",
      "Episode length: 6.41 +/- 3.64\n",
      "Eval num_timesteps=40000, episode_reward=-0.45 +/- 1.00\n",
      "Episode length: 7.17 +/- 3.17\n",
      "Eval num_timesteps=50000, episode_reward=-0.60 +/- 0.91\n",
      "Episode length: 8.14 +/- 2.65\n",
      "Eval num_timesteps=60000, episode_reward=-0.72 +/- 0.92\n",
      "Episode length: 8.34 +/- 2.59\n",
      "Eval num_timesteps=70000, episode_reward=-0.42 +/- 0.99\n",
      "Episode length: 7.35 +/- 2.95\n",
      "Eval num_timesteps=80000, episode_reward=-0.44 +/- 0.98\n",
      "Episode length: 7.48 +/- 3.07\n",
      "Eval num_timesteps=90000, episode_reward=-0.54 +/- 0.98\n",
      "Episode length: 7.59 +/- 2.79\n",
      "Eval num_timesteps=100000, episode_reward=-0.44 +/- 0.97\n",
      "Episode length: 7.66 +/- 2.49\n",
      "Eval num_timesteps=110000, episode_reward=-0.57 +/- 0.96\n",
      "Episode length: 7.73 +/- 3.22\n",
      "Eval num_timesteps=120000, episode_reward=-0.38 +/- 0.99\n",
      "Episode length: 7.08 +/- 2.99\n",
      "Eval num_timesteps=130000, episode_reward=-0.39 +/- 0.99\n",
      "Episode length: 7.27 +/- 2.87\n",
      "Eval num_timesteps=140000, episode_reward=-0.53 +/- 0.97\n",
      "Episode length: 7.69 +/- 3.07\n",
      "Eval num_timesteps=150000, episode_reward=-0.38 +/- 0.89\n",
      "Episode length: 7.49 +/- 2.96\n",
      "Eval num_timesteps=160000, episode_reward=-0.45 +/- 1.00\n",
      "Episode length: 7.32 +/- 3.06\n",
      "Eval num_timesteps=170000, episode_reward=-0.64 +/- 0.97\n",
      "Episode length: 7.84 +/- 2.76\n",
      "Eval num_timesteps=180000, episode_reward=-0.37 +/- 1.03\n",
      "Episode length: 6.92 +/- 3.12\n",
      "Eval num_timesteps=190000, episode_reward=-0.42 +/- 0.95\n",
      "Episode length: 7.28 +/- 3.21\n",
      "Eval num_timesteps=200000, episode_reward=-0.34 +/- 1.00\n",
      "Episode length: 6.97 +/- 3.03\n",
      "Evaluating best model for run 52...\n",
      "Run 52 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5900 (59.00%)\n",
      "  Average Detection Time (Successful Episodes): 3.92\n",
      "  Average Episode Reward: -0.3965\n",
      "  Average Episode Length: 6.41\n",
      "===== COMPLETED DQN RUN 52 ====\n",
      "\n",
      "===== STARTING DQN RUN 53 with SEED 53 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.63 +/- 1.05\n",
      "Episode length: 7.40 +/- 3.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.49 +/- 1.00\n",
      "Episode length: 7.25 +/- 3.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.50 +/- 0.97\n",
      "Episode length: 7.74 +/- 2.73\n",
      "Eval num_timesteps=40000, episode_reward=-0.35 +/- 1.01\n",
      "Episode length: 6.64 +/- 3.12\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.39 +/- 1.05\n",
      "Episode length: 6.76 +/- 3.24\n",
      "Eval num_timesteps=60000, episode_reward=-0.42 +/- 0.99\n",
      "Episode length: 6.98 +/- 3.45\n",
      "Eval num_timesteps=70000, episode_reward=-0.44 +/- 0.98\n",
      "Episode length: 7.19 +/- 2.90\n",
      "Eval num_timesteps=80000, episode_reward=-0.55 +/- 1.05\n",
      "Episode length: 7.15 +/- 3.43\n",
      "Eval num_timesteps=90000, episode_reward=-0.32 +/- 1.03\n",
      "Episode length: 6.60 +/- 3.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.42 +/- 0.93\n",
      "Episode length: 7.19 +/- 3.28\n",
      "Eval num_timesteps=110000, episode_reward=-0.41 +/- 1.01\n",
      "Episode length: 6.87 +/- 3.57\n",
      "Eval num_timesteps=120000, episode_reward=-0.53 +/- 0.96\n",
      "Episode length: 7.39 +/- 3.34\n",
      "Eval num_timesteps=130000, episode_reward=-0.37 +/- 0.99\n",
      "Episode length: 6.95 +/- 3.08\n",
      "Eval num_timesteps=140000, episode_reward=-0.40 +/- 1.00\n",
      "Episode length: 6.97 +/- 3.52\n",
      "Eval num_timesteps=150000, episode_reward=-0.33 +/- 1.01\n",
      "Episode length: 6.72 +/- 3.47\n",
      "Eval num_timesteps=160000, episode_reward=-0.41 +/- 0.98\n",
      "Episode length: 7.05 +/- 3.04\n",
      "Eval num_timesteps=170000, episode_reward=-0.53 +/- 1.04\n",
      "Episode length: 7.20 +/- 3.35\n",
      "Eval num_timesteps=180000, episode_reward=-0.50 +/- 0.97\n",
      "Episode length: 7.43 +/- 3.19\n",
      "Eval num_timesteps=190000, episode_reward=-0.36 +/- 1.05\n",
      "Episode length: 6.56 +/- 3.40\n",
      "Eval num_timesteps=200000, episode_reward=-0.31 +/- 1.08\n",
      "Episode length: 6.37 +/- 3.72\n",
      "New best mean reward!\n",
      "Evaluating best model for run 53...\n",
      "Run 53 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5400 (54.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.30\n",
      "  Average Episode Reward: -0.4445\n",
      "  Average Episode Length: 6.92\n",
      "===== COMPLETED DQN RUN 53 ====\n",
      "\n",
      "===== STARTING DQN RUN 54 with SEED 54 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.68 +/- 1.25\n",
      "Episode length: 6.76 +/- 3.78\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.57 +/- 1.03\n",
      "Episode length: 7.24 +/- 3.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.45 +/- 1.01\n",
      "Episode length: 7.10 +/- 3.18\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.41 +/- 1.04\n",
      "Episode length: 6.77 +/- 3.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.39 +/- 1.03\n",
      "Episode length: 6.96 +/- 3.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.46 +/- 0.95\n",
      "Episode length: 7.42 +/- 3.06\n",
      "Eval num_timesteps=70000, episode_reward=-0.36 +/- 0.99\n",
      "Episode length: 7.02 +/- 3.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.45 +/- 1.00\n",
      "Episode length: 6.94 +/- 3.31\n",
      "Eval num_timesteps=90000, episode_reward=-0.47 +/- 0.93\n",
      "Episode length: 7.52 +/- 3.07\n",
      "Eval num_timesteps=100000, episode_reward=-0.46 +/- 1.05\n",
      "Episode length: 7.05 +/- 3.16\n",
      "Eval num_timesteps=110000, episode_reward=-0.46 +/- 1.00\n",
      "Episode length: 7.35 +/- 2.97\n",
      "Eval num_timesteps=120000, episode_reward=-0.58 +/- 0.96\n",
      "Episode length: 7.68 +/- 2.99\n",
      "Eval num_timesteps=130000, episode_reward=-0.49 +/- 1.05\n",
      "Episode length: 6.89 +/- 3.28\n",
      "Eval num_timesteps=140000, episode_reward=-0.26 +/- 1.00\n",
      "Episode length: 6.53 +/- 3.41\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-0.12 +/- 0.97\n",
      "Episode length: 6.35 +/- 3.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-0.39 +/- 0.99\n",
      "Episode length: 6.86 +/- 3.19\n",
      "Eval num_timesteps=170000, episode_reward=-0.49 +/- 0.94\n",
      "Episode length: 7.69 +/- 2.94\n",
      "Eval num_timesteps=180000, episode_reward=-0.50 +/- 1.04\n",
      "Episode length: 7.10 +/- 3.20\n",
      "Eval num_timesteps=190000, episode_reward=-0.50 +/- 0.99\n",
      "Episode length: 7.33 +/- 3.08\n",
      "Eval num_timesteps=200000, episode_reward=-0.38 +/- 1.00\n",
      "Episode length: 6.85 +/- 3.14\n",
      "Evaluating best model for run 54...\n",
      "Run 54 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6100 (61.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.98\n",
      "  Average Episode Reward: -0.3210\n",
      "  Average Episode Length: 6.94\n",
      "===== COMPLETED DQN RUN 54 ====\n",
      "\n",
      "===== STARTING DQN RUN 55 with SEED 55 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.70 +/- 1.22\n",
      "Episode length: 7.02 +/- 3.64\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.48 +/- 0.93\n",
      "Episode length: 7.64 +/- 3.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.27 +/- 1.01\n",
      "Episode length: 6.54 +/- 3.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.48 +/- 1.08\n",
      "Episode length: 6.89 +/- 3.58\n",
      "Eval num_timesteps=50000, episode_reward=-0.45 +/- 1.06\n",
      "Episode length: 6.75 +/- 3.25\n",
      "Eval num_timesteps=60000, episode_reward=-0.36 +/- 1.06\n",
      "Episode length: 6.41 +/- 3.46\n",
      "Eval num_timesteps=70000, episode_reward=-0.42 +/- 1.06\n",
      "Episode length: 6.76 +/- 3.35\n",
      "Eval num_timesteps=80000, episode_reward=-0.54 +/- 0.97\n",
      "Episode length: 7.61 +/- 3.36\n",
      "Eval num_timesteps=90000, episode_reward=-0.52 +/- 1.13\n",
      "Episode length: 6.84 +/- 3.23\n",
      "Eval num_timesteps=100000, episode_reward=-0.41 +/- 0.98\n",
      "Episode length: 7.18 +/- 3.04\n",
      "Eval num_timesteps=110000, episode_reward=-0.42 +/- 1.02\n",
      "Episode length: 7.06 +/- 3.18\n",
      "Eval num_timesteps=120000, episode_reward=-0.25 +/- 1.06\n",
      "Episode length: 6.31 +/- 3.43\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.48 +/- 1.04\n",
      "Episode length: 7.20 +/- 3.28\n",
      "Eval num_timesteps=140000, episode_reward=-0.27 +/- 1.03\n",
      "Episode length: 6.10 +/- 3.21\n",
      "Eval num_timesteps=150000, episode_reward=-0.39 +/- 1.01\n",
      "Episode length: 6.76 +/- 3.20\n",
      "Eval num_timesteps=160000, episode_reward=-0.46 +/- 0.96\n",
      "Episode length: 7.25 +/- 3.20\n",
      "Eval num_timesteps=170000, episode_reward=-0.55 +/- 0.98\n",
      "Episode length: 7.61 +/- 3.04\n",
      "Eval num_timesteps=180000, episode_reward=-0.60 +/- 0.94\n",
      "Episode length: 7.70 +/- 2.80\n",
      "Eval num_timesteps=190000, episode_reward=-0.45 +/- 0.94\n",
      "Episode length: 7.33 +/- 2.97\n",
      "Eval num_timesteps=200000, episode_reward=-0.50 +/- 0.94\n",
      "Episode length: 7.54 +/- 3.03\n",
      "Evaluating best model for run 55...\n",
      "Run 55 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6000 (60.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.82\n",
      "  Average Episode Reward: -0.3475\n",
      "  Average Episode Length: 6.89\n",
      "===== COMPLETED DQN RUN 55 ====\n",
      "\n",
      "===== STARTING DQN RUN 56 with SEED 56 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.70 +/- 1.12\n",
      "Episode length: 7.30 +/- 3.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.40 +/- 0.90\n",
      "Episode length: 7.61 +/- 2.96\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.31 +/- 1.03\n",
      "Episode length: 6.74 +/- 3.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.58 +/- 1.03\n",
      "Episode length: 7.45 +/- 3.03\n",
      "Eval num_timesteps=50000, episode_reward=-0.35 +/- 1.02\n",
      "Episode length: 6.64 +/- 3.24\n",
      "Eval num_timesteps=60000, episode_reward=-0.60 +/- 1.01\n",
      "Episode length: 7.39 +/- 2.89\n",
      "Eval num_timesteps=70000, episode_reward=-0.43 +/- 0.91\n",
      "Episode length: 7.58 +/- 2.84\n",
      "Eval num_timesteps=80000, episode_reward=-0.51 +/- 1.00\n",
      "Episode length: 7.22 +/- 3.07\n",
      "Eval num_timesteps=90000, episode_reward=-0.42 +/- 0.99\n",
      "Episode length: 6.98 +/- 3.30\n",
      "Eval num_timesteps=100000, episode_reward=-0.47 +/- 0.94\n",
      "Episode length: 7.46 +/- 3.19\n",
      "Eval num_timesteps=110000, episode_reward=-0.42 +/- 0.97\n",
      "Episode length: 7.20 +/- 3.23\n",
      "Eval num_timesteps=120000, episode_reward=-0.50 +/- 1.05\n",
      "Episode length: 7.19 +/- 3.17\n",
      "Eval num_timesteps=130000, episode_reward=-0.36 +/- 1.11\n",
      "Episode length: 6.66 +/- 3.22\n",
      "Eval num_timesteps=140000, episode_reward=-0.43 +/- 1.03\n",
      "Episode length: 6.96 +/- 3.32\n",
      "Eval num_timesteps=150000, episode_reward=-0.55 +/- 0.98\n",
      "Episode length: 7.48 +/- 3.08\n",
      "Eval num_timesteps=160000, episode_reward=-0.66 +/- 0.97\n",
      "Episode length: 7.75 +/- 2.80\n",
      "Eval num_timesteps=170000, episode_reward=-0.43 +/- 1.01\n",
      "Episode length: 6.95 +/- 3.09\n",
      "Eval num_timesteps=180000, episode_reward=-0.55 +/- 0.96\n",
      "Episode length: 7.44 +/- 3.13\n",
      "Eval num_timesteps=190000, episode_reward=-0.51 +/- 0.92\n",
      "Episode length: 7.69 +/- 2.71\n",
      "Eval num_timesteps=200000, episode_reward=-0.47 +/- 1.03\n",
      "Episode length: 7.12 +/- 2.92\n",
      "Evaluating best model for run 56...\n",
      "Run 56 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.4600 (46.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.96\n",
      "  Average Episode Reward: -0.5850\n",
      "  Average Episode Length: 7.68\n",
      "===== COMPLETED DQN RUN 56 ====\n",
      "\n",
      "===== STARTING DQN RUN 57 with SEED 57 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.53 +/- 1.00\n",
      "Episode length: 7.29 +/- 2.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.44 +/- 1.03\n",
      "Episode length: 7.06 +/- 3.12\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.51 +/- 1.05\n",
      "Episode length: 7.03 +/- 3.16\n",
      "Eval num_timesteps=40000, episode_reward=-0.52 +/- 1.01\n",
      "Episode length: 7.33 +/- 2.89\n",
      "Eval num_timesteps=50000, episode_reward=-0.41 +/- 1.00\n",
      "Episode length: 7.10 +/- 3.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.48 +/- 1.08\n",
      "Episode length: 6.98 +/- 3.26\n",
      "Eval num_timesteps=70000, episode_reward=-0.63 +/- 1.07\n",
      "Episode length: 7.34 +/- 3.26\n",
      "Eval num_timesteps=80000, episode_reward=-0.57 +/- 0.94\n",
      "Episode length: 7.71 +/- 3.03\n",
      "Eval num_timesteps=90000, episode_reward=-0.59 +/- 0.96\n",
      "Episode length: 7.81 +/- 2.61\n",
      "Eval num_timesteps=100000, episode_reward=-0.46 +/- 0.99\n",
      "Episode length: 7.17 +/- 3.28\n",
      "Eval num_timesteps=110000, episode_reward=-0.43 +/- 0.98\n",
      "Episode length: 7.18 +/- 3.28\n",
      "Eval num_timesteps=120000, episode_reward=-0.26 +/- 0.98\n",
      "Episode length: 6.81 +/- 3.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.33 +/- 0.98\n",
      "Episode length: 6.90 +/- 3.15\n",
      "Eval num_timesteps=140000, episode_reward=-0.45 +/- 0.95\n",
      "Episode length: 7.38 +/- 3.12\n",
      "Eval num_timesteps=150000, episode_reward=-0.45 +/- 0.94\n",
      "Episode length: 7.59 +/- 2.95\n",
      "Eval num_timesteps=160000, episode_reward=-0.44 +/- 0.93\n",
      "Episode length: 7.54 +/- 2.65\n",
      "Eval num_timesteps=170000, episode_reward=-0.41 +/- 0.90\n",
      "Episode length: 7.58 +/- 2.62\n",
      "Eval num_timesteps=180000, episode_reward=-0.44 +/- 0.93\n",
      "Episode length: 7.54 +/- 2.78\n",
      "Eval num_timesteps=190000, episode_reward=-0.40 +/- 0.94\n",
      "Episode length: 7.61 +/- 2.71\n",
      "Eval num_timesteps=200000, episode_reward=-0.39 +/- 1.03\n",
      "Episode length: 7.00 +/- 3.40\n",
      "Evaluating best model for run 57...\n",
      "Run 57 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5700 (57.00%)\n",
      "  Average Detection Time (Successful Episodes): 6.11\n",
      "  Average Episode Reward: -0.5280\n",
      "  Average Episode Length: 7.78\n",
      "===== COMPLETED DQN RUN 57 ====\n",
      "\n",
      "===== STARTING DQN RUN 58 with SEED 58 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.48 +/- 1.00\n",
      "Episode length: 7.44 +/- 2.90\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.56 +/- 1.09\n",
      "Episode length: 7.12 +/- 3.12\n",
      "Eval num_timesteps=30000, episode_reward=-0.69 +/- 0.85\n",
      "Episode length: 8.60 +/- 2.26\n",
      "Eval num_timesteps=40000, episode_reward=-0.61 +/- 0.90\n",
      "Episode length: 8.01 +/- 2.87\n",
      "Eval num_timesteps=50000, episode_reward=-0.42 +/- 0.94\n",
      "Episode length: 7.40 +/- 2.70\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.43 +/- 0.95\n",
      "Episode length: 7.60 +/- 2.61\n",
      "Eval num_timesteps=70000, episode_reward=-0.37 +/- 0.97\n",
      "Episode length: 7.30 +/- 2.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.33 +/- 0.92\n",
      "Episode length: 6.91 +/- 3.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-0.34 +/- 0.98\n",
      "Episode length: 6.76 +/- 3.01\n",
      "Eval num_timesteps=100000, episode_reward=-0.43 +/- 0.94\n",
      "Episode length: 7.61 +/- 2.55\n",
      "Eval num_timesteps=110000, episode_reward=-0.45 +/- 1.03\n",
      "Episode length: 7.10 +/- 3.07\n",
      "Eval num_timesteps=120000, episode_reward=-0.51 +/- 1.00\n",
      "Episode length: 7.52 +/- 2.85\n",
      "Eval num_timesteps=130000, episode_reward=-0.55 +/- 0.97\n",
      "Episode length: 7.46 +/- 3.22\n",
      "Eval num_timesteps=140000, episode_reward=-0.36 +/- 1.00\n",
      "Episode length: 6.85 +/- 3.07\n",
      "Eval num_timesteps=150000, episode_reward=-0.48 +/- 0.95\n",
      "Episode length: 7.88 +/- 2.36\n",
      "Eval num_timesteps=160000, episode_reward=-0.50 +/- 0.98\n",
      "Episode length: 7.44 +/- 2.85\n",
      "Eval num_timesteps=170000, episode_reward=-0.48 +/- 0.91\n",
      "Episode length: 8.09 +/- 2.34\n",
      "Eval num_timesteps=180000, episode_reward=-0.50 +/- 1.03\n",
      "Episode length: 7.25 +/- 3.14\n",
      "Eval num_timesteps=190000, episode_reward=-0.31 +/- 0.98\n",
      "Episode length: 6.91 +/- 3.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-0.37 +/- 1.04\n",
      "Episode length: 6.91 +/- 3.36\n",
      "Evaluating best model for run 58...\n",
      "Run 58 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6200 (62.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.95\n",
      "  Average Episode Reward: -0.4580\n",
      "  Average Episode Length: 7.49\n",
      "===== COMPLETED DQN RUN 58 ====\n",
      "\n",
      "===== STARTING DQN RUN 59 with SEED 59 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.54 +/- 1.22\n",
      "Episode length: 6.42 +/- 3.53\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.65 +/- 0.90\n",
      "Episode length: 7.95 +/- 3.00\n",
      "Eval num_timesteps=30000, episode_reward=-0.33 +/- 1.10\n",
      "Episode length: 6.32 +/- 3.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.59 +/- 1.04\n",
      "Episode length: 7.43 +/- 3.43\n",
      "Eval num_timesteps=50000, episode_reward=-0.42 +/- 1.10\n",
      "Episode length: 6.75 +/- 3.43\n",
      "Eval num_timesteps=60000, episode_reward=-0.31 +/- 0.99\n",
      "Episode length: 6.65 +/- 3.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.37 +/- 1.02\n",
      "Episode length: 6.92 +/- 3.49\n",
      "Eval num_timesteps=80000, episode_reward=-0.40 +/- 0.99\n",
      "Episode length: 7.17 +/- 3.31\n",
      "Eval num_timesteps=90000, episode_reward=-0.50 +/- 0.97\n",
      "Episode length: 7.52 +/- 2.75\n",
      "Eval num_timesteps=100000, episode_reward=-0.36 +/- 0.98\n",
      "Episode length: 6.92 +/- 3.34\n",
      "Eval num_timesteps=110000, episode_reward=-0.32 +/- 1.02\n",
      "Episode length: 6.83 +/- 3.27\n",
      "Eval num_timesteps=120000, episode_reward=-0.21 +/- 0.87\n",
      "Episode length: 6.98 +/- 3.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.40 +/- 1.09\n",
      "Episode length: 6.75 +/- 3.24\n",
      "Eval num_timesteps=140000, episode_reward=-0.45 +/- 0.96\n",
      "Episode length: 7.48 +/- 2.83\n",
      "Eval num_timesteps=150000, episode_reward=-0.25 +/- 0.98\n",
      "Episode length: 6.79 +/- 3.11\n",
      "Eval num_timesteps=160000, episode_reward=-0.51 +/- 1.02\n",
      "Episode length: 7.40 +/- 3.04\n",
      "Eval num_timesteps=170000, episode_reward=-0.52 +/- 1.01\n",
      "Episode length: 7.43 +/- 3.12\n",
      "Eval num_timesteps=180000, episode_reward=-0.31 +/- 1.03\n",
      "Episode length: 6.80 +/- 3.28\n",
      "Eval num_timesteps=190000, episode_reward=-0.41 +/- 0.94\n",
      "Episode length: 7.29 +/- 2.94\n",
      "Eval num_timesteps=200000, episode_reward=-0.39 +/- 1.02\n",
      "Episode length: 6.92 +/- 3.08\n",
      "Evaluating best model for run 59...\n",
      "Run 59 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5700 (57.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.39\n",
      "  Average Episode Reward: -0.4225\n",
      "  Average Episode Length: 7.37\n",
      "===== COMPLETED DQN RUN 59 ====\n",
      "\n",
      "===== STARTING DQN RUN 60 with SEED 60 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.48 +/- 1.19\n",
      "Episode length: 6.43 +/- 3.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.37 +/- 1.08\n",
      "Episode length: 6.37 +/- 3.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.22 +/- 1.10\n",
      "Episode length: 5.72 +/- 3.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.50 +/- 1.08\n",
      "Episode length: 6.79 +/- 3.49\n",
      "Eval num_timesteps=50000, episode_reward=-0.47 +/- 1.04\n",
      "Episode length: 6.87 +/- 3.35\n",
      "Eval num_timesteps=60000, episode_reward=-0.48 +/- 0.92\n",
      "Episode length: 7.55 +/- 2.73\n",
      "Eval num_timesteps=70000, episode_reward=-0.44 +/- 1.05\n",
      "Episode length: 7.00 +/- 3.09\n",
      "Eval num_timesteps=80000, episode_reward=-0.54 +/- 0.99\n",
      "Episode length: 7.35 +/- 3.21\n",
      "Eval num_timesteps=90000, episode_reward=-0.52 +/- 1.02\n",
      "Episode length: 7.23 +/- 3.16\n",
      "Eval num_timesteps=100000, episode_reward=-0.54 +/- 0.98\n",
      "Episode length: 7.55 +/- 3.34\n",
      "Eval num_timesteps=110000, episode_reward=-0.42 +/- 0.97\n",
      "Episode length: 6.89 +/- 2.88\n",
      "Eval num_timesteps=120000, episode_reward=-0.26 +/- 1.02\n",
      "Episode length: 6.49 +/- 3.64\n",
      "Eval num_timesteps=130000, episode_reward=-0.22 +/- 0.94\n",
      "Episode length: 6.61 +/- 3.22\n",
      "Eval num_timesteps=140000, episode_reward=-0.56 +/- 1.02\n",
      "Episode length: 7.44 +/- 2.72\n",
      "Eval num_timesteps=150000, episode_reward=-0.47 +/- 1.07\n",
      "Episode length: 6.97 +/- 3.10\n",
      "Eval num_timesteps=160000, episode_reward=-0.33 +/- 0.94\n",
      "Episode length: 7.00 +/- 3.26\n",
      "Eval num_timesteps=170000, episode_reward=-0.35 +/- 1.00\n",
      "Episode length: 6.85 +/- 3.05\n",
      "Eval num_timesteps=180000, episode_reward=-0.54 +/- 0.94\n",
      "Episode length: 7.79 +/- 2.86\n",
      "Eval num_timesteps=190000, episode_reward=-0.47 +/- 1.04\n",
      "Episode length: 7.14 +/- 3.17\n",
      "Eval num_timesteps=200000, episode_reward=-0.39 +/- 0.94\n",
      "Episode length: 7.27 +/- 3.08\n",
      "Evaluating best model for run 60...\n",
      "Run 60 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6300 (63.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.29\n",
      "  Average Episode Reward: -0.4170\n",
      "  Average Episode Length: 6.40\n",
      "===== COMPLETED DQN RUN 60 ====\n",
      "\n",
      "===== STARTING DQN RUN 61 with SEED 61 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.75 +/- 1.13\n",
      "Episode length: 7.30 +/- 3.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.67 +/- 1.06\n",
      "Episode length: 7.39 +/- 3.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.51 +/- 1.02\n",
      "Episode length: 7.23 +/- 3.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.45 +/- 0.92\n",
      "Episode length: 7.25 +/- 3.09\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.45 +/- 1.02\n",
      "Episode length: 6.96 +/- 3.17\n",
      "Eval num_timesteps=60000, episode_reward=-0.43 +/- 0.95\n",
      "Episode length: 7.27 +/- 3.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.52 +/- 1.07\n",
      "Episode length: 7.02 +/- 3.49\n",
      "Eval num_timesteps=80000, episode_reward=-0.61 +/- 0.90\n",
      "Episode length: 7.90 +/- 2.75\n",
      "Eval num_timesteps=90000, episode_reward=-0.22 +/- 0.90\n",
      "Episode length: 6.69 +/- 3.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.39 +/- 1.01\n",
      "Episode length: 7.03 +/- 3.37\n",
      "Eval num_timesteps=110000, episode_reward=-0.36 +/- 1.00\n",
      "Episode length: 6.72 +/- 3.19\n",
      "Eval num_timesteps=120000, episode_reward=-0.43 +/- 1.03\n",
      "Episode length: 7.01 +/- 3.10\n",
      "Eval num_timesteps=130000, episode_reward=-0.34 +/- 1.03\n",
      "Episode length: 6.72 +/- 3.16\n",
      "Eval num_timesteps=140000, episode_reward=-0.45 +/- 1.02\n",
      "Episode length: 7.10 +/- 3.24\n",
      "Eval num_timesteps=150000, episode_reward=-0.40 +/- 0.97\n",
      "Episode length: 7.11 +/- 3.05\n",
      "Eval num_timesteps=160000, episode_reward=-0.39 +/- 1.04\n",
      "Episode length: 6.80 +/- 3.14\n",
      "Eval num_timesteps=170000, episode_reward=-0.48 +/- 0.95\n",
      "Episode length: 7.44 +/- 3.15\n",
      "Eval num_timesteps=180000, episode_reward=-0.42 +/- 0.95\n",
      "Episode length: 7.40 +/- 2.88\n",
      "Eval num_timesteps=190000, episode_reward=-0.49 +/- 1.03\n",
      "Episode length: 7.02 +/- 3.30\n",
      "Eval num_timesteps=200000, episode_reward=-0.48 +/- 0.97\n",
      "Episode length: 7.29 +/- 3.04\n",
      "Evaluating best model for run 61...\n",
      "Run 61 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.4700 (47.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.26\n",
      "  Average Episode Reward: -0.4980\n",
      "  Average Episode Length: 7.30\n",
      "===== COMPLETED DQN RUN 61 ====\n",
      "\n",
      "===== STARTING DQN RUN 62 with SEED 62 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.62 +/- 1.11\n",
      "Episode length: 7.21 +/- 3.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.54 +/- 1.17\n",
      "Episode length: 6.59 +/- 3.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.46 +/- 0.97\n",
      "Episode length: 7.32 +/- 2.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.34 +/- 0.99\n",
      "Episode length: 6.84 +/- 2.99\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.55 +/- 1.11\n",
      "Episode length: 6.92 +/- 3.14\n",
      "Eval num_timesteps=60000, episode_reward=-0.49 +/- 1.06\n",
      "Episode length: 7.06 +/- 3.28\n",
      "Eval num_timesteps=70000, episode_reward=-0.53 +/- 1.01\n",
      "Episode length: 7.40 +/- 3.15\n",
      "Eval num_timesteps=80000, episode_reward=-0.51 +/- 1.08\n",
      "Episode length: 6.95 +/- 3.39\n",
      "Eval num_timesteps=90000, episode_reward=-0.35 +/- 1.02\n",
      "Episode length: 6.83 +/- 2.93\n",
      "Eval num_timesteps=100000, episode_reward=-0.62 +/- 0.99\n",
      "Episode length: 7.79 +/- 2.77\n",
      "Eval num_timesteps=110000, episode_reward=-0.37 +/- 1.05\n",
      "Episode length: 6.77 +/- 3.15\n",
      "Eval num_timesteps=120000, episode_reward=-0.52 +/- 0.98\n",
      "Episode length: 7.55 +/- 2.87\n",
      "Eval num_timesteps=130000, episode_reward=-0.34 +/- 1.06\n",
      "Episode length: 6.65 +/- 3.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-0.41 +/- 1.01\n",
      "Episode length: 6.57 +/- 3.26\n",
      "Eval num_timesteps=150000, episode_reward=-0.40 +/- 1.02\n",
      "Episode length: 6.97 +/- 3.27\n",
      "Eval num_timesteps=160000, episode_reward=-0.37 +/- 1.05\n",
      "Episode length: 6.84 +/- 3.12\n",
      "Eval num_timesteps=170000, episode_reward=-0.27 +/- 1.05\n",
      "Episode length: 6.46 +/- 3.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-0.50 +/- 1.02\n",
      "Episode length: 7.39 +/- 2.94\n",
      "Eval num_timesteps=190000, episode_reward=-0.41 +/- 0.95\n",
      "Episode length: 7.30 +/- 3.06\n",
      "Eval num_timesteps=200000, episode_reward=-0.53 +/- 0.92\n",
      "Episode length: 7.67 +/- 2.73\n",
      "Evaluating best model for run 62...\n",
      "Run 62 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5100 (51.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.18\n",
      "  Average Episode Reward: -0.4880\n",
      "  Average Episode Length: 7.03\n",
      "===== COMPLETED DQN RUN 62 ====\n",
      "\n",
      "===== STARTING DQN RUN 63 with SEED 63 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.41 +/- 1.05\n",
      "Episode length: 6.88 +/- 3.34\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.50 +/- 1.00\n",
      "Episode length: 7.37 +/- 2.98\n",
      "Eval num_timesteps=30000, episode_reward=-0.37 +/- 1.06\n",
      "Episode length: 6.77 +/- 3.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.50 +/- 1.02\n",
      "Episode length: 7.27 +/- 3.32\n",
      "Eval num_timesteps=50000, episode_reward=-0.57 +/- 1.04\n",
      "Episode length: 7.41 +/- 3.01\n",
      "Eval num_timesteps=60000, episode_reward=-0.52 +/- 0.97\n",
      "Episode length: 7.66 +/- 3.04\n",
      "Eval num_timesteps=70000, episode_reward=-0.52 +/- 0.94\n",
      "Episode length: 7.66 +/- 2.81\n",
      "Eval num_timesteps=80000, episode_reward=-0.44 +/- 0.97\n",
      "Episode length: 7.26 +/- 3.07\n",
      "Eval num_timesteps=90000, episode_reward=-0.23 +/- 0.96\n",
      "Episode length: 6.67 +/- 3.34\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.44 +/- 0.95\n",
      "Episode length: 7.22 +/- 3.38\n",
      "Eval num_timesteps=110000, episode_reward=-0.50 +/- 0.95\n",
      "Episode length: 7.43 +/- 3.12\n",
      "Eval num_timesteps=120000, episode_reward=-0.79 +/- 0.91\n",
      "Episode length: 8.30 +/- 2.91\n",
      "Eval num_timesteps=130000, episode_reward=-0.43 +/- 1.03\n",
      "Episode length: 7.11 +/- 3.44\n",
      "Eval num_timesteps=140000, episode_reward=-0.46 +/- 1.05\n",
      "Episode length: 7.09 +/- 3.16\n",
      "Eval num_timesteps=150000, episode_reward=-0.35 +/- 0.95\n",
      "Episode length: 7.13 +/- 2.88\n",
      "Eval num_timesteps=160000, episode_reward=-0.27 +/- 0.93\n",
      "Episode length: 6.93 +/- 3.06\n",
      "Eval num_timesteps=170000, episode_reward=-1.03 +/- 0.86\n",
      "Episode length: 9.00 +/- 1.96\n",
      "Eval num_timesteps=180000, episode_reward=-0.42 +/- 1.04\n",
      "Episode length: 7.03 +/- 3.18\n",
      "Eval num_timesteps=190000, episode_reward=-0.42 +/- 1.05\n",
      "Episode length: 6.84 +/- 3.26\n",
      "Eval num_timesteps=200000, episode_reward=-0.53 +/- 0.99\n",
      "Episode length: 7.57 +/- 3.09\n",
      "Evaluating best model for run 63...\n",
      "Run 63 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.4800 (48.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.73\n",
      "  Average Episode Reward: -0.4975\n",
      "  Average Episode Length: 7.47\n",
      "===== COMPLETED DQN RUN 63 ====\n",
      "\n",
      "===== STARTING DQN RUN 64 with SEED 64 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.47 +/- 1.04\n",
      "Episode length: 6.97 +/- 3.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.44 +/- 1.07\n",
      "Episode length: 6.72 +/- 3.46\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.39 +/- 1.08\n",
      "Episode length: 6.46 +/- 3.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.55 +/- 1.01\n",
      "Episode length: 7.41 +/- 2.83\n",
      "Eval num_timesteps=50000, episode_reward=-0.47 +/- 0.97\n",
      "Episode length: 7.30 +/- 2.79\n",
      "Eval num_timesteps=60000, episode_reward=-0.53 +/- 1.03\n",
      "Episode length: 7.33 +/- 2.94\n",
      "Eval num_timesteps=70000, episode_reward=-0.35 +/- 1.03\n",
      "Episode length: 6.90 +/- 3.18\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.49 +/- 1.03\n",
      "Episode length: 7.29 +/- 3.09\n",
      "Eval num_timesteps=90000, episode_reward=-0.49 +/- 1.05\n",
      "Episode length: 7.04 +/- 3.30\n",
      "Eval num_timesteps=100000, episode_reward=-0.34 +/- 1.09\n",
      "Episode length: 6.53 +/- 3.53\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-0.62 +/- 1.01\n",
      "Episode length: 7.38 +/- 3.04\n",
      "Eval num_timesteps=120000, episode_reward=-0.47 +/- 0.88\n",
      "Episode length: 7.74 +/- 2.47\n",
      "Eval num_timesteps=130000, episode_reward=-0.43 +/- 0.99\n",
      "Episode length: 7.16 +/- 3.15\n",
      "Eval num_timesteps=140000, episode_reward=-0.51 +/- 0.89\n",
      "Episode length: 7.79 +/- 2.83\n",
      "Eval num_timesteps=150000, episode_reward=-0.54 +/- 0.89\n",
      "Episode length: 7.72 +/- 2.88\n",
      "Eval num_timesteps=160000, episode_reward=-0.64 +/- 1.04\n",
      "Episode length: 7.50 +/- 3.19\n",
      "Eval num_timesteps=170000, episode_reward=-0.58 +/- 1.00\n",
      "Episode length: 7.57 +/- 3.28\n",
      "Eval num_timesteps=180000, episode_reward=-0.50 +/- 0.93\n",
      "Episode length: 7.60 +/- 3.21\n",
      "Eval num_timesteps=190000, episode_reward=-0.50 +/- 1.06\n",
      "Episode length: 7.16 +/- 2.87\n",
      "Eval num_timesteps=200000, episode_reward=-0.46 +/- 1.01\n",
      "Episode length: 7.07 +/- 3.12\n",
      "Evaluating best model for run 64...\n",
      "Run 64 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6800 (68.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.66\n",
      "  Average Episode Reward: -0.2975\n",
      "  Average Episode Length: 6.37\n",
      "===== COMPLETED DQN RUN 64 ====\n",
      "\n",
      "===== STARTING DQN RUN 65 with SEED 65 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.52 +/- 1.11\n",
      "Episode length: 6.80 +/- 3.48\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.53 +/- 1.13\n",
      "Episode length: 6.78 +/- 3.38\n",
      "Eval num_timesteps=30000, episode_reward=-0.25 +/- 1.03\n",
      "Episode length: 6.20 +/- 3.39\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.44 +/- 1.02\n",
      "Episode length: 6.90 +/- 3.24\n",
      "Eval num_timesteps=50000, episode_reward=-0.37 +/- 1.00\n",
      "Episode length: 6.86 +/- 2.96\n",
      "Eval num_timesteps=60000, episode_reward=-0.51 +/- 1.05\n",
      "Episode length: 6.79 +/- 3.14\n",
      "Eval num_timesteps=70000, episode_reward=-0.27 +/- 1.04\n",
      "Episode length: 6.42 +/- 3.27\n",
      "Eval num_timesteps=80000, episode_reward=-0.46 +/- 1.04\n",
      "Episode length: 6.79 +/- 3.16\n",
      "Eval num_timesteps=90000, episode_reward=-0.45 +/- 1.00\n",
      "Episode length: 7.16 +/- 3.15\n",
      "Eval num_timesteps=100000, episode_reward=-0.47 +/- 1.01\n",
      "Episode length: 7.09 +/- 3.26\n",
      "Eval num_timesteps=110000, episode_reward=-0.33 +/- 1.04\n",
      "Episode length: 6.61 +/- 3.37\n",
      "Eval num_timesteps=120000, episode_reward=-0.33 +/- 1.02\n",
      "Episode length: 6.67 +/- 3.32\n",
      "Eval num_timesteps=130000, episode_reward=-0.48 +/- 1.04\n",
      "Episode length: 7.20 +/- 3.14\n",
      "Eval num_timesteps=140000, episode_reward=-0.51 +/- 0.99\n",
      "Episode length: 7.47 +/- 2.98\n",
      "Eval num_timesteps=150000, episode_reward=-0.39 +/- 1.01\n",
      "Episode length: 7.00 +/- 2.91\n",
      "Eval num_timesteps=160000, episode_reward=-0.34 +/- 0.97\n",
      "Episode length: 7.06 +/- 3.34\n",
      "Eval num_timesteps=170000, episode_reward=-0.34 +/- 1.05\n",
      "Episode length: 6.83 +/- 3.06\n",
      "Eval num_timesteps=180000, episode_reward=-0.56 +/- 0.96\n",
      "Episode length: 7.83 +/- 2.77\n",
      "Eval num_timesteps=190000, episode_reward=-0.54 +/- 1.01\n",
      "Episode length: 7.41 +/- 3.16\n",
      "Eval num_timesteps=200000, episode_reward=-0.61 +/- 0.95\n",
      "Episode length: 7.89 +/- 2.74\n",
      "Evaluating best model for run 65...\n",
      "Run 65 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6300 (63.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.21\n",
      "  Average Episode Reward: -0.3320\n",
      "  Average Episode Length: 6.35\n",
      "===== COMPLETED DQN RUN 65 ====\n",
      "\n",
      "===== STARTING DQN RUN 66 with SEED 66 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.54 +/- 1.12\n",
      "Episode length: 6.88 +/- 3.44\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.50 +/- 1.13\n",
      "Episode length: 6.67 +/- 3.35\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.55 +/- 1.03\n",
      "Episode length: 7.30 +/- 3.14\n",
      "Eval num_timesteps=40000, episode_reward=-0.36 +/- 1.06\n",
      "Episode length: 6.66 +/- 3.33\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.27 +/- 1.01\n",
      "Episode length: 6.49 +/- 3.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.49 +/- 1.04\n",
      "Episode length: 7.10 +/- 3.21\n",
      "Eval num_timesteps=70000, episode_reward=-0.27 +/- 1.04\n",
      "Episode length: 6.43 +/- 3.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.18 +/- 1.04\n",
      "Episode length: 6.17 +/- 3.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-0.60 +/- 1.04\n",
      "Episode length: 7.40 +/- 2.97\n",
      "Eval num_timesteps=100000, episode_reward=-0.57 +/- 0.93\n",
      "Episode length: 7.81 +/- 2.95\n",
      "Eval num_timesteps=110000, episode_reward=-0.30 +/- 0.96\n",
      "Episode length: 6.99 +/- 2.89\n",
      "Eval num_timesteps=120000, episode_reward=-0.34 +/- 0.98\n",
      "Episode length: 7.00 +/- 3.13\n",
      "Eval num_timesteps=130000, episode_reward=-0.46 +/- 1.01\n",
      "Episode length: 7.13 +/- 3.07\n",
      "Eval num_timesteps=140000, episode_reward=-0.44 +/- 0.95\n",
      "Episode length: 7.38 +/- 3.06\n",
      "Eval num_timesteps=150000, episode_reward=-0.19 +/- 0.91\n",
      "Episode length: 6.66 +/- 3.12\n",
      "Eval num_timesteps=160000, episode_reward=-0.43 +/- 0.98\n",
      "Episode length: 7.21 +/- 3.17\n",
      "Eval num_timesteps=170000, episode_reward=-0.44 +/- 1.02\n",
      "Episode length: 7.07 +/- 3.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.55 +/- 0.99\n",
      "Episode length: 7.51 +/- 2.93\n",
      "Eval num_timesteps=190000, episode_reward=-0.56 +/- 1.10\n",
      "Episode length: 7.10 +/- 3.01\n",
      "Eval num_timesteps=200000, episode_reward=-0.47 +/- 1.05\n",
      "Episode length: 6.98 +/- 3.05\n",
      "Evaluating best model for run 66...\n",
      "Run 66 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5700 (57.00%)\n",
      "  Average Detection Time (Successful Episodes): 3.77\n",
      "  Average Episode Reward: -0.3320\n",
      "  Average Episode Length: 6.45\n",
      "===== COMPLETED DQN RUN 66 ====\n",
      "\n",
      "===== STARTING DQN RUN 67 with SEED 67 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.58 +/- 1.15\n",
      "Episode length: 7.01 +/- 3.45\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.57 +/- 1.06\n",
      "Episode length: 7.20 +/- 3.04\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.61 +/- 0.91\n",
      "Episode length: 7.58 +/- 2.71\n",
      "Eval num_timesteps=40000, episode_reward=-0.51 +/- 0.94\n",
      "Episode length: 7.56 +/- 2.78\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.42 +/- 0.99\n",
      "Episode length: 7.24 +/- 3.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.27 +/- 1.00\n",
      "Episode length: 6.40 +/- 3.32\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.44 +/- 1.02\n",
      "Episode length: 7.06 +/- 3.06\n",
      "Eval num_timesteps=80000, episode_reward=-0.48 +/- 1.07\n",
      "Episode length: 6.85 +/- 3.40\n",
      "Eval num_timesteps=90000, episode_reward=-0.34 +/- 1.04\n",
      "Episode length: 6.76 +/- 3.43\n",
      "Eval num_timesteps=100000, episode_reward=-0.60 +/- 1.05\n",
      "Episode length: 7.41 +/- 3.03\n",
      "Eval num_timesteps=110000, episode_reward=-0.29 +/- 0.99\n",
      "Episode length: 6.57 +/- 3.27\n",
      "Eval num_timesteps=120000, episode_reward=-0.54 +/- 1.05\n",
      "Episode length: 7.16 +/- 3.64\n",
      "Eval num_timesteps=130000, episode_reward=-0.56 +/- 1.02\n",
      "Episode length: 7.41 +/- 3.31\n",
      "Eval num_timesteps=140000, episode_reward=-0.54 +/- 1.03\n",
      "Episode length: 7.34 +/- 2.94\n",
      "Eval num_timesteps=150000, episode_reward=-0.54 +/- 1.03\n",
      "Episode length: 7.21 +/- 2.97\n",
      "Eval num_timesteps=160000, episode_reward=-0.46 +/- 1.01\n",
      "Episode length: 7.15 +/- 3.22\n",
      "Eval num_timesteps=170000, episode_reward=-0.56 +/- 1.01\n",
      "Episode length: 7.42 +/- 3.36\n",
      "Eval num_timesteps=180000, episode_reward=-0.52 +/- 1.03\n",
      "Episode length: 7.15 +/- 3.24\n",
      "Eval num_timesteps=190000, episode_reward=-0.35 +/- 0.94\n",
      "Episode length: 7.09 +/- 2.96\n",
      "Eval num_timesteps=200000, episode_reward=-0.41 +/- 0.94\n",
      "Episode length: 7.48 +/- 2.82\n",
      "Evaluating best model for run 67...\n",
      "Run 67 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6000 (60.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.92\n",
      "  Average Episode Reward: -0.3965\n",
      "  Average Episode Length: 6.95\n",
      "===== COMPLETED DQN RUN 67 ====\n",
      "\n",
      "===== STARTING DQN RUN 68 with SEED 68 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.40 +/- 1.03\n",
      "Episode length: 6.96 +/- 3.38\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.44 +/- 0.95\n",
      "Episode length: 7.51 +/- 2.77\n",
      "Eval num_timesteps=30000, episode_reward=-0.46 +/- 1.16\n",
      "Episode length: 6.52 +/- 3.59\n",
      "Eval num_timesteps=40000, episode_reward=-0.27 +/- 1.02\n",
      "Episode length: 6.55 +/- 2.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.65 +/- 1.06\n",
      "Episode length: 7.50 +/- 3.27\n",
      "Eval num_timesteps=60000, episode_reward=-0.38 +/- 0.99\n",
      "Episode length: 7.13 +/- 3.15\n",
      "Eval num_timesteps=70000, episode_reward=-0.37 +/- 1.04\n",
      "Episode length: 6.63 +/- 3.14\n",
      "Eval num_timesteps=80000, episode_reward=-0.44 +/- 1.12\n",
      "Episode length: 6.53 +/- 3.45\n",
      "Eval num_timesteps=90000, episode_reward=-0.49 +/- 0.97\n",
      "Episode length: 7.44 +/- 2.81\n",
      "Eval num_timesteps=100000, episode_reward=-0.25 +/- 0.99\n",
      "Episode length: 6.44 +/- 3.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-0.43 +/- 0.99\n",
      "Episode length: 7.24 +/- 2.83\n",
      "Eval num_timesteps=120000, episode_reward=-0.49 +/- 1.02\n",
      "Episode length: 7.15 +/- 3.11\n",
      "Eval num_timesteps=130000, episode_reward=-0.27 +/- 1.01\n",
      "Episode length: 6.31 +/- 3.33\n",
      "Eval num_timesteps=140000, episode_reward=-0.47 +/- 1.05\n",
      "Episode length: 7.07 +/- 3.31\n",
      "Eval num_timesteps=150000, episode_reward=-0.32 +/- 1.03\n",
      "Episode length: 6.41 +/- 3.38\n",
      "Eval num_timesteps=160000, episode_reward=-0.32 +/- 1.06\n",
      "Episode length: 6.31 +/- 3.61\n",
      "Eval num_timesteps=170000, episode_reward=-0.42 +/- 1.03\n",
      "Episode length: 6.74 +/- 3.51\n",
      "Eval num_timesteps=180000, episode_reward=-0.34 +/- 1.03\n",
      "Episode length: 6.49 +/- 3.40\n",
      "Eval num_timesteps=190000, episode_reward=-0.36 +/- 0.98\n",
      "Episode length: 6.86 +/- 3.08\n",
      "Eval num_timesteps=200000, episode_reward=-0.32 +/- 1.03\n",
      "Episode length: 6.33 +/- 3.20\n",
      "Evaluating best model for run 68...\n",
      "Run 68 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5900 (59.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.92\n",
      "  Average Episode Reward: -0.4670\n",
      "  Average Episode Length: 7.00\n",
      "===== COMPLETED DQN RUN 68 ====\n",
      "\n",
      "===== STARTING DQN RUN 69 with SEED 69 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.53 +/- 1.12\n",
      "Episode length: 6.95 +/- 3.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.32 +/- 1.01\n",
      "Episode length: 6.69 +/- 3.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.30 +/- 0.90\n",
      "Episode length: 6.94 +/- 2.86\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.57 +/- 1.03\n",
      "Episode length: 7.36 +/- 2.92\n",
      "Eval num_timesteps=50000, episode_reward=-0.25 +/- 1.05\n",
      "Episode length: 6.26 +/- 3.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.34 +/- 0.98\n",
      "Episode length: 6.80 +/- 2.90\n",
      "Eval num_timesteps=70000, episode_reward=-0.50 +/- 1.06\n",
      "Episode length: 6.98 +/- 3.14\n",
      "Eval num_timesteps=80000, episode_reward=-0.58 +/- 1.07\n",
      "Episode length: 7.23 +/- 3.17\n",
      "Eval num_timesteps=90000, episode_reward=-0.49 +/- 1.10\n",
      "Episode length: 6.82 +/- 3.42\n",
      "Eval num_timesteps=100000, episode_reward=-0.37 +/- 1.07\n",
      "Episode length: 6.50 +/- 3.50\n",
      "Eval num_timesteps=110000, episode_reward=-0.59 +/- 1.04\n",
      "Episode length: 7.21 +/- 3.33\n",
      "Eval num_timesteps=120000, episode_reward=-0.42 +/- 1.03\n",
      "Episode length: 6.88 +/- 2.98\n",
      "Eval num_timesteps=130000, episode_reward=-0.35 +/- 0.97\n",
      "Episode length: 6.97 +/- 2.95\n",
      "Eval num_timesteps=140000, episode_reward=-0.42 +/- 1.06\n",
      "Episode length: 6.75 +/- 3.49\n",
      "Eval num_timesteps=150000, episode_reward=-0.41 +/- 1.02\n",
      "Episode length: 6.89 +/- 3.36\n",
      "Eval num_timesteps=160000, episode_reward=-0.32 +/- 0.99\n",
      "Episode length: 6.82 +/- 3.01\n",
      "Eval num_timesteps=170000, episode_reward=-0.49 +/- 0.97\n",
      "Episode length: 7.51 +/- 2.79\n",
      "Eval num_timesteps=180000, episode_reward=-0.30 +/- 1.01\n",
      "Episode length: 6.74 +/- 2.94\n",
      "Eval num_timesteps=190000, episode_reward=-0.50 +/- 1.05\n",
      "Episode length: 7.20 +/- 3.17\n",
      "Eval num_timesteps=200000, episode_reward=-0.03 +/- 0.88\n",
      "Episode length: 6.09 +/- 3.20\n",
      "New best mean reward!\n",
      "Evaluating best model for run 69...\n",
      "Run 69 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5100 (51.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.24\n",
      "  Average Episode Reward: -0.5975\n",
      "  Average Episode Length: 7.57\n",
      "===== COMPLETED DQN RUN 69 ====\n",
      "\n",
      "===== STARTING DQN RUN 70 with SEED 70 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.37 +/- 1.00\n",
      "Episode length: 6.98 +/- 3.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.45 +/- 1.14\n",
      "Episode length: 6.70 +/- 3.26\n",
      "Eval num_timesteps=30000, episode_reward=-0.30 +/- 1.14\n",
      "Episode length: 6.15 +/- 3.43\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.28 +/- 1.01\n",
      "Episode length: 6.42 +/- 3.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.58 +/- 0.99\n",
      "Episode length: 7.66 +/- 2.65\n",
      "Eval num_timesteps=60000, episode_reward=-0.49 +/- 1.00\n",
      "Episode length: 7.24 +/- 3.11\n",
      "Eval num_timesteps=70000, episode_reward=-0.52 +/- 1.07\n",
      "Episode length: 7.04 +/- 3.47\n",
      "Eval num_timesteps=80000, episode_reward=-0.46 +/- 1.06\n",
      "Episode length: 6.98 +/- 3.18\n",
      "Eval num_timesteps=90000, episode_reward=-0.33 +/- 0.97\n",
      "Episode length: 6.92 +/- 3.50\n",
      "Eval num_timesteps=100000, episode_reward=-0.35 +/- 1.02\n",
      "Episode length: 6.79 +/- 3.41\n",
      "Eval num_timesteps=110000, episode_reward=-0.41 +/- 0.99\n",
      "Episode length: 7.06 +/- 3.31\n",
      "Eval num_timesteps=120000, episode_reward=-0.35 +/- 1.00\n",
      "Episode length: 6.88 +/- 3.21\n",
      "Eval num_timesteps=130000, episode_reward=-0.36 +/- 1.02\n",
      "Episode length: 6.63 +/- 3.49\n",
      "Eval num_timesteps=140000, episode_reward=-0.54 +/- 1.01\n",
      "Episode length: 7.37 +/- 3.32\n",
      "Eval num_timesteps=150000, episode_reward=-0.46 +/- 0.98\n",
      "Episode length: 7.62 +/- 2.59\n",
      "Eval num_timesteps=160000, episode_reward=-0.52 +/- 1.04\n",
      "Episode length: 7.16 +/- 3.12\n",
      "Eval num_timesteps=170000, episode_reward=-0.54 +/- 0.95\n",
      "Episode length: 8.02 +/- 2.30\n",
      "Eval num_timesteps=180000, episode_reward=-0.32 +/- 1.00\n",
      "Episode length: 6.76 +/- 3.25\n",
      "Eval num_timesteps=190000, episode_reward=-0.57 +/- 1.10\n",
      "Episode length: 7.07 +/- 3.42\n",
      "Eval num_timesteps=200000, episode_reward=-0.50 +/- 1.01\n",
      "Episode length: 7.14 +/- 3.26\n",
      "Evaluating best model for run 70...\n",
      "Run 70 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6800 (68.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.24\n",
      "  Average Episode Reward: -0.2190\n",
      "  Average Episode Length: 6.08\n",
      "===== COMPLETED DQN RUN 70 ====\n",
      "\n",
      "===== STARTING DQN RUN 71 with SEED 71 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.90 +/- 0.98\n",
      "Episode length: 8.24 +/- 2.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.39 +/- 1.02\n",
      "Episode length: 6.94 +/- 2.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.33 +/- 1.02\n",
      "Episode length: 6.69 +/- 3.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.39 +/- 1.13\n",
      "Episode length: 6.35 +/- 3.48\n",
      "Eval num_timesteps=50000, episode_reward=-0.33 +/- 1.04\n",
      "Episode length: 6.57 +/- 3.35\n",
      "Eval num_timesteps=60000, episode_reward=-0.21 +/- 1.01\n",
      "Episode length: 6.35 +/- 3.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.53 +/- 0.90\n",
      "Episode length: 7.76 +/- 2.99\n",
      "Eval num_timesteps=80000, episode_reward=-0.59 +/- 0.90\n",
      "Episode length: 7.95 +/- 2.91\n",
      "Eval num_timesteps=90000, episode_reward=-0.61 +/- 1.00\n",
      "Episode length: 7.74 +/- 2.88\n",
      "Eval num_timesteps=100000, episode_reward=-0.48 +/- 0.98\n",
      "Episode length: 7.37 +/- 2.88\n",
      "Eval num_timesteps=110000, episode_reward=-0.45 +/- 0.98\n",
      "Episode length: 7.36 +/- 3.05\n",
      "Eval num_timesteps=120000, episode_reward=-0.43 +/- 0.98\n",
      "Episode length: 7.19 +/- 2.93\n",
      "Eval num_timesteps=130000, episode_reward=-0.37 +/- 1.06\n",
      "Episode length: 6.67 +/- 3.18\n",
      "Eval num_timesteps=140000, episode_reward=-0.54 +/- 0.93\n",
      "Episode length: 7.58 +/- 3.01\n",
      "Eval num_timesteps=150000, episode_reward=-0.50 +/- 1.03\n",
      "Episode length: 7.23 +/- 2.99\n",
      "Eval num_timesteps=160000, episode_reward=-0.40 +/- 1.00\n",
      "Episode length: 6.92 +/- 3.33\n",
      "Eval num_timesteps=170000, episode_reward=-0.45 +/- 0.99\n",
      "Episode length: 7.16 +/- 3.07\n",
      "Eval num_timesteps=180000, episode_reward=-0.35 +/- 1.03\n",
      "Episode length: 6.69 +/- 3.39\n",
      "Eval num_timesteps=190000, episode_reward=-0.52 +/- 1.04\n",
      "Episode length: 7.23 +/- 3.15\n",
      "Eval num_timesteps=200000, episode_reward=-0.55 +/- 1.04\n",
      "Episode length: 7.25 +/- 3.29\n",
      "Evaluating best model for run 71...\n",
      "Run 71 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6300 (63.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.68\n",
      "  Average Episode Reward: -0.3240\n",
      "  Average Episode Length: 6.65\n",
      "===== COMPLETED DQN RUN 71 ====\n",
      "\n",
      "===== STARTING DQN RUN 72 with SEED 72 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.47 +/- 0.97\n",
      "Episode length: 7.32 +/- 3.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.45 +/- 1.03\n",
      "Episode length: 7.14 +/- 3.10\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.47 +/- 0.94\n",
      "Episode length: 7.59 +/- 2.93\n",
      "Eval num_timesteps=40000, episode_reward=-0.49 +/- 1.06\n",
      "Episode length: 7.10 +/- 3.31\n",
      "Eval num_timesteps=50000, episode_reward=-0.44 +/- 0.94\n",
      "Episode length: 7.39 +/- 3.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.46 +/- 0.95\n",
      "Episode length: 7.48 +/- 3.06\n",
      "Eval num_timesteps=70000, episode_reward=-0.44 +/- 0.95\n",
      "Episode length: 7.34 +/- 3.27\n",
      "Eval num_timesteps=80000, episode_reward=-0.35 +/- 0.97\n",
      "Episode length: 7.09 +/- 3.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-0.30 +/- 0.99\n",
      "Episode length: 6.76 +/- 3.16\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.55 +/- 1.05\n",
      "Episode length: 7.39 +/- 3.14\n",
      "Eval num_timesteps=110000, episode_reward=-0.67 +/- 0.94\n",
      "Episode length: 8.06 +/- 2.63\n",
      "Eval num_timesteps=120000, episode_reward=-0.38 +/- 0.99\n",
      "Episode length: 7.07 +/- 3.40\n",
      "Eval num_timesteps=130000, episode_reward=-0.45 +/- 0.98\n",
      "Episode length: 7.38 +/- 2.90\n",
      "Eval num_timesteps=140000, episode_reward=-0.34 +/- 1.01\n",
      "Episode length: 6.88 +/- 3.14\n",
      "Eval num_timesteps=150000, episode_reward=-0.48 +/- 0.93\n",
      "Episode length: 7.72 +/- 2.71\n",
      "Eval num_timesteps=160000, episode_reward=-0.45 +/- 1.07\n",
      "Episode length: 6.96 +/- 3.30\n",
      "Eval num_timesteps=170000, episode_reward=-0.51 +/- 1.01\n",
      "Episode length: 7.31 +/- 3.06\n",
      "Eval num_timesteps=180000, episode_reward=-0.49 +/- 1.07\n",
      "Episode length: 7.12 +/- 3.34\n",
      "Eval num_timesteps=190000, episode_reward=-0.35 +/- 1.02\n",
      "Episode length: 6.76 +/- 3.39\n",
      "Eval num_timesteps=200000, episode_reward=-0.45 +/- 1.02\n",
      "Episode length: 7.23 +/- 3.11\n",
      "Evaluating best model for run 72...\n",
      "Run 72 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5300 (53.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.42\n",
      "  Average Episode Reward: -0.4095\n",
      "  Average Episode Length: 7.04\n",
      "===== COMPLETED DQN RUN 72 ====\n",
      "\n",
      "===== STARTING DQN RUN 73 with SEED 73 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.80 +/- 1.03\n",
      "Episode length: 7.90 +/- 2.91\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.54 +/- 1.15\n",
      "Episode length: 6.68 +/- 3.36\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.33 +/- 1.01\n",
      "Episode length: 6.81 +/- 3.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.45 +/- 1.08\n",
      "Episode length: 6.87 +/- 3.21\n",
      "Eval num_timesteps=50000, episode_reward=-0.35 +/- 0.99\n",
      "Episode length: 6.70 +/- 3.25\n",
      "Eval num_timesteps=60000, episode_reward=-0.33 +/- 1.05\n",
      "Episode length: 6.45 +/- 3.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.56 +/- 0.90\n",
      "Episode length: 7.80 +/- 2.85\n",
      "Eval num_timesteps=80000, episode_reward=-0.52 +/- 0.96\n",
      "Episode length: 7.54 +/- 3.04\n",
      "Eval num_timesteps=90000, episode_reward=-0.18 +/- 1.00\n",
      "Episode length: 6.16 +/- 3.51\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.29 +/- 0.97\n",
      "Episode length: 6.92 +/- 3.00\n",
      "Eval num_timesteps=110000, episode_reward=-0.22 +/- 0.97\n",
      "Episode length: 6.33 +/- 3.32\n",
      "Eval num_timesteps=120000, episode_reward=-0.39 +/- 1.01\n",
      "Episode length: 7.01 +/- 2.88\n",
      "Eval num_timesteps=130000, episode_reward=-0.40 +/- 1.04\n",
      "Episode length: 6.81 +/- 3.32\n",
      "Eval num_timesteps=140000, episode_reward=-0.45 +/- 0.97\n",
      "Episode length: 7.25 +/- 2.98\n",
      "Eval num_timesteps=150000, episode_reward=-0.31 +/- 0.98\n",
      "Episode length: 6.76 +/- 3.08\n",
      "Eval num_timesteps=160000, episode_reward=-0.47 +/- 0.98\n",
      "Episode length: 7.33 +/- 2.89\n",
      "Eval num_timesteps=170000, episode_reward=-0.43 +/- 1.04\n",
      "Episode length: 6.96 +/- 3.28\n",
      "Eval num_timesteps=180000, episode_reward=-0.46 +/- 0.94\n",
      "Episode length: 7.38 +/- 3.13\n",
      "Eval num_timesteps=190000, episode_reward=-0.56 +/- 0.93\n",
      "Episode length: 7.62 +/- 3.09\n",
      "Eval num_timesteps=200000, episode_reward=-0.47 +/- 0.93\n",
      "Episode length: 7.40 +/- 3.10\n",
      "Evaluating best model for run 73...\n",
      "Run 73 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5400 (54.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.15\n",
      "  Average Episode Reward: -0.3885\n",
      "  Average Episode Length: 6.84\n",
      "===== COMPLETED DQN RUN 73 ====\n",
      "\n",
      "===== STARTING DQN RUN 74 with SEED 74 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.17 +/- 1.13\n",
      "Episode length: 5.69 +/- 3.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.22 +/- 1.00\n",
      "Episode length: 6.39 +/- 3.25\n",
      "Eval num_timesteps=30000, episode_reward=-0.19 +/- 1.02\n",
      "Episode length: 6.09 +/- 3.24\n",
      "Eval num_timesteps=40000, episode_reward=-0.49 +/- 1.04\n",
      "Episode length: 7.15 +/- 3.20\n",
      "Eval num_timesteps=50000, episode_reward=-0.60 +/- 1.01\n",
      "Episode length: 7.77 +/- 2.82\n",
      "Eval num_timesteps=60000, episode_reward=-0.57 +/- 0.94\n",
      "Episode length: 7.76 +/- 2.99\n",
      "Eval num_timesteps=70000, episode_reward=-0.35 +/- 0.99\n",
      "Episode length: 7.02 +/- 3.18\n",
      "Eval num_timesteps=80000, episode_reward=-0.49 +/- 0.99\n",
      "Episode length: 7.39 +/- 3.16\n",
      "Eval num_timesteps=90000, episode_reward=-0.22 +/- 0.96\n",
      "Episode length: 6.54 +/- 3.38\n",
      "Eval num_timesteps=100000, episode_reward=-0.57 +/- 1.02\n",
      "Episode length: 7.50 +/- 2.94\n",
      "Eval num_timesteps=110000, episode_reward=-0.53 +/- 0.92\n",
      "Episode length: 7.60 +/- 3.14\n",
      "Eval num_timesteps=120000, episode_reward=-0.43 +/- 0.97\n",
      "Episode length: 7.25 +/- 3.32\n",
      "Eval num_timesteps=130000, episode_reward=-0.38 +/- 0.97\n",
      "Episode length: 7.07 +/- 3.18\n",
      "Eval num_timesteps=140000, episode_reward=-0.50 +/- 0.97\n",
      "Episode length: 7.47 +/- 3.16\n",
      "Eval num_timesteps=150000, episode_reward=-0.40 +/- 1.03\n",
      "Episode length: 7.03 +/- 2.96\n",
      "Eval num_timesteps=160000, episode_reward=-0.39 +/- 1.01\n",
      "Episode length: 7.12 +/- 3.05\n",
      "Eval num_timesteps=170000, episode_reward=-0.39 +/- 0.97\n",
      "Episode length: 7.28 +/- 3.14\n",
      "Eval num_timesteps=180000, episode_reward=-0.41 +/- 1.05\n",
      "Episode length: 7.10 +/- 3.26\n",
      "Eval num_timesteps=190000, episode_reward=-0.45 +/- 0.99\n",
      "Episode length: 7.27 +/- 2.91\n",
      "Eval num_timesteps=200000, episode_reward=-0.46 +/- 1.01\n",
      "Episode length: 7.34 +/- 3.05\n",
      "Evaluating best model for run 74...\n",
      "Run 74 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6900 (69.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.72\n",
      "  Average Episode Reward: -0.3290\n",
      "  Average Episode Length: 6.36\n",
      "===== COMPLETED DQN RUN 74 ====\n",
      "\n",
      "===== STARTING DQN RUN 75 with SEED 75 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.49 +/- 0.94\n",
      "Episode length: 7.42 +/- 3.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.59 +/- 1.08\n",
      "Episode length: 7.00 +/- 3.48\n",
      "Eval num_timesteps=30000, episode_reward=-0.53 +/- 1.02\n",
      "Episode length: 6.98 +/- 3.38\n",
      "Eval num_timesteps=40000, episode_reward=-0.50 +/- 1.10\n",
      "Episode length: 6.72 +/- 3.39\n",
      "Eval num_timesteps=50000, episode_reward=-0.52 +/- 0.95\n",
      "Episode length: 7.38 +/- 3.03\n",
      "Eval num_timesteps=60000, episode_reward=-0.03 +/- 0.98\n",
      "Episode length: 5.43 +/- 3.30\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.24 +/- 1.00\n",
      "Episode length: 6.35 +/- 3.42\n",
      "Eval num_timesteps=80000, episode_reward=-0.38 +/- 1.03\n",
      "Episode length: 6.64 +/- 3.47\n",
      "Eval num_timesteps=90000, episode_reward=-0.33 +/- 1.07\n",
      "Episode length: 6.43 +/- 3.71\n",
      "Eval num_timesteps=100000, episode_reward=-0.29 +/- 1.06\n",
      "Episode length: 6.29 +/- 3.58\n",
      "Eval num_timesteps=110000, episode_reward=-0.63 +/- 1.06\n",
      "Episode length: 7.34 +/- 3.17\n",
      "Eval num_timesteps=120000, episode_reward=-0.51 +/- 1.08\n",
      "Episode length: 6.96 +/- 3.17\n",
      "Eval num_timesteps=130000, episode_reward=-0.37 +/- 1.11\n",
      "Episode length: 6.32 +/- 3.56\n",
      "Eval num_timesteps=140000, episode_reward=-0.32 +/- 1.07\n",
      "Episode length: 6.38 +/- 3.27\n",
      "Eval num_timesteps=150000, episode_reward=-0.40 +/- 1.17\n",
      "Episode length: 6.32 +/- 3.37\n",
      "Eval num_timesteps=160000, episode_reward=-0.30 +/- 1.05\n",
      "Episode length: 6.51 +/- 3.39\n",
      "Eval num_timesteps=170000, episode_reward=-0.37 +/- 1.01\n",
      "Episode length: 6.71 +/- 3.13\n",
      "Eval num_timesteps=180000, episode_reward=-0.31 +/- 1.03\n",
      "Episode length: 6.55 +/- 3.40\n",
      "Eval num_timesteps=190000, episode_reward=-0.41 +/- 0.91\n",
      "Episode length: 7.24 +/- 3.18\n",
      "Eval num_timesteps=200000, episode_reward=-0.45 +/- 0.98\n",
      "Episode length: 7.19 +/- 3.39\n",
      "Evaluating best model for run 75...\n",
      "Run 75 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5800 (58.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.84\n",
      "  Average Episode Reward: -0.4700\n",
      "  Average Episode Length: 7.01\n",
      "===== COMPLETED DQN RUN 75 ====\n",
      "\n",
      "===== STARTING DQN RUN 76 with SEED 76 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.44 +/- 1.03\n",
      "Episode length: 7.06 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.56 +/- 1.03\n",
      "Episode length: 7.37 +/- 3.49\n",
      "Eval num_timesteps=30000, episode_reward=-0.37 +/- 0.98\n",
      "Episode length: 7.26 +/- 2.86\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.59 +/- 1.03\n",
      "Episode length: 7.17 +/- 3.07\n",
      "Eval num_timesteps=50000, episode_reward=-0.36 +/- 1.05\n",
      "Episode length: 6.52 +/- 3.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.43 +/- 0.99\n",
      "Episode length: 7.38 +/- 3.13\n",
      "Eval num_timesteps=70000, episode_reward=-0.58 +/- 1.01\n",
      "Episode length: 7.52 +/- 3.30\n",
      "Eval num_timesteps=80000, episode_reward=-0.39 +/- 0.93\n",
      "Episode length: 7.35 +/- 3.06\n",
      "Eval num_timesteps=90000, episode_reward=-0.26 +/- 1.00\n",
      "Episode length: 6.43 +/- 3.45\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.54 +/- 0.98\n",
      "Episode length: 7.49 +/- 2.98\n",
      "Eval num_timesteps=110000, episode_reward=-0.21 +/- 1.04\n",
      "Episode length: 5.99 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.34 +/- 1.10\n",
      "Episode length: 6.42 +/- 3.44\n",
      "Eval num_timesteps=130000, episode_reward=-0.36 +/- 1.01\n",
      "Episode length: 6.72 +/- 3.06\n",
      "Eval num_timesteps=140000, episode_reward=-0.50 +/- 0.99\n",
      "Episode length: 7.18 +/- 3.26\n",
      "Eval num_timesteps=150000, episode_reward=-0.52 +/- 0.94\n",
      "Episode length: 7.51 +/- 2.94\n",
      "Eval num_timesteps=160000, episode_reward=-0.08 +/- 0.94\n",
      "Episode length: 5.79 +/- 3.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=-0.31 +/- 1.04\n",
      "Episode length: 6.39 +/- 3.65\n",
      "Eval num_timesteps=180000, episode_reward=-0.39 +/- 1.07\n",
      "Episode length: 6.59 +/- 3.35\n",
      "Eval num_timesteps=190000, episode_reward=-0.40 +/- 0.98\n",
      "Episode length: 6.93 +/- 3.31\n",
      "Eval num_timesteps=200000, episode_reward=-0.48 +/- 0.94\n",
      "Episode length: 7.47 +/- 2.81\n",
      "Evaluating best model for run 76...\n",
      "Run 76 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5000 (50.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.34\n",
      "  Average Episode Reward: -0.5390\n",
      "  Average Episode Length: 7.17\n",
      "===== COMPLETED DQN RUN 76 ====\n",
      "\n",
      "===== STARTING DQN RUN 77 with SEED 77 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.38 +/- 1.02\n",
      "Episode length: 6.95 +/- 3.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.43 +/- 1.10\n",
      "Episode length: 6.61 +/- 3.31\n",
      "Eval num_timesteps=30000, episode_reward=-0.46 +/- 1.01\n",
      "Episode length: 7.13 +/- 3.11\n",
      "Eval num_timesteps=40000, episode_reward=-0.43 +/- 1.11\n",
      "Episode length: 6.58 +/- 3.51\n",
      "Eval num_timesteps=50000, episode_reward=-0.45 +/- 1.04\n",
      "Episode length: 7.01 +/- 3.26\n",
      "Eval num_timesteps=60000, episode_reward=-0.55 +/- 1.01\n",
      "Episode length: 7.05 +/- 3.03\n",
      "Eval num_timesteps=70000, episode_reward=-0.32 +/- 0.94\n",
      "Episode length: 6.89 +/- 3.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.51 +/- 0.99\n",
      "Episode length: 7.26 +/- 3.07\n",
      "Eval num_timesteps=90000, episode_reward=-0.41 +/- 1.00\n",
      "Episode length: 7.00 +/- 3.35\n",
      "Eval num_timesteps=100000, episode_reward=-0.61 +/- 0.98\n",
      "Episode length: 7.71 +/- 3.24\n",
      "Eval num_timesteps=110000, episode_reward=-0.55 +/- 0.98\n",
      "Episode length: 7.54 +/- 3.28\n",
      "Eval num_timesteps=120000, episode_reward=-0.65 +/- 0.93\n",
      "Episode length: 7.89 +/- 3.10\n",
      "Eval num_timesteps=130000, episode_reward=-0.41 +/- 0.96\n",
      "Episode length: 7.18 +/- 3.43\n",
      "Eval num_timesteps=140000, episode_reward=-0.51 +/- 0.92\n",
      "Episode length: 7.55 +/- 3.24\n",
      "Eval num_timesteps=150000, episode_reward=-0.54 +/- 0.97\n",
      "Episode length: 7.44 +/- 3.28\n",
      "Eval num_timesteps=160000, episode_reward=-0.55 +/- 0.96\n",
      "Episode length: 7.58 +/- 3.09\n",
      "Eval num_timesteps=170000, episode_reward=-0.38 +/- 0.94\n",
      "Episode length: 7.09 +/- 3.02\n",
      "Eval num_timesteps=180000, episode_reward=-0.36 +/- 1.03\n",
      "Episode length: 6.78 +/- 3.46\n",
      "Eval num_timesteps=190000, episode_reward=-0.46 +/- 0.99\n",
      "Episode length: 7.19 +/- 3.46\n",
      "Eval num_timesteps=200000, episode_reward=-0.15 +/- 1.03\n",
      "Episode length: 6.03 +/- 3.51\n",
      "New best mean reward!\n",
      "Evaluating best model for run 77...\n",
      "Run 77 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5900 (59.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.59\n",
      "  Average Episode Reward: -0.3895\n",
      "  Average Episode Length: 6.81\n",
      "===== COMPLETED DQN RUN 77 ====\n",
      "\n",
      "===== STARTING DQN RUN 78 with SEED 78 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.49 +/- 1.10\n",
      "Episode length: 7.04 +/- 3.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.34 +/- 1.07\n",
      "Episode length: 6.51 +/- 3.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.43 +/- 1.05\n",
      "Episode length: 6.77 +/- 3.41\n",
      "Eval num_timesteps=40000, episode_reward=-0.35 +/- 1.01\n",
      "Episode length: 6.77 +/- 3.19\n",
      "Eval num_timesteps=50000, episode_reward=-0.61 +/- 1.02\n",
      "Episode length: 7.48 +/- 3.21\n",
      "Eval num_timesteps=60000, episode_reward=-0.44 +/- 1.12\n",
      "Episode length: 6.60 +/- 3.62\n",
      "Eval num_timesteps=70000, episode_reward=-0.57 +/- 0.99\n",
      "Episode length: 7.58 +/- 2.92\n",
      "Eval num_timesteps=80000, episode_reward=-0.48 +/- 0.95\n",
      "Episode length: 7.34 +/- 3.19\n",
      "Eval num_timesteps=90000, episode_reward=-0.52 +/- 0.94\n",
      "Episode length: 7.61 +/- 3.13\n",
      "Eval num_timesteps=100000, episode_reward=-0.37 +/- 0.94\n",
      "Episode length: 7.28 +/- 2.88\n",
      "Eval num_timesteps=110000, episode_reward=-0.38 +/- 1.03\n",
      "Episode length: 6.76 +/- 3.34\n",
      "Eval num_timesteps=120000, episode_reward=-0.48 +/- 0.99\n",
      "Episode length: 7.16 +/- 3.10\n",
      "Eval num_timesteps=130000, episode_reward=-0.52 +/- 0.90\n",
      "Episode length: 7.81 +/- 2.72\n",
      "Eval num_timesteps=140000, episode_reward=-0.41 +/- 0.99\n",
      "Episode length: 7.19 +/- 2.95\n",
      "Eval num_timesteps=150000, episode_reward=-0.33 +/- 1.09\n",
      "Episode length: 6.22 +/- 3.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-0.56 +/- 0.96\n",
      "Episode length: 7.49 +/- 2.89\n",
      "Eval num_timesteps=170000, episode_reward=-0.34 +/- 0.88\n",
      "Episode length: 7.40 +/- 3.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.57 +/- 0.90\n",
      "Episode length: 7.83 +/- 2.68\n",
      "Eval num_timesteps=190000, episode_reward=-0.61 +/- 1.04\n",
      "Episode length: 7.44 +/- 3.02\n",
      "Eval num_timesteps=200000, episode_reward=-0.52 +/- 0.90\n",
      "Episode length: 7.73 +/- 2.89\n",
      "Evaluating best model for run 78...\n",
      "Run 78 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6000 (60.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.00\n",
      "  Average Episode Reward: -0.4600\n",
      "  Average Episode Length: 7.00\n",
      "===== COMPLETED DQN RUN 78 ====\n",
      "\n",
      "===== STARTING DQN RUN 79 with SEED 79 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.46 +/- 1.06\n",
      "Episode length: 6.94 +/- 3.09\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.38 +/- 1.03\n",
      "Episode length: 6.78 +/- 2.93\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.36 +/- 1.04\n",
      "Episode length: 6.83 +/- 3.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.49 +/- 1.01\n",
      "Episode length: 7.30 +/- 2.93\n",
      "Eval num_timesteps=50000, episode_reward=-0.52 +/- 1.07\n",
      "Episode length: 7.16 +/- 3.07\n",
      "Eval num_timesteps=60000, episode_reward=-0.54 +/- 0.96\n",
      "Episode length: 7.64 +/- 2.66\n",
      "Eval num_timesteps=70000, episode_reward=-0.62 +/- 1.00\n",
      "Episode length: 7.59 +/- 2.82\n",
      "Eval num_timesteps=80000, episode_reward=-0.54 +/- 0.96\n",
      "Episode length: 7.47 +/- 3.01\n",
      "Eval num_timesteps=90000, episode_reward=-0.23 +/- 0.97\n",
      "Episode length: 6.60 +/- 3.12\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.36 +/- 1.03\n",
      "Episode length: 6.75 +/- 3.11\n",
      "Eval num_timesteps=110000, episode_reward=-0.33 +/- 1.03\n",
      "Episode length: 6.69 +/- 3.11\n",
      "Eval num_timesteps=120000, episode_reward=-0.49 +/- 1.02\n",
      "Episode length: 7.23 +/- 3.38\n",
      "Eval num_timesteps=130000, episode_reward=-0.45 +/- 0.92\n",
      "Episode length: 7.45 +/- 2.98\n",
      "Eval num_timesteps=140000, episode_reward=-0.30 +/- 0.93\n",
      "Episode length: 7.05 +/- 2.94\n",
      "Eval num_timesteps=150000, episode_reward=-0.29 +/- 0.99\n",
      "Episode length: 6.68 +/- 3.16\n",
      "Eval num_timesteps=160000, episode_reward=-0.40 +/- 1.01\n",
      "Episode length: 7.00 +/- 3.06\n",
      "Eval num_timesteps=170000, episode_reward=-0.48 +/- 0.99\n",
      "Episode length: 7.22 +/- 3.17\n",
      "Eval num_timesteps=180000, episode_reward=-0.29 +/- 1.05\n",
      "Episode length: 6.48 +/- 3.29\n",
      "Eval num_timesteps=190000, episode_reward=-0.30 +/- 1.02\n",
      "Episode length: 6.63 +/- 3.06\n",
      "Eval num_timesteps=200000, episode_reward=-0.48 +/- 1.03\n",
      "Episode length: 7.24 +/- 3.22\n",
      "Evaluating best model for run 79...\n",
      "Run 79 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6400 (64.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.88\n",
      "  Average Episode Reward: -0.3380\n",
      "  Average Episode Length: 6.72\n",
      "===== COMPLETED DQN RUN 79 ====\n",
      "\n",
      "===== STARTING DQN RUN 80 with SEED 80 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.97 +/- 1.18\n",
      "Episode length: 7.62 +/- 3.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.60 +/- 0.85\n",
      "Episode length: 8.37 +/- 2.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.54 +/- 1.11\n",
      "Episode length: 6.93 +/- 3.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.30 +/- 1.00\n",
      "Episode length: 6.72 +/- 3.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.48 +/- 1.02\n",
      "Episode length: 7.18 +/- 3.14\n",
      "Eval num_timesteps=60000, episode_reward=-0.54 +/- 0.98\n",
      "Episode length: 7.32 +/- 3.05\n",
      "Eval num_timesteps=70000, episode_reward=-0.63 +/- 0.99\n",
      "Episode length: 7.83 +/- 2.91\n",
      "Eval num_timesteps=80000, episode_reward=-0.34 +/- 0.95\n",
      "Episode length: 7.32 +/- 2.99\n",
      "Eval num_timesteps=90000, episode_reward=-0.39 +/- 1.01\n",
      "Episode length: 7.06 +/- 3.14\n",
      "Eval num_timesteps=100000, episode_reward=-0.52 +/- 0.95\n",
      "Episode length: 7.58 +/- 3.10\n",
      "Eval num_timesteps=110000, episode_reward=-0.55 +/- 0.93\n",
      "Episode length: 7.68 +/- 3.10\n",
      "Eval num_timesteps=120000, episode_reward=-0.36 +/- 1.04\n",
      "Episode length: 6.83 +/- 3.32\n",
      "Eval num_timesteps=130000, episode_reward=-0.30 +/- 1.02\n",
      "Episode length: 6.65 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-0.56 +/- 0.97\n",
      "Episode length: 7.51 +/- 3.21\n",
      "Eval num_timesteps=150000, episode_reward=-0.30 +/- 1.03\n",
      "Episode length: 6.64 +/- 3.21\n",
      "Eval num_timesteps=160000, episode_reward=-0.27 +/- 1.08\n",
      "Episode length: 6.14 +/- 3.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=-0.56 +/- 1.00\n",
      "Episode length: 7.63 +/- 3.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.47 +/- 0.96\n",
      "Episode length: 7.51 +/- 2.79\n",
      "Eval num_timesteps=190000, episode_reward=-0.43 +/- 0.99\n",
      "Episode length: 7.19 +/- 3.07\n",
      "Eval num_timesteps=200000, episode_reward=-0.39 +/- 0.90\n",
      "Episode length: 7.49 +/- 2.77\n",
      "Evaluating best model for run 80...\n",
      "Run 80 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6700 (67.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.54\n",
      "  Average Episode Reward: -0.2835\n",
      "  Average Episode Length: 6.34\n",
      "===== COMPLETED DQN RUN 80 ====\n",
      "\n",
      "===== STARTING DQN RUN 81 with SEED 81 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.63 +/- 1.14\n",
      "Episode length: 7.00 +/- 3.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.50 +/- 1.19\n",
      "Episode length: 6.58 +/- 3.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.46 +/- 1.01\n",
      "Episode length: 7.25 +/- 3.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.50 +/- 1.05\n",
      "Episode length: 7.08 +/- 3.14\n",
      "Eval num_timesteps=50000, episode_reward=-0.58 +/- 1.07\n",
      "Episode length: 7.31 +/- 3.15\n",
      "Eval num_timesteps=60000, episode_reward=-0.40 +/- 0.94\n",
      "Episode length: 7.30 +/- 3.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.35 +/- 1.03\n",
      "Episode length: 6.72 +/- 3.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.66 +/- 0.90\n",
      "Episode length: 8.10 +/- 2.76\n",
      "Eval num_timesteps=90000, episode_reward=-0.47 +/- 1.00\n",
      "Episode length: 7.24 +/- 3.10\n",
      "Eval num_timesteps=100000, episode_reward=-0.41 +/- 1.02\n",
      "Episode length: 6.80 +/- 3.35\n",
      "Eval num_timesteps=110000, episode_reward=-0.42 +/- 0.99\n",
      "Episode length: 7.13 +/- 3.44\n",
      "Eval num_timesteps=120000, episode_reward=-0.30 +/- 0.99\n",
      "Episode length: 6.76 +/- 3.28\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.18 +/- 1.11\n",
      "Episode length: 5.71 +/- 3.46\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-0.68 +/- 0.90\n",
      "Episode length: 8.16 +/- 2.92\n",
      "Eval num_timesteps=150000, episode_reward=-0.37 +/- 0.91\n",
      "Episode length: 7.18 +/- 2.94\n",
      "Eval num_timesteps=160000, episode_reward=-0.42 +/- 1.01\n",
      "Episode length: 7.12 +/- 3.18\n",
      "Eval num_timesteps=170000, episode_reward=-0.43 +/- 0.99\n",
      "Episode length: 7.21 +/- 3.30\n",
      "Eval num_timesteps=180000, episode_reward=-0.48 +/- 0.91\n",
      "Episode length: 7.70 +/- 2.90\n",
      "Eval num_timesteps=190000, episode_reward=-0.44 +/- 1.02\n",
      "Episode length: 7.22 +/- 3.08\n",
      "Eval num_timesteps=200000, episode_reward=-0.62 +/- 0.93\n",
      "Episode length: 7.90 +/- 2.74\n",
      "Evaluating best model for run 81...\n",
      "Run 81 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6800 (68.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.25\n",
      "  Average Episode Reward: -0.2625\n",
      "  Average Episode Length: 6.09\n",
      "===== COMPLETED DQN RUN 81 ====\n",
      "\n",
      "===== STARTING DQN RUN 82 with SEED 82 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.27 +/- 1.03\n",
      "Episode length: 6.63 +/- 3.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.52 +/- 1.00\n",
      "Episode length: 7.64 +/- 2.67\n",
      "Eval num_timesteps=30000, episode_reward=-0.16 +/- 1.00\n",
      "Episode length: 6.41 +/- 3.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.36 +/- 1.03\n",
      "Episode length: 6.89 +/- 3.14\n",
      "Eval num_timesteps=50000, episode_reward=-0.86 +/- 1.02\n",
      "Episode length: 7.76 +/- 2.83\n",
      "Eval num_timesteps=60000, episode_reward=-0.28 +/- 0.93\n",
      "Episode length: 7.00 +/- 3.15\n",
      "Eval num_timesteps=70000, episode_reward=-0.58 +/- 0.86\n",
      "Episode length: 8.32 +/- 2.78\n",
      "Eval num_timesteps=80000, episode_reward=-0.44 +/- 1.04\n",
      "Episode length: 7.15 +/- 3.06\n",
      "Eval num_timesteps=90000, episode_reward=-0.50 +/- 0.97\n",
      "Episode length: 7.53 +/- 3.12\n",
      "Eval num_timesteps=100000, episode_reward=-0.49 +/- 1.00\n",
      "Episode length: 7.51 +/- 2.85\n",
      "Eval num_timesteps=110000, episode_reward=-0.29 +/- 0.95\n",
      "Episode length: 7.00 +/- 3.05\n",
      "Eval num_timesteps=120000, episode_reward=-0.38 +/- 1.09\n",
      "Episode length: 6.83 +/- 3.12\n",
      "Eval num_timesteps=130000, episode_reward=-0.28 +/- 1.04\n",
      "Episode length: 6.60 +/- 3.08\n",
      "Eval num_timesteps=140000, episode_reward=-0.44 +/- 0.91\n",
      "Episode length: 7.43 +/- 3.19\n",
      "Eval num_timesteps=150000, episode_reward=-0.40 +/- 1.01\n",
      "Episode length: 7.14 +/- 3.33\n",
      "Eval num_timesteps=160000, episode_reward=-0.50 +/- 1.03\n",
      "Episode length: 7.21 +/- 3.09\n",
      "Eval num_timesteps=170000, episode_reward=-0.49 +/- 1.03\n",
      "Episode length: 7.15 +/- 3.03\n",
      "Eval num_timesteps=180000, episode_reward=-0.29 +/- 0.96\n",
      "Episode length: 7.15 +/- 2.75\n",
      "Eval num_timesteps=190000, episode_reward=-0.39 +/- 1.05\n",
      "Episode length: 6.83 +/- 3.10\n",
      "Eval num_timesteps=200000, episode_reward=-0.28 +/- 1.02\n",
      "Episode length: 6.71 +/- 2.96\n",
      "Evaluating best model for run 82...\n",
      "Run 82 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5600 (56.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.23\n",
      "  Average Episode Reward: -0.4725\n",
      "  Average Episode Length: 7.33\n",
      "===== COMPLETED DQN RUN 82 ====\n",
      "\n",
      "===== STARTING DQN RUN 83 with SEED 83 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.43 +/- 1.11\n",
      "Episode length: 6.67 +/- 3.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.35 +/- 1.05\n",
      "Episode length: 6.76 +/- 3.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.52 +/- 1.09\n",
      "Episode length: 7.16 +/- 3.09\n",
      "Eval num_timesteps=40000, episode_reward=-0.35 +/- 1.02\n",
      "Episode length: 6.84 +/- 3.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.29 +/- 1.09\n",
      "Episode length: 6.35 +/- 3.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.34 +/- 1.01\n",
      "Episode length: 6.82 +/- 3.17\n",
      "Eval num_timesteps=70000, episode_reward=-0.26 +/- 1.06\n",
      "Episode length: 6.36 +/- 3.12\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.48 +/- 1.02\n",
      "Episode length: 7.28 +/- 3.00\n",
      "Eval num_timesteps=90000, episode_reward=-0.61 +/- 1.05\n",
      "Episode length: 7.42 +/- 3.07\n",
      "Eval num_timesteps=100000, episode_reward=-0.28 +/- 1.05\n",
      "Episode length: 6.52 +/- 3.24\n",
      "Eval num_timesteps=110000, episode_reward=-0.31 +/- 1.06\n",
      "Episode length: 6.59 +/- 3.20\n",
      "Eval num_timesteps=120000, episode_reward=-0.29 +/- 0.99\n",
      "Episode length: 6.73 +/- 3.20\n",
      "Eval num_timesteps=130000, episode_reward=-0.58 +/- 0.98\n",
      "Episode length: 7.51 +/- 3.14\n",
      "Eval num_timesteps=140000, episode_reward=-0.28 +/- 1.04\n",
      "Episode length: 6.45 +/- 3.39\n",
      "Eval num_timesteps=150000, episode_reward=-0.40 +/- 1.02\n",
      "Episode length: 7.03 +/- 3.44\n",
      "Eval num_timesteps=160000, episode_reward=-0.37 +/- 1.03\n",
      "Episode length: 6.84 +/- 3.21\n",
      "Eval num_timesteps=170000, episode_reward=-0.41 +/- 1.01\n",
      "Episode length: 6.93 +/- 3.06\n",
      "Eval num_timesteps=180000, episode_reward=-0.36 +/- 1.02\n",
      "Episode length: 6.85 +/- 3.17\n",
      "Eval num_timesteps=190000, episode_reward=-0.53 +/- 1.00\n",
      "Episode length: 7.39 +/- 3.06\n",
      "Eval num_timesteps=200000, episode_reward=-0.25 +/- 0.97\n",
      "Episode length: 6.64 +/- 3.12\n",
      "New best mean reward!\n",
      "Evaluating best model for run 83...\n",
      "Run 83 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5000 (50.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.24\n",
      "  Average Episode Reward: -0.4495\n",
      "  Average Episode Length: 7.12\n",
      "===== COMPLETED DQN RUN 83 ====\n",
      "\n",
      "===== STARTING DQN RUN 84 with SEED 84 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.81 +/- 0.99\n",
      "Episode length: 7.90 +/- 3.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.32 +/- 1.06\n",
      "Episode length: 6.54 +/- 3.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.61 +/- 1.00\n",
      "Episode length: 7.68 +/- 3.03\n",
      "Eval num_timesteps=40000, episode_reward=-0.57 +/- 1.09\n",
      "Episode length: 7.21 +/- 3.19\n",
      "Eval num_timesteps=50000, episode_reward=-0.39 +/- 0.96\n",
      "Episode length: 7.06 +/- 2.92\n",
      "Eval num_timesteps=60000, episode_reward=-0.42 +/- 1.04\n",
      "Episode length: 7.03 +/- 2.99\n",
      "Eval num_timesteps=70000, episode_reward=-0.56 +/- 0.96\n",
      "Episode length: 7.69 +/- 2.91\n",
      "Eval num_timesteps=80000, episode_reward=-0.39 +/- 1.07\n",
      "Episode length: 6.82 +/- 3.29\n",
      "Eval num_timesteps=90000, episode_reward=-0.55 +/- 1.06\n",
      "Episode length: 7.27 +/- 3.06\n",
      "Eval num_timesteps=100000, episode_reward=-0.36 +/- 0.98\n",
      "Episode length: 6.96 +/- 2.89\n",
      "Eval num_timesteps=110000, episode_reward=-0.64 +/- 0.87\n",
      "Episode length: 8.22 +/- 2.58\n",
      "Eval num_timesteps=120000, episode_reward=-0.32 +/- 1.06\n",
      "Episode length: 6.41 +/- 3.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.29 +/- 1.11\n",
      "Episode length: 6.23 +/- 3.52\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-0.08 +/- 0.90\n",
      "Episode length: 6.10 +/- 3.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-0.55 +/- 0.90\n",
      "Episode length: 7.79 +/- 3.05\n",
      "Eval num_timesteps=160000, episode_reward=-0.31 +/- 1.07\n",
      "Episode length: 6.44 +/- 3.40\n",
      "Eval num_timesteps=170000, episode_reward=-0.33 +/- 0.99\n",
      "Episode length: 6.97 +/- 3.02\n",
      "Eval num_timesteps=180000, episode_reward=-0.50 +/- 1.06\n",
      "Episode length: 7.09 +/- 3.20\n",
      "Eval num_timesteps=190000, episode_reward=-0.43 +/- 0.99\n",
      "Episode length: 7.07 +/- 3.29\n",
      "Eval num_timesteps=200000, episode_reward=-0.34 +/- 1.06\n",
      "Episode length: 6.63 +/- 3.13\n",
      "Evaluating best model for run 84...\n",
      "Run 84 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5800 (58.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.50\n",
      "  Average Episode Reward: -0.3975\n",
      "  Average Episode Length: 6.81\n",
      "===== COMPLETED DQN RUN 84 ====\n",
      "\n",
      "===== STARTING DQN RUN 85 with SEED 85 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.43 +/- 1.06\n",
      "Episode length: 6.77 +/- 3.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.44 +/- 1.08\n",
      "Episode length: 6.91 +/- 3.24\n",
      "Eval num_timesteps=30000, episode_reward=-0.43 +/- 0.98\n",
      "Episode length: 7.24 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.72 +/- 1.06\n",
      "Episode length: 7.59 +/- 3.03\n",
      "Eval num_timesteps=50000, episode_reward=-0.52 +/- 1.04\n",
      "Episode length: 7.29 +/- 3.17\n",
      "Eval num_timesteps=60000, episode_reward=-0.51 +/- 1.04\n",
      "Episode length: 7.25 +/- 3.10\n",
      "Eval num_timesteps=70000, episode_reward=-0.62 +/- 1.06\n",
      "Episode length: 7.48 +/- 3.04\n",
      "Eval num_timesteps=80000, episode_reward=-0.56 +/- 1.05\n",
      "Episode length: 7.36 +/- 3.09\n",
      "Eval num_timesteps=90000, episode_reward=-0.77 +/- 1.02\n",
      "Episode length: 7.73 +/- 2.91\n",
      "Eval num_timesteps=100000, episode_reward=-0.35 +/- 1.01\n",
      "Episode length: 6.84 +/- 3.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-0.39 +/- 0.98\n",
      "Episode length: 7.19 +/- 2.98\n",
      "Eval num_timesteps=120000, episode_reward=-0.41 +/- 1.00\n",
      "Episode length: 7.13 +/- 3.15\n",
      "Eval num_timesteps=130000, episode_reward=-0.38 +/- 1.02\n",
      "Episode length: 6.93 +/- 3.22\n",
      "Eval num_timesteps=140000, episode_reward=-0.34 +/- 1.05\n",
      "Episode length: 6.76 +/- 3.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-0.28 +/- 1.00\n",
      "Episode length: 6.67 +/- 3.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-0.49 +/- 1.03\n",
      "Episode length: 7.27 +/- 2.86\n",
      "Eval num_timesteps=170000, episode_reward=-0.48 +/- 1.03\n",
      "Episode length: 7.23 +/- 3.19\n",
      "Eval num_timesteps=180000, episode_reward=-0.14 +/- 0.96\n",
      "Episode length: 6.34 +/- 3.10\n",
      "New best mean reward!\n",
      "Eval num_timesteps=190000, episode_reward=-0.32 +/- 0.98\n",
      "Episode length: 6.89 +/- 3.13\n",
      "Eval num_timesteps=200000, episode_reward=-0.45 +/- 1.05\n",
      "Episode length: 7.04 +/- 3.38\n",
      "Evaluating best model for run 85...\n",
      "Run 85 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5800 (58.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.26\n",
      "  Average Episode Reward: -0.3580\n",
      "  Average Episode Length: 6.67\n",
      "===== COMPLETED DQN RUN 85 ====\n",
      "\n",
      "===== STARTING DQN RUN 86 with SEED 86 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.43 +/- 1.00\n",
      "Episode length: 7.15 +/- 3.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.43 +/- 1.04\n",
      "Episode length: 7.07 +/- 2.89\n",
      "Eval num_timesteps=30000, episode_reward=-0.37 +/- 1.04\n",
      "Episode length: 6.98 +/- 3.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.40 +/- 0.98\n",
      "Episode length: 7.06 +/- 3.13\n",
      "Eval num_timesteps=50000, episode_reward=-0.42 +/- 1.09\n",
      "Episode length: 6.73 +/- 3.38\n",
      "Eval num_timesteps=60000, episode_reward=-0.70 +/- 1.03\n",
      "Episode length: 7.69 +/- 2.85\n",
      "Eval num_timesteps=70000, episode_reward=-0.48 +/- 0.96\n",
      "Episode length: 7.38 +/- 3.21\n",
      "Eval num_timesteps=80000, episode_reward=-0.41 +/- 0.89\n",
      "Episode length: 7.61 +/- 2.69\n",
      "Eval num_timesteps=90000, episode_reward=-0.45 +/- 0.96\n",
      "Episode length: 7.34 +/- 3.23\n",
      "Eval num_timesteps=100000, episode_reward=-0.45 +/- 0.99\n",
      "Episode length: 7.45 +/- 2.80\n",
      "Eval num_timesteps=110000, episode_reward=-0.58 +/- 0.96\n",
      "Episode length: 7.71 +/- 3.05\n",
      "Eval num_timesteps=120000, episode_reward=-0.49 +/- 0.86\n",
      "Episode length: 8.10 +/- 2.45\n",
      "Eval num_timesteps=130000, episode_reward=-0.53 +/- 0.94\n",
      "Episode length: 7.65 +/- 2.97\n",
      "Eval num_timesteps=140000, episode_reward=-0.32 +/- 1.04\n",
      "Episode length: 6.78 +/- 3.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=-0.47 +/- 0.99\n",
      "Episode length: 7.31 +/- 3.14\n",
      "Eval num_timesteps=160000, episode_reward=-0.27 +/- 0.97\n",
      "Episode length: 6.73 +/- 3.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=-0.41 +/- 0.93\n",
      "Episode length: 7.50 +/- 3.08\n",
      "Eval num_timesteps=180000, episode_reward=-0.55 +/- 0.95\n",
      "Episode length: 7.70 +/- 3.01\n",
      "Eval num_timesteps=190000, episode_reward=-0.44 +/- 0.99\n",
      "Episode length: 7.32 +/- 2.92\n",
      "Eval num_timesteps=200000, episode_reward=-0.47 +/- 0.97\n",
      "Episode length: 7.41 +/- 3.10\n",
      "Evaluating best model for run 86...\n",
      "Run 86 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5400 (54.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.96\n",
      "  Average Episode Reward: -0.4095\n",
      "  Average Episode Length: 7.28\n",
      "===== COMPLETED DQN RUN 86 ====\n",
      "\n",
      "===== STARTING DQN RUN 87 with SEED 87 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.37 +/- 1.04\n",
      "Episode length: 6.87 +/- 3.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.46 +/- 1.04\n",
      "Episode length: 6.86 +/- 3.21\n",
      "Eval num_timesteps=30000, episode_reward=-0.40 +/- 1.02\n",
      "Episode length: 6.97 +/- 3.26\n",
      "Eval num_timesteps=40000, episode_reward=-0.57 +/- 0.88\n",
      "Episode length: 8.11 +/- 2.47\n",
      "Eval num_timesteps=50000, episode_reward=-0.38 +/- 1.03\n",
      "Episode length: 6.73 +/- 3.34\n",
      "Eval num_timesteps=60000, episode_reward=-0.51 +/- 0.96\n",
      "Episode length: 7.61 +/- 2.82\n",
      "Eval num_timesteps=70000, episode_reward=-0.53 +/- 0.99\n",
      "Episode length: 7.28 +/- 3.39\n",
      "Eval num_timesteps=80000, episode_reward=-0.57 +/- 0.91\n",
      "Episode length: 7.87 +/- 2.87\n",
      "Eval num_timesteps=90000, episode_reward=-0.49 +/- 0.94\n",
      "Episode length: 7.59 +/- 3.00\n",
      "Eval num_timesteps=100000, episode_reward=-0.49 +/- 0.95\n",
      "Episode length: 7.54 +/- 2.86\n",
      "Eval num_timesteps=110000, episode_reward=-0.42 +/- 1.00\n",
      "Episode length: 7.12 +/- 2.97\n",
      "Eval num_timesteps=120000, episode_reward=-0.30 +/- 0.96\n",
      "Episode length: 6.95 +/- 2.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=-0.43 +/- 1.03\n",
      "Episode length: 7.18 +/- 2.88\n",
      "Eval num_timesteps=140000, episode_reward=-0.59 +/- 0.95\n",
      "Episode length: 7.95 +/- 2.53\n",
      "Eval num_timesteps=150000, episode_reward=-0.47 +/- 1.03\n",
      "Episode length: 7.10 +/- 3.38\n",
      "Eval num_timesteps=160000, episode_reward=-0.27 +/- 0.94\n",
      "Episode length: 7.03 +/- 2.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=-0.29 +/- 0.97\n",
      "Episode length: 6.97 +/- 2.77\n",
      "Eval num_timesteps=180000, episode_reward=-0.34 +/- 1.03\n",
      "Episode length: 6.78 +/- 3.34\n",
      "Eval num_timesteps=190000, episode_reward=-0.37 +/- 1.02\n",
      "Episode length: 6.89 +/- 3.22\n",
      "Eval num_timesteps=200000, episode_reward=-0.44 +/- 0.98\n",
      "Episode length: 7.46 +/- 2.93\n",
      "Evaluating best model for run 87...\n",
      "Run 87 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6200 (62.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.16\n",
      "  Average Episode Reward: -0.3370\n",
      "  Average Episode Length: 7.00\n",
      "===== COMPLETED DQN RUN 87 ====\n",
      "\n",
      "===== STARTING DQN RUN 88 with SEED 88 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.34 +/- 1.02\n",
      "Episode length: 6.68 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.66 +/- 1.00\n",
      "Episode length: 7.86 +/- 2.81\n",
      "Eval num_timesteps=30000, episode_reward=-0.30 +/- 1.02\n",
      "Episode length: 6.77 +/- 3.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.78 +/- 0.97\n",
      "Episode length: 8.28 +/- 2.56\n",
      "Eval num_timesteps=50000, episode_reward=-0.25 +/- 1.03\n",
      "Episode length: 6.63 +/- 2.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.59 +/- 0.97\n",
      "Episode length: 7.88 +/- 2.87\n",
      "Eval num_timesteps=70000, episode_reward=-0.54 +/- 1.04\n",
      "Episode length: 7.21 +/- 3.26\n",
      "Eval num_timesteps=80000, episode_reward=-0.23 +/- 0.97\n",
      "Episode length: 6.71 +/- 3.12\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-0.42 +/- 1.03\n",
      "Episode length: 7.14 +/- 3.14\n",
      "Eval num_timesteps=100000, episode_reward=-0.26 +/- 1.02\n",
      "Episode length: 6.41 +/- 3.31\n",
      "Eval num_timesteps=110000, episode_reward=-0.37 +/- 1.02\n",
      "Episode length: 6.81 +/- 3.45\n",
      "Eval num_timesteps=120000, episode_reward=-0.51 +/- 0.98\n",
      "Episode length: 7.61 +/- 2.85\n",
      "Eval num_timesteps=130000, episode_reward=-0.53 +/- 0.95\n",
      "Episode length: 7.66 +/- 3.04\n",
      "Eval num_timesteps=140000, episode_reward=-0.35 +/- 1.05\n",
      "Episode length: 6.88 +/- 3.21\n",
      "Eval num_timesteps=150000, episode_reward=-0.42 +/- 0.90\n",
      "Episode length: 7.54 +/- 2.79\n",
      "Eval num_timesteps=160000, episode_reward=-0.41 +/- 0.96\n",
      "Episode length: 7.27 +/- 2.94\n",
      "Eval num_timesteps=170000, episode_reward=-0.57 +/- 0.93\n",
      "Episode length: 7.71 +/- 2.82\n",
      "Eval num_timesteps=180000, episode_reward=-0.37 +/- 1.00\n",
      "Episode length: 7.07 +/- 3.14\n",
      "Eval num_timesteps=190000, episode_reward=-0.35 +/- 1.03\n",
      "Episode length: 6.71 +/- 3.22\n",
      "Eval num_timesteps=200000, episode_reward=-0.49 +/- 1.02\n",
      "Episode length: 7.29 +/- 3.05\n",
      "Evaluating best model for run 88...\n",
      "Run 88 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5600 (56.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.68\n",
      "  Average Episode Reward: -0.4835\n",
      "  Average Episode Length: 7.58\n",
      "===== COMPLETED DQN RUN 88 ====\n",
      "\n",
      "===== STARTING DQN RUN 89 with SEED 89 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.46 +/- 1.22\n",
      "Episode length: 6.34 +/- 3.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.51 +/- 1.01\n",
      "Episode length: 7.34 +/- 2.85\n",
      "Eval num_timesteps=30000, episode_reward=-0.35 +/- 1.05\n",
      "Episode length: 6.66 +/- 3.31\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.37 +/- 0.98\n",
      "Episode length: 7.13 +/- 2.87\n",
      "Eval num_timesteps=50000, episode_reward=-0.39 +/- 0.95\n",
      "Episode length: 7.13 +/- 3.31\n",
      "Eval num_timesteps=60000, episode_reward=-0.41 +/- 0.99\n",
      "Episode length: 7.14 +/- 3.15\n",
      "Eval num_timesteps=70000, episode_reward=-0.46 +/- 0.98\n",
      "Episode length: 7.38 +/- 3.18\n",
      "Eval num_timesteps=80000, episode_reward=-0.47 +/- 0.96\n",
      "Episode length: 7.44 +/- 3.20\n",
      "Eval num_timesteps=90000, episode_reward=-0.46 +/- 0.96\n",
      "Episode length: 7.41 +/- 3.06\n",
      "Eval num_timesteps=100000, episode_reward=-0.54 +/- 1.00\n",
      "Episode length: 7.64 +/- 3.05\n",
      "Eval num_timesteps=110000, episode_reward=-0.31 +/- 1.04\n",
      "Episode length: 6.80 +/- 2.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.36 +/- 0.97\n",
      "Episode length: 7.11 +/- 2.90\n",
      "Eval num_timesteps=130000, episode_reward=-0.55 +/- 0.94\n",
      "Episode length: 7.72 +/- 2.98\n",
      "Eval num_timesteps=140000, episode_reward=-0.54 +/- 0.96\n",
      "Episode length: 7.57 +/- 3.10\n",
      "Eval num_timesteps=150000, episode_reward=-0.53 +/- 0.98\n",
      "Episode length: 7.49 +/- 3.17\n",
      "Eval num_timesteps=160000, episode_reward=-0.28 +/- 0.98\n",
      "Episode length: 6.75 +/- 2.88\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=-0.52 +/- 0.95\n",
      "Episode length: 7.82 +/- 2.64\n",
      "Eval num_timesteps=180000, episode_reward=-0.29 +/- 1.00\n",
      "Episode length: 6.47 +/- 3.27\n",
      "Eval num_timesteps=190000, episode_reward=-0.33 +/- 1.08\n",
      "Episode length: 6.51 +/- 3.30\n",
      "Eval num_timesteps=200000, episode_reward=-0.33 +/- 1.02\n",
      "Episode length: 6.75 +/- 3.28\n",
      "Evaluating best model for run 89...\n",
      "Run 89 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5300 (53.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.89\n",
      "  Average Episode Reward: -0.4930\n",
      "  Average Episode Length: 7.29\n",
      "===== COMPLETED DQN RUN 89 ====\n",
      "\n",
      "===== STARTING DQN RUN 90 with SEED 90 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.60 +/- 1.07\n",
      "Episode length: 7.47 +/- 3.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.31 +/- 0.86\n",
      "Episode length: 7.57 +/- 2.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.60 +/- 1.03\n",
      "Episode length: 7.59 +/- 3.06\n",
      "Eval num_timesteps=40000, episode_reward=-0.20 +/- 0.90\n",
      "Episode length: 6.79 +/- 3.10\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.53 +/- 1.05\n",
      "Episode length: 7.28 +/- 2.95\n",
      "Eval num_timesteps=60000, episode_reward=-0.45 +/- 1.11\n",
      "Episode length: 6.78 +/- 3.16\n",
      "Eval num_timesteps=70000, episode_reward=-0.48 +/- 1.06\n",
      "Episode length: 7.15 +/- 3.09\n",
      "Eval num_timesteps=80000, episode_reward=-0.56 +/- 0.95\n",
      "Episode length: 7.72 +/- 3.02\n",
      "Eval num_timesteps=90000, episode_reward=-0.67 +/- 0.96\n",
      "Episode length: 7.88 +/- 2.94\n",
      "Eval num_timesteps=100000, episode_reward=-0.50 +/- 0.97\n",
      "Episode length: 7.45 +/- 3.12\n",
      "Eval num_timesteps=110000, episode_reward=-0.25 +/- 0.94\n",
      "Episode length: 6.87 +/- 3.24\n",
      "Eval num_timesteps=120000, episode_reward=-0.45 +/- 1.01\n",
      "Episode length: 7.08 +/- 3.14\n",
      "Eval num_timesteps=130000, episode_reward=-0.30 +/- 0.95\n",
      "Episode length: 6.86 +/- 2.85\n",
      "Eval num_timesteps=140000, episode_reward=-0.48 +/- 0.93\n",
      "Episode length: 7.51 +/- 2.97\n",
      "Eval num_timesteps=150000, episode_reward=-0.26 +/- 1.04\n",
      "Episode length: 6.55 +/- 3.11\n",
      "Eval num_timesteps=160000, episode_reward=-0.46 +/- 0.98\n",
      "Episode length: 7.33 +/- 3.03\n",
      "Eval num_timesteps=170000, episode_reward=-0.41 +/- 0.95\n",
      "Episode length: 7.32 +/- 2.90\n",
      "Eval num_timesteps=180000, episode_reward=-0.28 +/- 0.97\n",
      "Episode length: 6.94 +/- 3.10\n",
      "Eval num_timesteps=190000, episode_reward=-0.41 +/- 1.01\n",
      "Episode length: 7.11 +/- 3.39\n",
      "Eval num_timesteps=200000, episode_reward=-0.33 +/- 0.94\n",
      "Episode length: 7.06 +/- 3.00\n",
      "Evaluating best model for run 90...\n",
      "Run 90 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.4800 (48.00%)\n",
      "  Average Detection Time (Successful Episodes): 6.27\n",
      "  Average Episode Reward: -0.6880\n",
      "  Average Episode Length: 8.21\n",
      "===== COMPLETED DQN RUN 90 ====\n",
      "\n",
      "===== STARTING DQN RUN 91 with SEED 91 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.80 +/- 1.05\n",
      "Episode length: 7.48 +/- 3.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.75 +/- 0.96\n",
      "Episode length: 8.21 +/- 2.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.47 +/- 0.93\n",
      "Episode length: 7.27 +/- 3.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.33 +/- 0.99\n",
      "Episode length: 6.75 +/- 3.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.42 +/- 1.01\n",
      "Episode length: 6.95 +/- 3.18\n",
      "Eval num_timesteps=60000, episode_reward=-0.32 +/- 1.02\n",
      "Episode length: 6.78 +/- 3.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.52 +/- 1.06\n",
      "Episode length: 7.00 +/- 3.30\n",
      "Eval num_timesteps=80000, episode_reward=-0.43 +/- 1.04\n",
      "Episode length: 7.08 +/- 3.34\n",
      "Eval num_timesteps=90000, episode_reward=-0.51 +/- 1.00\n",
      "Episode length: 7.32 +/- 3.15\n",
      "Eval num_timesteps=100000, episode_reward=-0.28 +/- 1.00\n",
      "Episode length: 6.34 +/- 3.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-0.35 +/- 1.00\n",
      "Episode length: 6.76 +/- 3.40\n",
      "Eval num_timesteps=120000, episode_reward=-0.42 +/- 0.96\n",
      "Episode length: 7.18 +/- 3.12\n",
      "Eval num_timesteps=130000, episode_reward=-0.58 +/- 0.96\n",
      "Episode length: 7.69 +/- 2.95\n",
      "Eval num_timesteps=140000, episode_reward=-0.53 +/- 0.91\n",
      "Episode length: 7.65 +/- 3.31\n",
      "Eval num_timesteps=150000, episode_reward=-0.44 +/- 1.09\n",
      "Episode length: 6.68 +/- 3.43\n",
      "Eval num_timesteps=160000, episode_reward=-0.50 +/- 1.04\n",
      "Episode length: 7.09 +/- 3.28\n",
      "Eval num_timesteps=170000, episode_reward=-0.29 +/- 1.03\n",
      "Episode length: 6.40 +/- 3.46\n",
      "Eval num_timesteps=180000, episode_reward=-0.29 +/- 1.07\n",
      "Episode length: 6.36 +/- 3.45\n",
      "Eval num_timesteps=190000, episode_reward=-0.52 +/- 1.02\n",
      "Episode length: 7.29 +/- 3.07\n",
      "Eval num_timesteps=200000, episode_reward=-0.50 +/- 1.02\n",
      "Episode length: 7.30 +/- 3.15\n",
      "Evaluating best model for run 91...\n",
      "Run 91 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5600 (56.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.11\n",
      "  Average Episode Reward: -0.5115\n",
      "  Average Episode Length: 7.26\n",
      "===== COMPLETED DQN RUN 91 ====\n",
      "\n",
      "===== STARTING DQN RUN 92 with SEED 92 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.54 +/- 1.11\n",
      "Episode length: 7.00 +/- 3.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.58 +/- 1.02\n",
      "Episode length: 7.43 +/- 3.17\n",
      "Eval num_timesteps=30000, episode_reward=-0.34 +/- 1.10\n",
      "Episode length: 6.25 +/- 3.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.55 +/- 0.97\n",
      "Episode length: 7.49 +/- 3.01\n",
      "Eval num_timesteps=50000, episode_reward=-0.36 +/- 1.01\n",
      "Episode length: 6.80 +/- 3.45\n",
      "Eval num_timesteps=60000, episode_reward=-0.51 +/- 0.95\n",
      "Episode length: 7.31 +/- 3.18\n",
      "Eval num_timesteps=70000, episode_reward=-0.42 +/- 1.01\n",
      "Episode length: 6.92 +/- 3.39\n",
      "Eval num_timesteps=80000, episode_reward=-0.31 +/- 1.02\n",
      "Episode length: 6.69 +/- 3.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-0.43 +/- 1.03\n",
      "Episode length: 6.98 +/- 3.07\n",
      "Eval num_timesteps=100000, episode_reward=-0.49 +/- 0.99\n",
      "Episode length: 7.28 +/- 3.10\n",
      "Eval num_timesteps=110000, episode_reward=-0.45 +/- 1.01\n",
      "Episode length: 7.13 +/- 3.21\n",
      "Eval num_timesteps=120000, episode_reward=-0.43 +/- 1.05\n",
      "Episode length: 7.02 +/- 3.18\n",
      "Eval num_timesteps=130000, episode_reward=-0.58 +/- 0.92\n",
      "Episode length: 7.75 +/- 2.93\n",
      "Eval num_timesteps=140000, episode_reward=-0.32 +/- 1.05\n",
      "Episode length: 6.47 +/- 3.40\n",
      "Eval num_timesteps=150000, episode_reward=-0.40 +/- 1.02\n",
      "Episode length: 7.09 +/- 3.19\n",
      "Eval num_timesteps=160000, episode_reward=-0.36 +/- 1.01\n",
      "Episode length: 6.93 +/- 3.28\n",
      "Eval num_timesteps=170000, episode_reward=-0.34 +/- 0.96\n",
      "Episode length: 7.13 +/- 3.08\n",
      "Eval num_timesteps=180000, episode_reward=-0.54 +/- 0.94\n",
      "Episode length: 7.71 +/- 2.93\n",
      "Eval num_timesteps=190000, episode_reward=-0.46 +/- 1.04\n",
      "Episode length: 7.31 +/- 3.10\n",
      "Eval num_timesteps=200000, episode_reward=-0.36 +/- 0.96\n",
      "Episode length: 7.17 +/- 3.09\n",
      "Evaluating best model for run 92...\n",
      "Run 92 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5700 (57.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.67\n",
      "  Average Episode Reward: -0.3970\n",
      "  Average Episode Length: 6.96\n",
      "===== COMPLETED DQN RUN 92 ====\n",
      "\n",
      "===== STARTING DQN RUN 93 with SEED 93 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.55 +/- 1.13\n",
      "Episode length: 6.97 +/- 3.31\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.44 +/- 1.03\n",
      "Episode length: 7.15 +/- 2.97\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.28 +/- 1.00\n",
      "Episode length: 6.85 +/- 3.31\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.30 +/- 1.00\n",
      "Episode length: 6.84 +/- 3.37\n",
      "Eval num_timesteps=50000, episode_reward=-0.58 +/- 1.08\n",
      "Episode length: 7.22 +/- 3.24\n",
      "Eval num_timesteps=60000, episode_reward=-0.43 +/- 1.07\n",
      "Episode length: 6.89 +/- 3.43\n",
      "Eval num_timesteps=70000, episode_reward=-0.50 +/- 1.06\n",
      "Episode length: 7.16 +/- 3.26\n",
      "Eval num_timesteps=80000, episode_reward=-0.59 +/- 0.89\n",
      "Episode length: 7.86 +/- 3.08\n",
      "Eval num_timesteps=90000, episode_reward=-0.44 +/- 0.92\n",
      "Episode length: 7.63 +/- 2.96\n",
      "Eval num_timesteps=100000, episode_reward=-0.31 +/- 1.02\n",
      "Episode length: 6.77 +/- 3.55\n",
      "Eval num_timesteps=110000, episode_reward=-0.40 +/- 0.93\n",
      "Episode length: 7.23 +/- 3.15\n",
      "Eval num_timesteps=120000, episode_reward=-0.65 +/- 1.11\n",
      "Episode length: 7.12 +/- 3.19\n",
      "Eval num_timesteps=130000, episode_reward=-0.26 +/- 0.86\n",
      "Episode length: 7.31 +/- 2.74\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-0.51 +/- 0.93\n",
      "Episode length: 7.66 +/- 3.08\n",
      "Eval num_timesteps=150000, episode_reward=-0.50 +/- 0.96\n",
      "Episode length: 7.59 +/- 3.13\n",
      "Eval num_timesteps=160000, episode_reward=-0.44 +/- 0.96\n",
      "Episode length: 7.26 +/- 3.14\n",
      "Eval num_timesteps=170000, episode_reward=-0.34 +/- 0.94\n",
      "Episode length: 7.11 +/- 3.21\n",
      "Eval num_timesteps=180000, episode_reward=-0.21 +/- 0.94\n",
      "Episode length: 6.55 +/- 3.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=190000, episode_reward=-0.34 +/- 0.94\n",
      "Episode length: 7.01 +/- 3.16\n",
      "Eval num_timesteps=200000, episode_reward=-0.49 +/- 0.95\n",
      "Episode length: 7.39 +/- 3.20\n",
      "Evaluating best model for run 93...\n",
      "Run 93 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6300 (63.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.97\n",
      "  Average Episode Reward: -0.2825\n",
      "  Average Episode Length: 6.83\n",
      "===== COMPLETED DQN RUN 93 ====\n",
      "\n",
      "===== STARTING DQN RUN 94 with SEED 94 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.55 +/- 1.16\n",
      "Episode length: 6.79 +/- 3.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.44 +/- 1.21\n",
      "Episode length: 6.34 +/- 3.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-0.50 +/- 1.06\n",
      "Episode length: 6.99 +/- 3.22\n",
      "Eval num_timesteps=40000, episode_reward=-0.36 +/- 1.02\n",
      "Episode length: 6.75 +/- 3.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.50 +/- 1.01\n",
      "Episode length: 7.23 +/- 3.00\n",
      "Eval num_timesteps=60000, episode_reward=-0.55 +/- 0.98\n",
      "Episode length: 7.66 +/- 2.51\n",
      "Eval num_timesteps=70000, episode_reward=-0.40 +/- 1.02\n",
      "Episode length: 6.93 +/- 3.13\n",
      "Eval num_timesteps=80000, episode_reward=-0.34 +/- 0.96\n",
      "Episode length: 6.98 +/- 2.72\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-0.43 +/- 1.11\n",
      "Episode length: 6.91 +/- 3.15\n",
      "Eval num_timesteps=100000, episode_reward=-0.38 +/- 0.99\n",
      "Episode length: 7.34 +/- 2.84\n",
      "Eval num_timesteps=110000, episode_reward=-0.90 +/- 0.52\n",
      "Episode length: 9.45 +/- 1.76\n",
      "Eval num_timesteps=120000, episode_reward=-0.43 +/- 0.97\n",
      "Episode length: 7.43 +/- 2.96\n",
      "Eval num_timesteps=130000, episode_reward=-0.57 +/- 0.91\n",
      "Episode length: 8.07 +/- 2.52\n",
      "Eval num_timesteps=140000, episode_reward=-0.39 +/- 0.94\n",
      "Episode length: 7.39 +/- 2.80\n",
      "Eval num_timesteps=150000, episode_reward=-0.36 +/- 0.92\n",
      "Episode length: 7.34 +/- 2.78\n",
      "Eval num_timesteps=160000, episode_reward=-0.45 +/- 0.98\n",
      "Episode length: 7.26 +/- 3.14\n",
      "Eval num_timesteps=170000, episode_reward=-0.29 +/- 0.95\n",
      "Episode length: 7.04 +/- 3.02\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-0.56 +/- 0.98\n",
      "Episode length: 7.90 +/- 2.46\n",
      "Eval num_timesteps=190000, episode_reward=-0.41 +/- 0.90\n",
      "Episode length: 7.46 +/- 2.87\n",
      "Eval num_timesteps=200000, episode_reward=-0.40 +/- 1.01\n",
      "Episode length: 7.00 +/- 2.96\n",
      "Evaluating best model for run 94...\n",
      "Run 94 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5200 (52.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.96\n",
      "  Average Episode Reward: -0.4215\n",
      "  Average Episode Length: 7.38\n",
      "===== COMPLETED DQN RUN 94 ====\n",
      "\n",
      "===== STARTING DQN RUN 95 with SEED 95 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.52 +/- 1.05\n",
      "Episode length: 7.16 +/- 3.10\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.70 +/- 1.03\n",
      "Episode length: 7.82 +/- 2.74\n",
      "Eval num_timesteps=30000, episode_reward=-0.25 +/- 0.95\n",
      "Episode length: 6.72 +/- 3.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.36 +/- 1.03\n",
      "Episode length: 6.86 +/- 3.06\n",
      "Eval num_timesteps=50000, episode_reward=-0.40 +/- 1.00\n",
      "Episode length: 7.07 +/- 3.05\n",
      "Eval num_timesteps=60000, episode_reward=-0.50 +/- 1.07\n",
      "Episode length: 7.22 +/- 3.02\n",
      "Eval num_timesteps=70000, episode_reward=-0.71 +/- 1.03\n",
      "Episode length: 7.75 +/- 3.01\n",
      "Eval num_timesteps=80000, episode_reward=-0.39 +/- 0.94\n",
      "Episode length: 7.31 +/- 2.95\n",
      "Eval num_timesteps=90000, episode_reward=-0.52 +/- 1.03\n",
      "Episode length: 7.34 +/- 3.09\n",
      "Eval num_timesteps=100000, episode_reward=-0.43 +/- 1.01\n",
      "Episode length: 7.08 +/- 3.06\n",
      "Eval num_timesteps=110000, episode_reward=-0.56 +/- 0.88\n",
      "Episode length: 8.01 +/- 2.75\n",
      "Eval num_timesteps=120000, episode_reward=-0.43 +/- 1.00\n",
      "Episode length: 7.15 +/- 2.90\n",
      "Eval num_timesteps=130000, episode_reward=-0.32 +/- 1.01\n",
      "Episode length: 6.76 +/- 3.44\n",
      "Eval num_timesteps=140000, episode_reward=-0.40 +/- 1.05\n",
      "Episode length: 6.95 +/- 2.77\n",
      "Eval num_timesteps=150000, episode_reward=-0.45 +/- 0.99\n",
      "Episode length: 7.30 +/- 2.99\n",
      "Eval num_timesteps=160000, episode_reward=-0.49 +/- 0.96\n",
      "Episode length: 7.45 +/- 2.87\n",
      "Eval num_timesteps=170000, episode_reward=-0.28 +/- 0.89\n",
      "Episode length: 7.04 +/- 3.20\n",
      "Eval num_timesteps=180000, episode_reward=-0.38 +/- 0.98\n",
      "Episode length: 7.10 +/- 2.87\n",
      "Eval num_timesteps=190000, episode_reward=-0.31 +/- 0.95\n",
      "Episode length: 6.99 +/- 3.32\n",
      "Eval num_timesteps=200000, episode_reward=-0.52 +/- 1.04\n",
      "Episode length: 7.26 +/- 3.13\n",
      "Evaluating best model for run 95...\n",
      "Run 95 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5700 (57.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.19\n",
      "  Average Episode Reward: -0.2910\n",
      "  Average Episode Length: 6.69\n",
      "===== COMPLETED DQN RUN 95 ====\n",
      "\n",
      "===== STARTING DQN RUN 96 with SEED 96 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.58 +/- 1.10\n",
      "Episode length: 6.85 +/- 3.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.70 +/- 1.06\n",
      "Episode length: 7.49 +/- 3.25\n",
      "Eval num_timesteps=30000, episode_reward=-0.47 +/- 0.98\n",
      "Episode length: 7.30 +/- 2.92\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.45 +/- 1.09\n",
      "Episode length: 6.63 +/- 3.25\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.56 +/- 0.93\n",
      "Episode length: 7.71 +/- 2.79\n",
      "Eval num_timesteps=60000, episode_reward=-0.56 +/- 1.08\n",
      "Episode length: 7.11 +/- 3.16\n",
      "Eval num_timesteps=70000, episode_reward=-0.25 +/- 0.94\n",
      "Episode length: 6.80 +/- 2.95\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.53 +/- 0.96\n",
      "Episode length: 7.47 +/- 2.95\n",
      "Eval num_timesteps=90000, episode_reward=-0.44 +/- 1.01\n",
      "Episode length: 7.11 +/- 2.87\n",
      "Eval num_timesteps=100000, episode_reward=-0.37 +/- 1.05\n",
      "Episode length: 6.54 +/- 3.28\n",
      "Eval num_timesteps=110000, episode_reward=-0.32 +/- 1.02\n",
      "Episode length: 6.45 +/- 3.29\n",
      "Eval num_timesteps=120000, episode_reward=-0.48 +/- 1.00\n",
      "Episode length: 7.07 +/- 3.07\n",
      "Eval num_timesteps=130000, episode_reward=-0.49 +/- 0.96\n",
      "Episode length: 7.22 +/- 2.85\n",
      "Eval num_timesteps=140000, episode_reward=-0.25 +/- 1.06\n",
      "Episode length: 6.25 +/- 3.58\n",
      "Eval num_timesteps=150000, episode_reward=-0.55 +/- 0.99\n",
      "Episode length: 7.41 +/- 3.22\n",
      "Eval num_timesteps=160000, episode_reward=-0.53 +/- 1.01\n",
      "Episode length: 7.32 +/- 3.35\n",
      "Eval num_timesteps=170000, episode_reward=-0.62 +/- 0.98\n",
      "Episode length: 7.57 +/- 2.89\n",
      "Eval num_timesteps=180000, episode_reward=-0.41 +/- 1.05\n",
      "Episode length: 6.77 +/- 3.29\n",
      "Eval num_timesteps=190000, episode_reward=-0.32 +/- 1.00\n",
      "Episode length: 6.66 +/- 3.19\n",
      "Eval num_timesteps=200000, episode_reward=-0.29 +/- 1.07\n",
      "Episode length: 6.39 +/- 3.31\n",
      "Evaluating best model for run 96...\n",
      "Run 96 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.5800 (58.00%)\n",
      "  Average Detection Time (Successful Episodes): 5.14\n",
      "  Average Episode Reward: -0.4295\n",
      "  Average Episode Length: 7.18\n",
      "===== COMPLETED DQN RUN 96 ====\n",
      "\n",
      "===== STARTING DQN RUN 97 with SEED 97 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.46 +/- 1.13\n",
      "Episode length: 6.78 +/- 3.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.55 +/- 1.12\n",
      "Episode length: 7.12 +/- 2.99\n",
      "Eval num_timesteps=30000, episode_reward=-0.43 +/- 1.08\n",
      "Episode length: 6.83 +/- 3.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.14 +/- 0.99\n",
      "Episode length: 6.15 +/- 2.98\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.52 +/- 1.02\n",
      "Episode length: 7.25 +/- 2.89\n",
      "Eval num_timesteps=60000, episode_reward=-0.53 +/- 1.00\n",
      "Episode length: 7.47 +/- 3.03\n",
      "Eval num_timesteps=70000, episode_reward=-0.56 +/- 1.00\n",
      "Episode length: 7.39 +/- 2.80\n",
      "Eval num_timesteps=80000, episode_reward=-0.38 +/- 1.10\n",
      "Episode length: 6.55 +/- 3.45\n",
      "Eval num_timesteps=90000, episode_reward=-0.53 +/- 1.04\n",
      "Episode length: 7.27 +/- 3.24\n",
      "Eval num_timesteps=100000, episode_reward=-0.42 +/- 1.05\n",
      "Episode length: 6.94 +/- 3.01\n",
      "Eval num_timesteps=110000, episode_reward=-0.30 +/- 1.08\n",
      "Episode length: 6.53 +/- 3.56\n",
      "Eval num_timesteps=120000, episode_reward=-0.53 +/- 0.99\n",
      "Episode length: 7.56 +/- 3.21\n",
      "Eval num_timesteps=130000, episode_reward=-0.53 +/- 1.03\n",
      "Episode length: 7.27 +/- 3.15\n",
      "Eval num_timesteps=140000, episode_reward=-0.33 +/- 1.03\n",
      "Episode length: 6.65 +/- 3.24\n",
      "Eval num_timesteps=150000, episode_reward=-0.49 +/- 1.02\n",
      "Episode length: 7.07 +/- 3.09\n",
      "Eval num_timesteps=160000, episode_reward=-0.37 +/- 1.04\n",
      "Episode length: 6.84 +/- 2.85\n",
      "Eval num_timesteps=170000, episode_reward=-0.43 +/- 0.96\n",
      "Episode length: 7.17 +/- 3.11\n",
      "Eval num_timesteps=180000, episode_reward=-0.38 +/- 1.04\n",
      "Episode length: 7.12 +/- 3.07\n",
      "Eval num_timesteps=190000, episode_reward=-0.44 +/- 1.00\n",
      "Episode length: 7.40 +/- 2.89\n",
      "Eval num_timesteps=200000, episode_reward=-0.65 +/- 1.01\n",
      "Episode length: 7.87 +/- 3.11\n",
      "Evaluating best model for run 97...\n",
      "Run 97 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6600 (66.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.92\n",
      "  Average Episode Reward: -0.3725\n",
      "  Average Episode Length: 6.65\n",
      "===== COMPLETED DQN RUN 97 ====\n",
      "\n",
      "===== STARTING DQN RUN 98 with SEED 98 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.43 +/- 1.09\n",
      "Episode length: 6.60 +/- 3.41\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.55 +/- 1.12\n",
      "Episode length: 7.07 +/- 3.15\n",
      "Eval num_timesteps=30000, episode_reward=-0.54 +/- 1.00\n",
      "Episode length: 7.51 +/- 2.81\n",
      "Eval num_timesteps=40000, episode_reward=-0.40 +/- 1.04\n",
      "Episode length: 6.79 +/- 3.14\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.37 +/- 1.04\n",
      "Episode length: 6.78 +/- 3.19\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.59 +/- 1.07\n",
      "Episode length: 7.34 +/- 3.39\n",
      "Eval num_timesteps=70000, episode_reward=-0.30 +/- 0.99\n",
      "Episode length: 6.70 +/- 3.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.16 +/- 1.02\n",
      "Episode length: 5.96 +/- 3.29\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-0.30 +/- 1.04\n",
      "Episode length: 6.48 +/- 3.45\n",
      "Eval num_timesteps=100000, episode_reward=-0.31 +/- 1.03\n",
      "Episode length: 6.59 +/- 3.34\n",
      "Eval num_timesteps=110000, episode_reward=-0.57 +/- 1.06\n",
      "Episode length: 7.24 +/- 2.97\n",
      "Eval num_timesteps=120000, episode_reward=-0.21 +/- 1.01\n",
      "Episode length: 6.35 +/- 3.30\n",
      "Eval num_timesteps=130000, episode_reward=-0.26 +/- 0.99\n",
      "Episode length: 6.63 +/- 3.32\n",
      "Eval num_timesteps=140000, episode_reward=-0.41 +/- 1.09\n",
      "Episode length: 6.68 +/- 3.53\n",
      "Eval num_timesteps=150000, episode_reward=-0.22 +/- 0.96\n",
      "Episode length: 6.59 +/- 3.10\n",
      "Eval num_timesteps=160000, episode_reward=-0.32 +/- 1.01\n",
      "Episode length: 6.61 +/- 3.21\n",
      "Eval num_timesteps=170000, episode_reward=-0.56 +/- 0.96\n",
      "Episode length: 7.62 +/- 2.85\n",
      "Eval num_timesteps=180000, episode_reward=-0.53 +/- 1.01\n",
      "Episode length: 7.48 +/- 2.95\n",
      "Eval num_timesteps=190000, episode_reward=-0.49 +/- 0.99\n",
      "Episode length: 7.33 +/- 3.11\n",
      "Eval num_timesteps=200000, episode_reward=-0.24 +/- 1.06\n",
      "Episode length: 6.21 +/- 3.60\n",
      "Evaluating best model for run 98...\n",
      "Run 98 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6000 (60.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.07\n",
      "  Average Episode Reward: -0.3570\n",
      "  Average Episode Length: 6.44\n",
      "===== COMPLETED DQN RUN 98 ====\n",
      "\n",
      "===== STARTING DQN RUN 99 with SEED 99 ====\n",
      "Eval num_timesteps=10000, episode_reward=-0.58 +/- 1.01\n",
      "Episode length: 7.49 +/- 3.10\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.58 +/- 1.03\n",
      "Episode length: 7.49 +/- 2.85\n",
      "Eval num_timesteps=30000, episode_reward=-0.55 +/- 0.98\n",
      "Episode length: 7.43 +/- 2.87\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.42 +/- 1.03\n",
      "Episode length: 6.94 +/- 3.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-0.45 +/- 0.99\n",
      "Episode length: 7.12 +/- 3.12\n",
      "Eval num_timesteps=60000, episode_reward=-0.39 +/- 1.02\n",
      "Episode length: 6.79 +/- 3.37\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-0.60 +/- 1.03\n",
      "Episode length: 7.33 +/- 2.97\n",
      "Eval num_timesteps=80000, episode_reward=-0.45 +/- 1.02\n",
      "Episode length: 6.99 +/- 3.12\n",
      "Eval num_timesteps=90000, episode_reward=-0.26 +/- 0.99\n",
      "Episode length: 6.42 +/- 2.96\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.52 +/- 1.09\n",
      "Episode length: 6.95 +/- 3.08\n",
      "Eval num_timesteps=110000, episode_reward=-0.43 +/- 1.01\n",
      "Episode length: 6.99 +/- 3.17\n",
      "Eval num_timesteps=120000, episode_reward=-0.38 +/- 1.11\n",
      "Episode length: 6.64 +/- 3.27\n",
      "Eval num_timesteps=130000, episode_reward=-0.52 +/- 1.02\n",
      "Episode length: 7.27 +/- 3.18\n",
      "Eval num_timesteps=140000, episode_reward=-0.47 +/- 1.04\n",
      "Episode length: 7.11 +/- 3.26\n",
      "Eval num_timesteps=150000, episode_reward=-0.40 +/- 1.02\n",
      "Episode length: 6.75 +/- 3.07\n",
      "Eval num_timesteps=160000, episode_reward=-0.29 +/- 0.98\n",
      "Episode length: 6.69 +/- 2.99\n",
      "Eval num_timesteps=170000, episode_reward=-0.33 +/- 0.98\n",
      "Episode length: 6.73 +/- 3.19\n",
      "Eval num_timesteps=180000, episode_reward=-0.50 +/- 1.01\n",
      "Episode length: 7.22 +/- 3.13\n",
      "Eval num_timesteps=190000, episode_reward=-0.24 +/- 1.02\n",
      "Episode length: 6.34 +/- 3.23\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-0.38 +/- 1.02\n",
      "Episode length: 6.93 +/- 2.92\n",
      "Evaluating best model for run 99...\n",
      "Run 99 Final Evaluation Results (Best Model):\n",
      "  Success Rate: 0.6500 (65.00%)\n",
      "  Average Detection Time (Successful Episodes): 4.42\n",
      "  Average Episode Reward: -0.3035\n",
      "  Average Episode Length: 6.37\n",
      "===== COMPLETED DQN RUN 99 ====\n",
      "\n",
      "All 100 DQN training runs are complete.\n",
      "Log files are saved in './dqn_z_vector_logs/'.\n",
      "Total time for all runs: 116855.54 seconds\n",
      "\n",
      "--- Overall DQN Policy Results Across All Seeds ---\n",
      "Average Success Rate: 0.5857 (58.57%)\n",
      "Standard Deviation of Success Rate: 0.0565\n",
      "Average Detection Time (Successful Episodes): 4.82\n",
      "Standard Deviation of Detection Time: 0.53\n",
      "Average Episode Reward: -0.4059\n",
      "Standard Deviation of Episode Reward: 0.0981\n",
      "Average Episode Length: 6.97\n",
      "Standard Deviation of Episode Length: 0.45\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import os\n",
    "import time\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "# =============================================================================\n",
    "# 1. THE (p0, z, theta) ENVIRONMENT\n",
    "# =============================================================================\n",
    "\n",
    "class SearchEnvVec(gym.Env):\n",
    "    \"\"\"\n",
    "    A custom Gymnasium environment for the two-object search problem.\n",
    "\n",
    "    This version uses a flat state representation based on:\n",
    "    1. The search count vector (z_t)\n",
    "    2. The static prior (p_0)\n",
    "    3. The related object status (theta)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p0: np.ndarray, gamma1: np.ndarray, gamma2: np.ndarray, T: int, cost_vector: np.ndarray):\n",
    "        super(SearchEnvVec, self).__init__()\n",
    "\n",
    "        self.n = p0.shape[0]\n",
    "        self.T = T\n",
    "        self.p0 = p0.astype(np.float32)\n",
    "        self.p0_flat = self.p0.flatten()\n",
    "        self.gamma1 = gamma1.astype(np.float32)\n",
    "        self.gamma2 = gamma2.astype(np.float32)\n",
    "        self.cost_vector = cost_vector.astype(np.float32)\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.n)\n",
    "\n",
    "        # --- Define the observation space ---\n",
    "        # 1. z_vector: n elements\n",
    "        # 2. prior: n*n elements\n",
    "        # 3. theta: n+1 elements (one-hot)\n",
    "        obs_size = self.n + (self.n * self.n) + (self.n + 1)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0.0,\n",
    "            high=float(self.T),\n",
    "            shape=(obs_size,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self._precompute_conditionals()\n",
    "\n",
    "        # Environment state variables\n",
    "        self.z_vector = np.zeros(self.n, dtype=np.int32)\n",
    "        self.theta = 0\n",
    "        self.current_step = 0\n",
    "        self.true_pos_o1 = 0\n",
    "        self.true_pos_o2 = 0\n",
    "\n",
    "    def _precompute_conditionals(self):\n",
    "        self.conditionals = np.zeros_like(self.p0)\n",
    "        for j in range(self.n):\n",
    "            col_sum = np.sum(self.p0[:, j])\n",
    "            if col_sum > 0:\n",
    "                self.conditionals[:, j] = self.p0[:, j] / col_sum\n",
    "            else:\n",
    "                self.conditionals[:, j] = 1.0 / self.n\n",
    "\n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Constructs the observation vector from z_t, p_0, and theta.\n",
    "        \"\"\"\n",
    "        z_flat = self.z_vector.astype(np.float32)\n",
    "        theta_one_hot = np.zeros(self.n + 1, dtype=np.float32)\n",
    "        theta_one_hot[self.theta] = 1.0\n",
    "\n",
    "        # Concatenate all parts into a single flat vector\n",
    "        obs = np.concatenate([z_flat, self.p0_flat, theta_one_hot])\n",
    "        return obs\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.z_vector = np.zeros(self.n, dtype=np.int32)\n",
    "        self.theta = 0\n",
    "        self.current_step = 0\n",
    "\n",
    "        p_flat = self.p0.flatten()\n",
    "        # Handle case where prior is all zeros by adding a small epsilon or checking sum\n",
    "        prior_sum = np.sum(p_flat)\n",
    "        if prior_sum == 0:\n",
    "             true_idx = self.np_random.choice(self.n * self.n) # Choose randomly if prior is zero\n",
    "        else:\n",
    "             true_idx = self.np_random.choice(self.n * self.n, p=p_flat / prior_sum) # Normalize\n",
    "\n",
    "        self.true_pos_o1, self.true_pos_o2 = np.unravel_index(true_idx, (self.n, self.n))\n",
    "\n",
    "        info = {}\n",
    "        return self._get_obs(), info\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        if not self.action_space.contains(action):\n",
    "            raise ValueError(f\"Invalid action: {action}\")\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        reward = -float(self.cost_vector[action])\n",
    "        terminated = False\n",
    "        found_o1 = False\n",
    "        found_o2_this_step = False\n",
    "\n",
    "        # --- Check for O1 (Target) Detection ---\n",
    "        # O1 is present if its true location is the action cell AND\n",
    "        # (O2 is hidden AND O1 is at action cell) OR (O2 is found at its true location AND O1 is at action cell)\n",
    "        is_o1_present = (action == self.true_pos_o1)\n",
    "\n",
    "        if is_o1_present:\n",
    "            if self.np_random.random() > self.gamma1[action]:\n",
    "                found_o1 = True\n",
    "\n",
    "        if found_o1:\n",
    "            reward = 1.0\n",
    "            terminated = True\n",
    "        else:\n",
    "            # --- Check for O2 (Related) Detection ---\n",
    "            if self.theta == 0: # Only check for O2 if it's currently hidden\n",
    "                if action == self.true_pos_o2:\n",
    "                    if self.np_random.random() > self.gamma2[action]:\n",
    "                        self.theta = action + 1 # O2 found, update theta\n",
    "\n",
    "            self.z_vector[action] += 1 # Increment search count for the actioned cell\n",
    "\n",
    "\n",
    "        if self.current_step >= self.T:\n",
    "            terminated = True\n",
    "\n",
    "        info = {'found_o1': found_o1, 'found_o2': found_o2_this_step}\n",
    "        truncated = False # We use terminated for end of horizon\n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DQN TRAINING FUNCTION FOR A SINGLE RUN\n",
    "# =============================================================================\n",
    "\n",
    "def run_single_experiment(run_id: int, base_log_dir: str, seed: int, total_timesteps: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Runs a single DQN training and evaluation experiment.\n",
    "    Returns a dictionary of final evaluation metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n===== STARTING DQN RUN {run_id} with SEED {seed} ====\")\n",
    "\n",
    "    # --- 3.1. Problem and Environment Definition ---\n",
    "    NUM_CELLS = 5\n",
    "    TIME_HORIZON = 10\n",
    "\n",
    "    set_random_seed(seed)\n",
    "\n",
    "    prior = np.array([\n",
    "        [0.152,  0.0039, 0.003,  0.0108, 0.011],\n",
    "        [0.0038, 0.0052, 0.117,  0.0162, 0.165],\n",
    "        [0.0057, 0.195,  0.015,  0.009,  0.011],\n",
    "        [0.0038, 0.0091, 0.0075, 0.027,  0.011],\n",
    "        [0.0247, 0.0468, 0.0075, 0.117,  0.022]\n",
    "    ])\n",
    "\n",
    "    gammas1 = np.array([0.8, 0.65, 0.82, 0.75, 0.7])\n",
    "    gammas2 = np.array([0.2, 0.1, 0.25, 0.15, 0.2])\n",
    "    # Using 0 cost to maximize probability\n",
    "    costs =  np.array([0.15,0.2,0.25,0.1,0.2])#[0.0] * NUM_CELLS)\n",
    "\n",
    "    # --- 3.2. Create Environments with unique seed for this run ---\n",
    "    env = SearchEnvVec(p0=prior, gamma1=gammas1, gamma2=gammas2, T=TIME_HORIZON, cost_vector=costs)\n",
    "    eval_env = SearchEnvVec(p0=prior, gamma1=gammas1, gamma2=gammas2, T=TIME_HORIZON, cost_vector=costs)\n",
    "\n",
    "    # --- 3.3. Setup Logging and Callbacks for this specific run ---\n",
    "    run_log_dir = os.path.join(base_log_dir, f\"run_{run_id}\")\n",
    "    best_model_dir = os.path.join(run_log_dir, \"best_model\")\n",
    "    os.makedirs(run_log_dir, exist_ok=True)\n",
    "    os.makedirs(best_model_dir, exist_ok=True)\n",
    "\n",
    "    new_logger = configure(run_log_dir, [\"csv\", \"tensorboard\"])\n",
    "\n",
    "    # Set eval_freq lower to get more frequent evaluation points\n",
    "    eval_callback = EvalCallback(eval_env,\n",
    "                                 best_model_save_path=best_model_dir,\n",
    "                                 log_path=run_log_dir, # Save eval results in run dir\n",
    "                                 eval_freq=10000, # Evaluate more frequently\n",
    "                                 n_eval_episodes=100, # Evaluate on 100 episodes during training\n",
    "                                 deterministic=True,\n",
    "                                 render=False)\n",
    "\n",
    "    # --- 3.4. DQN Model Training ---\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=0,\n",
    "        buffer_size=50000,\n",
    "        learning_starts=1000,\n",
    "        batch_size=64,\n",
    "        gamma=0.95, #\n",
    "        train_freq=(10, \"step\"),\n",
    "        gradient_steps=10,\n",
    "        target_update_interval=1000,\n",
    "        exploration_fraction=0.2,\n",
    "        exploration_final_eps=0.01,\n",
    "        seed=seed,\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "    model.set_logger(new_logger)\n",
    "\n",
    "    model.learn(total_timesteps=total_timesteps, callback=eval_callback)\n",
    "\n",
    "    # --- 3.5. Final Evaluation of the Best Model ---\n",
    "    print(f\"Evaluating best model for run {run_id}...\")\n",
    "    best_model_path = os.path.join(best_model_dir, \"best_model.zip\")\n",
    "    final_metrics = {}\n",
    "\n",
    "    if os.path.exists(best_model_path):\n",
    "        best_model = DQN.load(best_model_path, env=eval_env)\n",
    "        num_final_eval_episodes = 100\n",
    "        total_reward = 0\n",
    "        num_successes = 0\n",
    "        detection_times = []\n",
    "        episode_lengths = []\n",
    "\n",
    "        # Each episode will now have a unique seed based on the run_id and episode index\n",
    "        for i_episode in range(num_final_eval_episodes):\n",
    "            episode_seed = seed * num_final_eval_episodes + i_episode # Unique seed for each episode\n",
    "            obs, info = eval_env.reset(seed=episode_seed)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            episode_len = 0\n",
    "\n",
    "            while not done:\n",
    "                action, _ = best_model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "                done = terminated or truncated\n",
    "                episode_reward += reward\n",
    "                episode_len += 1\n",
    "\n",
    "                if info.get('found_o1'):\n",
    "                    detection_times.append(episode_len)\n",
    "\n",
    "            total_reward += episode_reward\n",
    "            episode_lengths.append(episode_len)\n",
    "            if info.get('found_o1'):\n",
    "                 num_successes += 1\n",
    "\n",
    "        mean_reward = total_reward / num_final_eval_episodes\n",
    "        success_rate = num_successes / num_final_eval_episodes\n",
    "        mean_detection_time = np.mean(detection_times) if detection_times else -1\n",
    "        mean_episode_length = np.mean(episode_lengths)\n",
    "\n",
    "        final_metrics = {\n",
    "            'success_rate': success_rate,\n",
    "            'avg_detection_time': mean_detection_time,\n",
    "            'avg_episode_reward': mean_reward,\n",
    "            'avg_episode_length': mean_episode_length\n",
    "        }\n",
    "\n",
    "\n",
    "        print(f\"Run {run_id} Final Evaluation Results (Best Model):\")\n",
    "        print(f\"  Success Rate: {final_metrics['success_rate']:.4f} ({final_metrics['success_rate']*100:.2f}%)\")\n",
    "        print(f\"  Average Detection Time (Successful Episodes): {final_metrics['avg_detection_time']:.2f}\")\n",
    "        print(f\"  Average Episode Reward: {final_metrics['avg_episode_reward']:.4f}\")\n",
    "        print(f\"  Average Episode Length: {final_metrics['avg_episode_length']:.2f}\")\n",
    "    else:\n",
    "        print(f\"No best model found for run {run_id}.\")\n",
    "\n",
    "\n",
    "    print(f\"===== COMPLETED DQN RUN {run_id} ====\")\n",
    "    return final_metrics\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration for Multiple Runs ---\n",
    "    NUM_RUNS = 100 # Set to 100 seeds as requested\n",
    "    TOTAL_TIMESTEPS = 200_000\n",
    "    BASE_LOG_DIR = \"./dqn_z_vector_logs/\"\n",
    "\n",
    "    # --- Execute All Runs ---\n",
    "    all_final_success_rates = []\n",
    "    all_final_detection_times = []\n",
    "    all_final_rewards = []\n",
    "    all_final_episode_lengths = []\n",
    "\n",
    "    total_experiment_start_time = time.time()\n",
    "\n",
    "    for i in range(NUM_RUNS):\n",
    "        run_seed = i # Seeds go from 0 to 99\n",
    "        final_metrics = run_single_experiment(\n",
    "            run_id=i,\n",
    "            base_log_dir=BASE_LOG_DIR,\n",
    "            seed=run_seed,\n",
    "            total_timesteps=TOTAL_TIMESTEPS\n",
    "        )\n",
    "\n",
    "        # Collect metrics from each run\n",
    "        if final_metrics: # Only collect if evaluation was successful\n",
    "            all_final_success_rates.append(final_metrics['success_rate'])\n",
    "            if final_metrics['avg_detection_time'] != -1:\n",
    "                 all_final_detection_times.append(final_metrics['avg_detection_time'])\n",
    "            all_final_rewards.append(final_metrics['avg_episode_reward'])\n",
    "            all_final_episode_lengths.append(final_metrics['avg_episode_length'])\n",
    "\n",
    "\n",
    "    total_experiment_end_time = time.time()\n",
    "\n",
    "    # --- Calculate and Display Overall Metrics ---\n",
    "    print(f\"\\nAll {NUM_RUNS} DQN training runs are complete.\")\n",
    "    print(f\"Log files are saved in '{BASE_LOG_DIR}'.\")\n",
    "    print(f\"Total time for all runs: {total_experiment_end_time - total_experiment_start_time:.2f} seconds\")\n",
    "\n",
    "    print(\"\\n--- Overall DQN Policy Results Across All Seeds ---\")\n",
    "    print(f\"Average Success Rate: {np.mean(all_final_success_rates):.4f} ({np.mean(all_final_success_rates)*100:.2f}%)\")\n",
    "    print(f\"Standard Deviation of Success Rate: {np.std(all_final_success_rates):.4f}\")\n",
    "\n",
    "    if all_final_detection_times:\n",
    "        print(f\"Average Detection Time (Successful Episodes): {np.mean(all_final_detection_times):.2f}\")\n",
    "        print(f\"Standard Deviation of Detection Time: {np.std(all_final_detection_times):.2f}\")\n",
    "    else:\n",
    "        print(\"Average Detection Time (Successful Episodes): N/A (no successful episodes across all runs)\")\n",
    "\n",
    "    print(f\"Average Episode Reward: {np.mean(all_final_rewards):.4f}\")\n",
    "    print(f\"Standard Deviation of Episode Reward: {np.std(all_final_rewards):.4f}\")\n",
    "\n",
    "    print(f\"Average Episode Length: {np.mean(all_final_episode_lengths):.2f}\")\n",
    "    print(f\"Standard Deviation of Episode Length: {np.std(all_final_episode_lengths):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae244877-ac13-43c1-97d3-a9e7a2c26103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from './dqn_z_vector_logs/'...\n",
      "Saved plot to 'dqn_z_vector_mean_rollout_reward_plot.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_865939/617927569.py:96: UserWarning: linestyle is redundantly defined by the 'linestyle' keyword argument and the fmt string \"-o\" (-> linestyle='-'). The keyword argument will take precedence.\n",
      "  plt.errorbar(timesteps, mean_rewards, yerr=std_devs, fmt='-o', color=color,\n",
      "/tmp/ipykernel_865939/617927569.py:96: UserWarning: marker is redundantly defined by the 'marker' keyword argument and the fmt string \"-o\" (-> marker='o'). The keyword argument will take precedence.\n",
      "  plt.errorbar(timesteps, mean_rewards, yerr=std_devs, fmt='-o', color=color,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot to 'dqn_z_vector_mean_evaluation_reward_plot.png'\n",
      "\n",
      "Plotting complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_LOG_DIR = \"./dqn_z_vector_logs/\" # <-- MUST MATCH THE TRAINING SCRIPT\n",
    "NUM_RUNS = 10 # <-- MUST MATCH THE TRAINING SCRIPT\n",
    "ROLLOUT_PLOT_FILENAME = 'dqn_z_vector_mean_rollout_reward_plot.png'\n",
    "EVAL_PLOT_FILENAME = 'dqn_z_vector_mean_evaluation_reward_plot.png'\n",
    "EVAL_FREQ = 5000 # <-- Must match the eval_freq in the training script\n",
    "\n",
    "def process_and_plot_line(list_of_dfs: list, x_col: str, y_col: str, title: str, \n",
    "                          xlabel: str, ylabel: str, color: str, output_filename: str):\n",
    "    \"\"\"\n",
    "    Processes a list of dataframes to calculate and plot the mean and std dev\n",
    "    as a continuous, interpolated line with a shaded region.\n",
    "    \"\"\"\n",
    "    if not list_of_dfs:\n",
    "        print(f\"Warning: No data found for column '{y_col}'. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    # 1. Create a common x-axis (timesteps) for all runs using interpolation\n",
    "    max_x = max(df[x_col].max() for df in list_of_dfs)\n",
    "    common_x_axis = np.linspace(0, max_x, num=500) # 500 points for a smooth curve\n",
    "    \n",
    "    # 2. Interpolate y-values for each run onto the common x-axis\n",
    "    interpolated_y_values = []\n",
    "    for df in list_of_dfs:\n",
    "        interpolated_y = np.interp(common_x_axis, df[x_col], df[y_col])\n",
    "        interpolated_y_values.append(interpolated_y)\n",
    "        \n",
    "    # 3. Calculate mean and standard deviation across all runs\n",
    "    y_matrix = np.array(interpolated_y_values)\n",
    "    mean_y = np.mean(y_matrix, axis=0)\n",
    "    std_y = np.std(y_matrix, axis=0)\n",
    "    \n",
    "    # 4. Plotting\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(common_x_axis, mean_y, color=color, label='Mean')\n",
    "    plt.fill_between(common_x_axis, mean_y - std_y, mean_y + std_y, \n",
    "                     color=color, alpha=0.25, label='Standard Deviation')\n",
    "                     \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 5. Save the plot\n",
    "    plt.savefig(output_filename)\n",
    "    print(f\"Saved plot to '{output_filename}'\")\n",
    "    plt.close()\n",
    "\n",
    "def process_and_plot_errorbars(list_of_dfs: list, x_col: str, y_col: str, title: str, \n",
    "                               xlabel: str, ylabel: str, color: str, output_filename: str):\n",
    "    \"\"\"\n",
    "    Processes a list of dataframes to calculate and plot the mean and std dev\n",
    "    as discrete error bar points.\n",
    "    \"\"\"\n",
    "    if not list_of_dfs:\n",
    "        print(f\"Warning: No data found for column '{y_col}'. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    # 1. Group rewards by the evaluation timestep\n",
    "    # We round the timesteps to the nearest EVAL_FREQ to group them\n",
    "    eval_data = {}\n",
    "    for df in list_of_dfs:\n",
    "        for _index, row in df.iterrows():\n",
    "            # Round the timestep to the nearest evaluation point\n",
    "            timestep = int(round(row[x_col] / EVAL_FREQ)) * EVAL_FREQ\n",
    "            if timestep == 0: continue # Skip 0-step evaluation if present\n",
    "            \n",
    "            reward = row[y_col]\n",
    "            if timestep not in eval_data:\n",
    "                eval_data[timestep] = []\n",
    "            eval_data[timestep].append(reward)\n",
    "\n",
    "    if not eval_data:\n",
    "        print(f\"Warning: No evaluation data could be grouped. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    # 2. Calculate mean and std dev for each timestep\n",
    "    timesteps = sorted(eval_data.keys())\n",
    "    mean_rewards = [np.mean(eval_data[t]) for t in timesteps]\n",
    "    std_devs = [np.std(eval_data[t]) for t in timesteps]\n",
    "    \n",
    "    # 3. Plotting\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Use plt.errorbar to create the plot\n",
    "    # --- MODIFIED ---\n",
    "    # fmt='-o' creates a line connecting the points, with markers at each point.\n",
    "    # capsize=5 adds the small caps to the error bars\n",
    "    plt.errorbar(timesteps, mean_rewards, yerr=std_devs, fmt='-o', color=color, \n",
    "                 capsize=5, label='Mean Evaluation Reward ( 1 Std Dev)',\n",
    "                 linestyle='-', marker='o', markersize=5) # Explicitly set line and marker\n",
    "                     \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 4. Save the plot\n",
    "    plt.savefig(output_filename)\n",
    "    print(f\"Saved plot to '{output_filename}'\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    all_rollout_dfs = []\n",
    "    all_eval_dfs = []\n",
    "\n",
    "    # --- Load Data from All Runs ---\n",
    "    print(f\"Loading data from '{BASE_LOG_DIR}'...\")\n",
    "    for i in range(NUM_RUNS):\n",
    "        file_path = os.path.join(BASE_LOG_DIR, f\"run_{i}\", \"progress.csv\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Add rollout data if the column exists\n",
    "            if 'rollout/ep_rew_mean' in df.columns:\n",
    "                all_rollout_dfs.append(df[['time/total_timesteps', 'rollout/ep_rew_mean']].dropna())\n",
    "            # Add evaluation data if the column exists\n",
    "            if 'eval/mean_reward' in df.columns:\n",
    "                 # We need all timesteps for the error bar plot, not just the interpolated ones\n",
    "                 all_eval_dfs.append(df[['time/total_timesteps', 'eval/mean_reward']].dropna())\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Could not find '{file_path}'. Skipping run {i}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred loading data for run {i}: {e}\")\n",
    "\n",
    "    # --- Generate Plots ---\n",
    "    \n",
    "    # Plot 1: Mean Training (Rollout) Reward - USES THE LINE PLOT\n",
    "    process_and_plot_line(\n",
    "        list_of_dfs=all_rollout_dfs,\n",
    "        x_col='time/total_timesteps',\n",
    "        y_col='rollout/ep_rew_mean',\n",
    "        title='DQN (z-vector) - Mean Training Reward (Rollout) vs. Timesteps',\n",
    "        xlabel='Total Timesteps',\n",
    "        ylabel='Mean Episode Reward',\n",
    "        color='darkorange',\n",
    "        output_filename=ROLLOUT_PLOT_FILENAME\n",
    "    )\n",
    "    \n",
    "    # Plot 2: Mean Evaluation Reward - USES THE NEW ERROR BAR PLOT\n",
    "    process_and_plot_errorbars(\n",
    "        list_of_dfs=all_eval_dfs,\n",
    "        x_col='time/total_timesteps',\n",
    "        y_col='eval/mean_reward',\n",
    "        title='DQN (z-vector) - Mean Evaluation Reward vs. Timesteps',\n",
    "        xlabel='Total Timesteps',\n",
    "        ylabel='Mean Episode Reward',\n",
    "        color='darkred',\n",
    "        output_filename=EVAL_PLOT_FILENAME\n",
    "    )\n",
    "\n",
    "    print(\"\\nPlotting complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
