{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_problem_parameters(n: int, seed: int = 42) -> tuple:\n",
    "    \"\"\"\n",
    "    Generates correlated joint prior, cost vector, and miss-detection rates\n",
    "    for a two-object search problem.\n",
    "\n",
    "    Args:\n",
    "        n (int): The number of cells.\n",
    "        seed (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (prior_joint, gamma1, gamma2, c)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 1. Generate gamma1 in a range that allows gamma2 to be smaller\n",
    "    gamma1 = np.random.uniform(low=0.6, high=0.85, size=n)\n",
    "\n",
    "    # 2. Generate gamma2 such that gamma2 < gamma1, and within its own desired range [0.15, 0.25]\n",
    "    gamma2 = np.zeros(n)\n",
    "    min_gamma2_val = 0.15\n",
    "    max_gamma2_val = 0.25\n",
    "    for i in range(n):\n",
    "        # Upper bound for gamma2 is min(max_gamma2_val, gamma1[i] - 0.02)\n",
    "        upper_bound_for_this_gamma2 = min(max_gamma2_val, gamma1[i] - 0.02) # Small buffer to ensure gamma2 < gamma1\n",
    "        if upper_bound_for_this_gamma2 < min_gamma2_val: # If buffer makes it too small, ensure min\n",
    "            gamma2[i] = np.random.uniform(low=min_gamma2_val, high=min_gamma2_val + 0.01) # Force a small valid range\n",
    "        else:\n",
    "            gamma2[i] = np.random.uniform(low=min_gamma2_val, high=upper_bound_for_this_gamma2)\n",
    "    gamma2 = np.clip(gamma2, None, max_gamma2_val) # Ensure max bound\n",
    "\n",
    "    # 3. Generate cost vector c\n",
    "    c = np.random.uniform(low=0.1, high=0.3, size=n)\n",
    "\n",
    "    # 4. Generate correlated joint prior (n x n matrix)\n",
    "    prior_joint = np.zeros((n, n))\n",
    "    correlation_strength = 2.0 # Adjust this value to control correlation strength\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            # Higher probability when O1 and O2 are in the same or adjacent cells\n",
    "            distance = abs(i - j)\n",
    "            prior_joint[i, j] = np.exp(-correlation_strength * distance) # Exponential decay with distance\n",
    "\n",
    "    # Add some random noise to ensure variety (but keep it small)\n",
    "    prior_joint += np.random.uniform(low=0.01, high=0.05, size=(n, n))\n",
    "    prior_joint[prior_joint < 0.001] = 0.001 # Ensure no zero probabilities\n",
    "\n",
    "    # Normalize the joint prior\n",
    "    prior_joint /= np.sum(prior_joint)\n",
    "\n",
    "    # Explicitly ensure the sum is exactly 1.0 by adjusting the last element\n",
    "    # This handles potential floating-point inaccuracies after normalization.\n",
    "    prior_joint[-1, -1] += (1.0 - np.sum(prior_joint))\n",
    "\n",
    "    return prior_joint, gamma1, gamma2, c\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example usage:\n",
    "    \"\"\"NUM_CELLS = 25\n",
    "    SEED = 20\n",
    "\n",
    "    prior_joint, gamma1, gamma2, c = generate_problem_parameters(NUM_CELLS, SEED)\n",
    "\n",
    "    print(f\"Generated Parameters (N={NUM_CELLS}, Seed={SEED}):\\n\")\n",
    "    print(\"Joint Prior (p(O1, O2)):\\n\", np.round(prior_joint, 4))\n",
    "    print(\"\\nSum of Joint Prior: \", np.sum(prior_joint))\n",
    "    print(\"\\nMiss-detection rates for O1 (gamma1):\", np.round(gamma1, 4))\n",
    "    print(\"Miss-detection rates for O2 (gamma2):\", np.round(gamma2, 4))\n",
    "    print(\"\\nCost vector (c):\", np.round(c, 4))\n",
    "\n",
    "    # Verify gamma1 > gamma2\n",
    "    print(\"\\nIs gamma1 > gamma2 for all elements?:\", np.all(gamma1 > gamma2))\"\"\"\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNgLojO6pzq_",
    "outputId": "e8586ed1-9ccb-4dc3-9c84-87485c29667d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Empirical Experiment for k-step lookahead Policy over Multiple Seeds ---\n",
      "Number of seeds: 10\n",
      "Number of evaluation episodes per seed: 10\n",
      "Lookahead depth (k): 1\n",
      "Rollout simulations: 10\n",
      "--------------------\n",
      "\n",
      "--- Running with Seed: 0 ---\n",
      "--- Seed 0 Results ---\n",
      "Success Rate: 0.2000 (20.00%)\n",
      "Average Detection Time (Successful Episodes): 13.00\n",
      "Average Episode Reward: -2.7656\n",
      "Average Episode Length: 18.60\n",
      "Time taken for this seed: 222.41 seconds\n",
      "\n",
      "--- Running with Seed: 1 ---\n",
      "--- Seed 1 Results ---\n",
      "Success Rate: 0.0000 (0.00%)\n",
      "Average Detection Time (Successful Episodes): N/A (no successes)\n",
      "Average Episode Reward: -2.5895\n",
      "Average Episode Length: 20.00\n",
      "Time taken for this seed: 213.83 seconds\n",
      "\n",
      "--- Running with Seed: 2 ---\n",
      "--- Seed 2 Results ---\n",
      "Success Rate: 0.2000 (20.00%)\n",
      "Average Detection Time (Successful Episodes): 9.50\n",
      "Average Episode Reward: -2.7598\n",
      "Average Episode Length: 17.90\n",
      "Time taken for this seed: 168.49 seconds\n",
      "\n",
      "--- Running with Seed: 3 ---\n",
      "--- Seed 3 Results ---\n",
      "Success Rate: 0.3000 (30.00%)\n",
      "Average Detection Time (Successful Episodes): 17.67\n",
      "Average Episode Reward: -3.0043\n",
      "Average Episode Length: 19.30\n",
      "Time taken for this seed: 193.14 seconds\n",
      "\n",
      "--- Running with Seed: 4 ---\n",
      "--- Seed 4 Results ---\n",
      "Success Rate: 0.2000 (20.00%)\n",
      "Average Detection Time (Successful Episodes): 12.00\n",
      "Average Episode Reward: -2.8634\n",
      "Average Episode Length: 18.40\n",
      "Time taken for this seed: 205.16 seconds\n",
      "\n",
      "--- Running with Seed: 5 ---\n",
      "--- Seed 5 Results ---\n",
      "Success Rate: 0.2000 (20.00%)\n",
      "Average Detection Time (Successful Episodes): 9.00\n",
      "Average Episode Reward: -2.6226\n",
      "Average Episode Length: 17.80\n",
      "Time taken for this seed: 185.66 seconds\n",
      "\n",
      "--- Running with Seed: 6 ---\n",
      "--- Seed 6 Results ---\n",
      "Success Rate: 0.3000 (30.00%)\n",
      "Average Detection Time (Successful Episodes): 7.33\n",
      "Average Episode Reward: -2.5375\n",
      "Average Episode Length: 16.20\n",
      "Time taken for this seed: 191.08 seconds\n",
      "\n",
      "--- Running with Seed: 7 ---\n",
      "--- Seed 7 Results ---\n",
      "Success Rate: 0.2000 (20.00%)\n",
      "Average Detection Time (Successful Episodes): 12.00\n",
      "Average Episode Reward: -2.6568\n",
      "Average Episode Length: 18.40\n",
      "Time taken for this seed: 197.23 seconds\n",
      "\n",
      "--- Running with Seed: 8 ---\n",
      "--- Seed 8 Results ---\n",
      "Success Rate: 0.3000 (30.00%)\n",
      "Average Detection Time (Successful Episodes): 14.00\n",
      "Average Episode Reward: -3.2408\n",
      "Average Episode Length: 18.20\n",
      "Time taken for this seed: 191.99 seconds\n",
      "\n",
      "--- Running with Seed: 9 ---\n",
      "--- Seed 9 Results ---\n",
      "Success Rate: 0.3000 (30.00%)\n",
      "Average Detection Time (Successful Episodes): 7.33\n",
      "Average Episode Reward: -2.3425\n",
      "Average Episode Length: 16.20\n",
      "Time taken for this seed: 164.64 seconds\n",
      "\n",
      "--- Overall 1-step Lookahead Policy Results ---\n",
      "Total experiment time across 10 seeds: 1933.66 seconds\n",
      "Average Success Rate over 10 seeds and 10 episodes each: 0.2200 (22.00%)\n",
      "Standard Deviation of Success Rate: 0.0872\n",
      "Average Detection Time (Successful Episodes) across all seeds: 11.36\n",
      "Standard Deviation of Detection Time: 6.18\n",
      "Average Episode Reward across all seeds: -2.7383\n",
      "Standard Deviation of Reward: 1.0847\n",
      "Average Episode Length across all seeds: 18.10\n",
      "Standard Deviation of Episode Length: 4.61\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import time\n",
    "\n",
    "# --- Start of SingleObjectDPSolver (copied from txTdQaQVhkrA) ---\n",
    "class SingleObjectDPSolver:\n",
    "    \"\"\"\n",
    "    An exact Dynamic Programming solver for a single-object search problem.\n",
    "\n",
    "    This class uses backward recursion to find the optimal policy based on a\n",
    "    marginal prior distribution for a single target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n: int, T: int, p0_marginal: np.ndarray, gamma1: np.ndarray, c: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initializes the solver with the single-object problem parameters.\n",
    "\n",
    "        Args:\n",
    "            n (int): The number of cells.\n",
    "            T (int): The time horizon.\n",
    "            p0_marginal (np.ndarray): The n-element marginal prior probability vector for O1.\n",
    "            gamma1 (np.ndarray): Miss-detection rates for O1.\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.T = T\n",
    "        self.p0_marginal = p0_marginal\n",
    "        self.gamma1 = gamma1\n",
    "        self.c = c\n",
    "\n",
    "        # Data structures to store the results\n",
    "        self.J_values: Dict[int, Dict[Tuple[int, ...], float]] = {}  # Value function J_t(z_t)\n",
    "        self.Policy: Dict[int, Dict[Tuple[int, ...], int]] = {}      # Policy mu_t(z_t)\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None) # Memoization for performance\n",
    "    def _generate_z_vectors(t: int, n: int) -> List[Tuple[int, ...]]:\n",
    "        \"\"\"Recursively generates all state vectors z_t where sum(z_i) = t.\"\"\"\n",
    "        if n == 1:\n",
    "            return [(t,)]\n",
    "\n",
    "        vectors = []\n",
    "        for i in range(t + 1):\n",
    "            for sub_vector in SingleObjectDPSolver._generate_z_vectors(t - i, n - 1):\n",
    "                vectors.append((i,) + sub_vector)\n",
    "        return vectors\n",
    "\n",
    "    def _calculate_posterior(self, z_vector: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculates the posterior belief p(O1=i | z).\"\"\"\n",
    "        g1_z = np.power(self.gamma1, z_vector)\n",
    "        numerator = self.p0_marginal * g1_z\n",
    "        norm = np.sum(numerator)\n",
    "        return numerator / norm if norm > 0 else numerator\n",
    "\n",
    "    def solve(self):\n",
    "        \"\"\"\n",
    "        Executes the backward recursion to solve the DP problem.\n",
    "        \"\"\"\n",
    "        # print(\"Starting Single-Object DP solver...\")\n",
    "        # --- Initialization at T ---\n",
    "        # print(f\"Initializing for T={self.T}...\")\n",
    "        self.J_values[self.T] = {}\n",
    "        z_vectors_T = self._generate_z_vectors(self.T, self.n)\n",
    "        for z_T in z_vectors_T:\n",
    "            self.J_values[self.T][z_T] = 0.0\n",
    "\n",
    "        # --- Backward Recursion ---\n",
    "        for t in range(self.T - 1, -1, -1):\n",
    "            #start_time = time.time()\n",
    "            J_t = {}\n",
    "            policy_t = {}\n",
    "            z_vectors_t = self._generate_z_vectors(t, self.n)\n",
    "\n",
    "            for z_t_tuple in z_vectors_t:\n",
    "                z_t = np.array(z_t_tuple)\n",
    "                action_values = []\n",
    "\n",
    "                # Calculate current belief vector\n",
    "                belief_t = self._calculate_posterior(z_t)\n",
    "\n",
    "                # Iterate over all possible actions\n",
    "                for a_t in range(self.n):\n",
    "                    # Probability of finding the target if we search cell a_t\n",
    "                    p_success = (1 - self.gamma1[a_t]) * belief_t[a_t]\n",
    "                    p_fail = 1 - ((1 - self.gamma1[a_t]) * belief_t[a_t])\n",
    "\n",
    "                    # Get the value of the state we transition to upon failure\n",
    "                    next_z_tuple = tuple(z_t + np.eye(self.n, dtype=int)[a_t])\n",
    "                    val_if_fail = self.J_values[t + 1][next_z_tuple]\n",
    "\n",
    "                    # Expected value for this action (Bellman equation)\n",
    "                    # Reward for success is 1, reward for failure is 0\n",
    "                    expected_value = (p_success * 1.0) - self.c[a_t] + (p_fail * val_if_fail)\n",
    "                    action_values.append(expected_value)\n",
    "\n",
    "                # Find best action and store value/policy\n",
    "                best_value = np.max(action_values)\n",
    "                best_action = np.argmax(action_values)\n",
    "                J_t[z_t_tuple] = best_value\n",
    "                policy_t[z_t_tuple] = best_action\n",
    "\n",
    "            self.J_values[t] = J_t\n",
    "            self.Policy[t] = policy_t\n",
    "            #end_time = time.time()\n",
    "            # print(f\"Completed t={t}. Found {len(z_vectors_t)} states. Took {end_time - start_time:.2f}s.\")\n",
    "\n",
    "        # print(\"DP solver finished.\")\n",
    "\n",
    "    def get_optimal_value(self) -> float:\n",
    "        \"\"\"Returns the optimal value J(z_0).\"\"\"\n",
    "        initial_z = tuple([0] * self.n)\n",
    "        return self.J_values[0][initial_z]\n",
    "# --- End of SingleObjectDPSolver ---\n",
    "\n",
    "\n",
    "# --- Start of TwoObjectDPSolver (copied from Uu36DFAN5dbB) ---\n",
    "class TwoObjectDPSolver:\n",
    "    \"\"\"\n",
    "    An exact Dynamic Programming solver for the two-object search problem.\n",
    "\n",
    "    This class uses backward recursion to find the optimal policy and the\n",
    "    maximum probability of detecting the primary target (O1).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n: int, T: int, p0: np.ndarray, gamma1: np.ndarray, gamma2: np.ndarray, c: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initializes the solver with the problem parameters.\n",
    "\n",
    "        Args:\n",
    "            n (int): The number of cells.\n",
    "            T (int): The time horizon.\n",
    "            p0 (np.ndarray): The n x n prior joint probability matrix.\n",
    "            gamma1 (np.ndarray): Miss-detection rates for O1.\n",
    "            gamma2 (np.ndarray): Miss-detection rates for O2.\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.T = T\n",
    "        self.p0 = p0\n",
    "        self.gamma1 = gamma1\n",
    "        self.gamma2 = gamma2\n",
    "        self.c = c\n",
    "\n",
    "        # Data structures to store the results\n",
    "        self.J_values: Dict[int, Dict[Tuple, float]] = {}  # Value function J_t(s_t)\n",
    "        self.Policy: Dict[int, Dict[Tuple, int]] = {}      # Policy mu_t(s_t)\n",
    "\n",
    "        # Pre-calculate initial conditional priors for efficiency\n",
    "        self._p0_conditionals = self._precompute_p0_conditionals()\n",
    "\n",
    "    def _precompute_p0_conditionals(self) -> List[np.ndarray]:\n",
    "        \"\"\"Pre-calculates P(O1=i | O2=k) for all k.\"\"\"\n",
    "        conditionals = []\n",
    "        for k in range(self.n):\n",
    "            marginal_o2 = np.sum(self.p0[:, k])\n",
    "            if marginal_o2 > 0:\n",
    "                conditionals.append(self.p0[:, k] / marginal_o2)\n",
    "            else:\n",
    "                # If O2 can never be in cell k, the conditional is undefined. Use zeros.\n",
    "                conditionals.append(np.zeros(self.n))\n",
    "        return conditionals\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None) # Memoization for performance\n",
    "    def _generate_z_vectors(t: int, n: int) -> List[Tuple[int, ...]]:\n",
    "        \"\"\"Recursively generates all state vectors z_t where sum(z_i) = t.\"\"\"\n",
    "        if n == 1:\n",
    "            return [(t,)]\n",
    "\n",
    "        vectors = []\n",
    "        for i in range(t + 1):\n",
    "            for sub_vector in TwoObjectDPSolver._generate_z_vectors(t - i, n - 1):\n",
    "                vectors.append((i,) + sub_vector)\n",
    "        \"\"\"if n == 5:\n",
    "           for vec in vectors:\n",
    "               print(vec)\"\"\"\n",
    "        return vectors\n",
    "\n",
    "    def _calculate_joint_posterior(self, z_vector: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculates p(O1=i, O2=j | z).\"\"\"\n",
    "        g1_z = np.power(self.gamma1, z_vector)\n",
    "        g2_z = np.power(self.gamma2, z_vector)\n",
    "        likelihood = np.outer(g1_z, g2_z)\n",
    "        numerator = self.p0 * likelihood\n",
    "        norm = np.sum(numerator)\n",
    "        return numerator / norm if norm > 0 else numerator\n",
    "\n",
    "    def _calculate_conditional_posterior(self, z_vector: np.ndarray, o2_loc: int) -> np.ndarray:\n",
    "        \"\"\"Calculates p(O1=i | z, O2=o2_loc).\"\"\"\n",
    "        p0_cond = self._p0_conditionals[o2_loc]\n",
    "        g1_z = np.power(self.gamma1, z_vector)\n",
    "        numerator = g1_z * p0_cond\n",
    "        norm = np.sum(numerator)\n",
    "        return numerator / norm if norm > 0 else numerator\n",
    "\n",
    "    def solve(self):\n",
    "        \"\"\"\n",
    "        Executes the backward recursion to solve the DP problem.\n",
    "        \"\"\"\n",
    "        #print(\"Starting DP solver...\")\n",
    "        # --- Initialization at T ---\n",
    "        #print(f\"Initializing for T={self.T}...\")\n",
    "        self.J_values[self.T] = {}\n",
    "        z_vectors_T = self._generate_z_vectors(self.T, self.n)\n",
    "        for z_T in z_vectors_T:\n",
    "            for theta2 in range(self.n + 1):\n",
    "                state = (z_T, theta2)\n",
    "                self.J_values[self.T][state] = 0.0\n",
    "\n",
    "        # --- Backward Recursion ---\n",
    "        for t in range(self.T - 1, -1, -1):\n",
    "            #start_time = time.time()\n",
    "            J_t = {}\n",
    "            policy_t = {}\n",
    "            z_vectors_t = self._generate_z_vectors(t, self.n)\n",
    "\n",
    "            for z_t_tuple in z_vectors_t:\n",
    "                z_t = np.array(z_t_tuple)\n",
    "                for theta2 in range(self.n + 1):\n",
    "                    if t == 0 and theta2 > 0:\n",
    "                        break\n",
    "                    current_state = (z_t_tuple, theta2)\n",
    "                    action_values = []\n",
    "\n",
    "                    # Calculate belief based on current state\n",
    "                    if theta2 == 0:\n",
    "                        belief = self._calculate_joint_posterior(z_t)\n",
    "                    else:\n",
    "                        o2_loc = theta2 - 1\n",
    "                        belief = self._calculate_conditional_posterior(z_t, o2_loc)\n",
    "\n",
    "                    # Iterate over all possible actions\n",
    "                    for a_t in range(self.n):\n",
    "                        next_z_tuple = tuple(z_t + np.eye(self.n, dtype=int)[a_t])\n",
    "\n",
    "                        # --- Case 1: Both objects hidden ---\n",
    "                        if theta2 == 0:\n",
    "                            p_marginal_o1_at_a = np.sum(belief[a_t, :])\n",
    "\n",
    "                            # Prob of success (finding O1)\n",
    "                            p_success = (1 - self.gamma1[a_t]) * p_marginal_o1_at_a\n",
    "\n",
    "                            # Prob of finding O2 only\n",
    "                            p_cond_sum = np.sum(belief[np.arange(self.n) != a_t, a_t])\n",
    "                            p_find_o2_only = (1 - self.gamma2[a_t]) * (self.gamma1[a_t] * belief[a_t, a_t] + p_cond_sum)\n",
    "\n",
    "                            # Prob of finding nothing\n",
    "                            p_nothing = 1 - p_success - p_find_o2_only\n",
    "\n",
    "                            # Future values from next stage\n",
    "                            val_if_nothing = self.J_values[t + 1][(next_z_tuple, 0)]\n",
    "                            val_if_found_o2 = self.J_values[t + 1][(next_z_tuple, a_t + 1)]\n",
    "\n",
    "                            # Expected value for this action\n",
    "                            expected_value = (p_success * 1.0) - self.c[a_t]+ \\\n",
    "                                             (p_nothing * val_if_nothing) + \\\n",
    "                                             (p_find_o2_only * val_if_found_o2)\n",
    "                            action_values.append(expected_value)\n",
    "\n",
    "                        # --- Case 2: O2 has been found ---\n",
    "                        else:\n",
    "                            p_o1_at_a = belief[a_t]\n",
    "                            p_success = (1 - self.gamma1[a_t]) * p_o1_at_a\n",
    "                            p_fail = 1 - p_success\n",
    "\n",
    "                            val_if_fail = self.J_values[t + 1][(next_z_tuple, theta2)]\n",
    "\n",
    "                            expected_value = (p_success * 1.0) - self.c[a_t]+ (p_fail * val_if_fail)\n",
    "                            action_values.append(expected_value)\n",
    "\n",
    "                    # Find best action and store value/policy\n",
    "                    best_value = np.max(action_values)\n",
    "                    best_action = np.argmax(action_values)\n",
    "                    J_t[current_state] = best_value\n",
    "                    policy_t[current_state] = best_action\n",
    "\n",
    "            self.J_values[t] = J_t\n",
    "            self.Policy[t] = policy_t\n",
    "            #end_time = time.time()\n",
    "            #print(f\"Completed t={t}. Found {len(z_vectors_t)} states. Took {end_time - start_time:.2f}s.\")\n",
    "\n",
    "        #print(\"DP solver finished.\")\n",
    "\n",
    "    def get_optimal_value(self) -> float:\n",
    "        \"\"\"Returns the optimal value J(s_0).\"\"\"\n",
    "        initial_z = tuple([0] * self.n)\n",
    "        initial_state = (initial_z, 0)\n",
    "        return self.J_values[0][initial_state]\n",
    "# --- End of TwoObjectDPSolver ---\n",
    "\n",
    "\n",
    "class KStepLookaheadSolver:\n",
    "    \"\"\"\n",
    "    A k-step lookahead solver using rollout with a greedy policy for value estimation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n: int, T: int, initial_episode_belief: Any, # Can be (n,n) or (n,)\n",
    "                 gamma1: np.ndarray, gamma2: np.ndarray, theta2_at_init: int, k: int, c: np.ndarray, rollout_sims: int = 100, solver_seed: int = None):\n",
    "        self.n = n\n",
    "        self.T_global = T # Store global horizon for rollout remaining_horizon calculation\n",
    "        self.initial_episode_belief = initial_episode_belief # This is the current belief from the global episode\n",
    "        self.gamma1 = gamma1\n",
    "        self.gamma2 = gamma2\n",
    "        self.c = c\n",
    "        self.k = k # Lookahead horizon for *this* solver instance\n",
    "        self.rollout_sims = rollout_sims\n",
    "        self.theta2_at_init = theta2_at_init # O2 state when this k-step solver was created\n",
    "\n",
    "        self.J_values: Dict[int, Dict[Tuple, float]] = {}\n",
    "        self.Policy: Dict[int, Dict[Tuple, int]] = {}\n",
    "\n",
    "        # Initialize a seeded random number generator for reproducible rollouts\n",
    "        self.rng = np.random.default_rng(solver_seed)\n",
    "\n",
    "        # Precompute conditionals from the initial joint belief if theta2_at_init == 0\n",
    "        # These are used if O2 is hidden initially, but discovered within the k-step lookahead\n",
    "        if self.theta2_at_init == 0 and self.initial_episode_belief.ndim == 2:\n",
    "            self._precomputed_conditionals_from_initial_joint = self._precompute_conditionals_from_joint(self.initial_episode_belief)\n",
    "        else:\n",
    "            self._precomputed_conditionals_from_initial_joint = None\n",
    "\n",
    "    def _precompute_conditionals_from_joint(self, joint_p: np.ndarray) -> List[np.ndarray]:\n",
    "        \"\"\"Pre-calculates P(O1=i | O2=k) for all k from a given joint prior.\"\"\"\n",
    "        conditionals = []\n",
    "        for k_idx in range(self.n):\n",
    "            marginal_o2 = np.sum(joint_p[:, k_idx])\n",
    "            conditionals.append(joint_p[:, k_idx] / marginal_o2 if marginal_o2 > 0 else np.zeros(self.n))\n",
    "        return conditionals\n",
    "\n",
    "    def _calculate_posterior_at_step_in_k_lookahead(self, z_relative: np.ndarray, current_theta2_in_k_step: int) -> Any:\n",
    "        \"\"\"\n",
    "        Calculates the posterior belief for a state (z_relative, current_theta2_in_k_step)\n",
    "        within this k-step lookahead segment, starting from `self.initial_episode_belief`.\n",
    "        `z_relative`: observations accumulated *within this k-step window*.\n",
    "        `current_theta2_in_k_step`: the theta2 state *within this k-step window*.\n",
    "        \"\"\"\n",
    "        if current_theta2_in_k_step == 0: # O2 is still hidden\n",
    "            # If self.theta2_at_init was already > 0, this case should not happen for a consistent path\n",
    "            if self.initial_episode_belief.ndim == 1:\n",
    "                # This implies an inconsistency, should raise error or handle carefully\n",
    "                return np.zeros((self.n, self.n)) # Return a zero joint if initial was marginal\n",
    "\n",
    "            g1_z = np.power(self.gamma1, z_relative)\n",
    "            g2_z = np.power(self.gamma2, z_relative)\n",
    "            likelihood = np.outer(g1_z, g2_z)\n",
    "            numerator = self.initial_episode_belief * likelihood\n",
    "            norm = np.sum(numerator)\n",
    "            return numerator / norm if norm > 0 else numerator\n",
    "        else: # O2 is found (either initially or within this k-step segment)\n",
    "            if self.theta2_at_init > 0:\n",
    "                # O2 was already found when KStepLookaheadSolver was instantiated\n",
    "                # initial_episode_belief is already the marginal p(O1 | O2=actual_loc)\n",
    "                base_prior_for_o1 = self.initial_episode_belief\n",
    "            else:\n",
    "                # O2 was hidden initially but found within the k-step lookahead\n",
    "                # We need to get the conditional prior for O1 from the initial joint belief\n",
    "                o2_loc = current_theta2_in_k_step - 1\n",
    "                base_prior_for_o1 = self._precomputed_conditionals_from_initial_joint[o2_loc]\n",
    "\n",
    "            g1_z = np.power(self.gamma1, z_relative)\n",
    "            numerator = g1_z * base_prior_for_o1\n",
    "            norm = np.sum(numerator)\n",
    "            return numerator / norm if norm > 0 else numerator\n",
    "\n",
    "    def _select_greedy_action(self, belief: np.ndarray, theta2_for_greedy: int) -> int:\n",
    "        \"\"\"Selects a greedy action based on the current belief.\"\"\"\n",
    "        if theta2_for_greedy == 0: # Belief is joint (n,n)\n",
    "            p_marginal_o1 = np.sum(belief, axis=1)\n",
    "            immediate_value = (1 - self.gamma1) * p_marginal_o1\n",
    "        else: # Belief is marginal for O1 (n,)\n",
    "            immediate_value = (1 - self.gamma1) * belief\n",
    "        return np.argmax(immediate_value - self.c)\n",
    "\n",
    "\n",
    "    def _run_rollout(self, rollout_z_relative: np.ndarray, rollout_theta2_in_k_step: int, remaining_global_horizon: int) -> float:\n",
    "        \"\"\"\n",
    "        Estimates the value by simulating episodes with a greedy policy.\n",
    "        `rollout_z_relative`: z_vector accumulated *within the k-step lookahead* before rollout.\n",
    "        `rollout_theta2_in_k_step`: theta2 state *within the k-step lookahead* before rollout.\n",
    "        `remaining_global_horizon`: Actual remaining steps in the *global* problem after the k-step lookahead.\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "\n",
    "        for _ in range(self.rollout_sims):\n",
    "            current_z_in_rollout = np.copy(rollout_z_relative)\n",
    "            current_theta2_in_rollout = rollout_theta2_in_k_step\n",
    "\n",
    "            # Sample true object locations for this rollout simulation based on the *current belief*\n",
    "            current_belief_for_sampling = self._calculate_posterior_at_step_in_k_lookahead(current_z_in_rollout, current_theta2_in_rollout)\n",
    "\n",
    "            if current_theta2_in_rollout == 0: # Belief is joint\n",
    "                flat_belief = current_belief_for_sampling.flatten()\n",
    "                if np.sum(flat_belief) > 0:\n",
    "                    choice_idx = self.rng.choice(self.n * self.n, p=flat_belief / np.sum(flat_belief))\n",
    "                    rollout_true_pos_o1, rollout_true_pos_o2 = np.unravel_index(choice_idx, (self.n, self.n))\n",
    "                else:\n",
    "                    rollout_true_pos_o1, rollout_true_pos_o2 = -1, -1\n",
    "            else: # Belief is marginal for O1\n",
    "                if np.sum(current_belief_for_sampling) > 0:\n",
    "                    rollout_true_pos_o1 = self.rng.choice(self.n, p=current_belief_for_sampling / np.sum(current_belief_for_sampling))\n",
    "                    rollout_true_pos_o2 = current_theta2_in_rollout - 1 # O2 location is known\n",
    "                else:\n",
    "                    rollout_true_pos_o1, rollout_true_pos_o2 = -1, -1\n",
    "\n",
    "            episode_rollout_reward = 0.0\n",
    "            for t_rollout in range(remaining_global_horizon): # Iterate for remaining time steps\n",
    "                # Get belief for greedy action decision (no z_relative update yet for this step's decision)\n",
    "                current_belief_for_greedy_action = self._calculate_posterior_at_step_in_k_lookahead(current_z_in_rollout, current_theta2_in_rollout)\n",
    "                action = self._select_greedy_action(current_belief_for_greedy_action, current_theta2_in_rollout)\n",
    "\n",
    "                episode_rollout_reward -= self.c[action]\n",
    "\n",
    "                # Simulate outcome of greedy action for rollout\n",
    "                found_o1_in_rollout = (action == rollout_true_pos_o1) and (self.rng.random() > self.gamma1[action])\n",
    "                if found_o1_in_rollout:\n",
    "                    episode_rollout_reward += 1.0\n",
    "                    break # O1 found, rollout ends\n",
    "\n",
    "                if current_theta2_in_rollout == 0 and action == rollout_true_pos_o2 and (self.rng.random() > self.gamma2[action]):\n",
    "                    current_theta2_in_rollout = action + 1 # O2 found within rollout\n",
    "\n",
    "                current_z_in_rollout[action] += 1 # Update z for next rollout step\n",
    "            total_reward += episode_rollout_reward\n",
    "        return total_reward / self.rollout_sims\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None)\n",
    "    def _generate_z_vectors(t: int, n: int) -> List[Tuple[int, ...]]:\n",
    "        if n == 1:\n",
    "            return [(t,)]\n",
    "        vectors = []\n",
    "        for i in range(t + 1):\n",
    "            for sub_vector in KStepLookaheadSolver._generate_z_vectors(t - i, n - 1):\n",
    "                vectors.append((i,) + sub_vector)\n",
    "        return vectors\n",
    "\n",
    "    def solve(self): # For theta2_at_init == 0\n",
    "        \"\"\"\n",
    "        Executes DP for k steps from self.initial_episode_belief, for theta2_at_init = 0.\n",
    "        \"\"\"\n",
    "        self.J_values[self.k] = {}\n",
    "        z_vectors_k = self._generate_z_vectors(self.k, self.n)\n",
    "        for z_k_relative_tuple in z_vectors_k:\n",
    "            for theta2_in_k_step in range(self.n + 1):\n",
    "                state = (z_k_relative_tuple, theta2_in_k_step)\n",
    "                z_k_relative = np.array(z_k_relative_tuple)\n",
    "                # Value at the leaf node of k-step DP is estimated by rollout\n",
    "                self.J_values[self.k][state] = self._run_rollout(z_k_relative, theta2_in_k_step, self.T_global - self.k)\n",
    "\n",
    "\n",
    "        for t_k_relative in range(self.k - 1, -1, -1):\n",
    "            J_t = {}\n",
    "            policy_t = {}\n",
    "            z_vectors_t = self._generate_z_vectors(t_k_relative, self.n)\n",
    "\n",
    "            for z_t_relative_tuple in z_vectors_t:\n",
    "                for theta2_in_k_step in range(self.n + 1):\n",
    "                    current_state_in_k_step = (z_t_relative_tuple, theta2_in_k_step)\n",
    "                    action_values = []\n",
    "                    z_t_relative = np.array(z_t_relative_tuple)\n",
    "\n",
    "                    current_belief_in_k_step = self._calculate_posterior_at_step_in_k_lookahead(z_t_relative, theta2_in_k_step)\n",
    "\n",
    "                    for action_k in range(self.n):\n",
    "                        next_z_relative_tuple = tuple(z_t_relative + np.eye(self.n, dtype=int)[action_k])\n",
    "\n",
    "                        if theta2_in_k_step == 0: # O2 still hidden in this k-step segment\n",
    "                            # Current belief is joint (n,n)\n",
    "                            p_marginal_o1_at_a = np.sum(current_belief_in_k_step[action_k, :])\n",
    "                            p_success = (1 - self.gamma1[action_k]) * p_marginal_o1_at_a\n",
    "\n",
    "                            # Prob of finding O2 only\n",
    "                            p_find_o2_only = (1 - self.gamma2[action_k]) * (self.gamma1[action_k] * current_belief_in_k_step[action_k, action_k] + np.sum(current_belief_in_k_step[np.arange(self.n) != action_k, action_k]))\n",
    "\n",
    "                            p_nothing = 1 - p_success - p_find_o2_only\n",
    "\n",
    "                            val_if_nothing = self.J_values[t_k_relative + 1][(next_z_relative_tuple, 0)]\n",
    "                            val_if_found_o2 = self.J_values[t_k_relative + 1][(next_z_relative_tuple, action_k + 1)]\n",
    "\n",
    "                            expected_value = (p_success * 1.0) - self.c[action_k] + \\\n",
    "                                             (p_nothing * val_if_nothing) + \\\n",
    "                                             (p_find_o2_only * val_if_found_o2)\n",
    "                            action_values.append(expected_value)\n",
    "\n",
    "                        else: # O2 already found (theta2_in_k_step > 0)\n",
    "                            # Current belief is marginal for O1 (n,)\n",
    "                            p_o1_at_a = current_belief_in_k_step[action_k]\n",
    "                            p_success = (1 - self.gamma1[action_k]) * p_o1_at_a\n",
    "                            p_fail = 1 - p_success\n",
    "\n",
    "                            val_if_fail = self.J_values[t_k_relative + 1][(next_z_relative_tuple, theta2_in_k_step)]\n",
    "\n",
    "                            expected_value = (p_success * 1.0) - self.c[action_k] + (p_fail * val_if_fail)\n",
    "                            action_values.append(expected_value)\n",
    "\n",
    "                    best_value = np.max(action_values)\n",
    "                    best_action = np.argmax(action_values)\n",
    "                    J_t[current_state_in_k_step] = best_value\n",
    "                    policy_t[current_state_in_k_step] = best_action\n",
    "\n",
    "                self.J_values[t_k_relative] = J_t\n",
    "                self.Policy[t_k_relative] = policy_t\n",
    "\n",
    "    def solve_with_theta(self): # For theta2_at_init > 0\n",
    "        \"\"\"\n",
    "        Executes DP for k steps from self.initial_episode_belief, for theta2_at_init > 0.\n",
    "        In this case, O2 is already found, so theta2 is fixed throughout the k steps.\n",
    "        \"\"\"\n",
    "        fixed_theta2_in_k_step = self.theta2_at_init\n",
    "\n",
    "        self.J_values[self.k] = {}\n",
    "        z_vectors_k = self._generate_z_vectors(self.k, self.n)\n",
    "        for z_k_relative_tuple in z_vectors_k:\n",
    "            z_k_relative = np.array(z_k_relative_tuple)\n",
    "            state = (z_k_relative_tuple, fixed_theta2_in_k_step)\n",
    "            # Value at the leaf node of k-step DP is estimated by rollout\n",
    "            self.J_values[self.k][state] = self._run_rollout(z_k_relative, fixed_theta2_in_k_step, self.T_global - self.k)\n",
    "\n",
    "        for t_k_relative in range(self.k - 1, -1, -1):\n",
    "            J_t = {}\n",
    "            policy_t = {}\n",
    "            z_vectors_t = self._generate_z_vectors(t_k_relative, self.n)\n",
    "\n",
    "            for z_t_relative_tuple in z_vectors_t:\n",
    "                z_t_relative = np.array(z_t_relative_tuple)\n",
    "                current_state_in_k_step = (z_t_relative_tuple, fixed_theta2_in_k_step)\n",
    "                action_values = []\n",
    "\n",
    "                current_belief_in_k_step = self._calculate_posterior_at_step_in_k_lookahead(z_t_relative, fixed_theta2_in_k_step)\n",
    "\n",
    "                for action_k in range(self.n):\n",
    "                    next_z_relative_tuple = tuple(z_t_relative + np.eye(self.n, dtype=int)[action_k])\n",
    "\n",
    "                    p_o1_at_a = current_belief_in_k_step[action_k]\n",
    "                    p_success = (1 - self.gamma1[action_k]) * p_o1_at_a\n",
    "                    p_fail = 1 - p_success\n",
    "\n",
    "                    val_if_fail = self.J_values[t_k_relative + 1][(next_z_relative_tuple, fixed_theta2_in_k_step)]\n",
    "\n",
    "                    expected_value = (p_success * 1.0) - self.c[action_k] + (p_fail * val_if_fail)\n",
    "                    action_values.append(expected_value)\n",
    "\n",
    "                best_value = np.max(action_values)\n",
    "                best_action = np.argmax(action_values)\n",
    "                J_t[current_state_in_k_step] = best_value\n",
    "                policy_t[current_state_in_k_step] = best_action\n",
    "\n",
    "            self.J_values[t_k_relative] = J_t\n",
    "            self.Policy[t_k_relative] = policy_t\n",
    "\n",
    "\n",
    "# Helper function definitions (assuming they are available or defined above this class)\n",
    "# Need to ensure calculate_joint_posterior and calculate_conditional_posterior are accessible.\n",
    "# Let's define them here for clarity, or assume they are in a common helpers file.\n",
    "\n",
    "# Re-including helper functions for self-containment\n",
    "def calculate_joint_posterior(z_vector: np.ndarray, initial_prior_joint: np.ndarray,\n",
    "                              gamma1: np.ndarray, gamma2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculates the joint posterior belief p(O1=i, O2=j | z) based on initial joint prior.\"\"\"\n",
    "    g1_z = np.power(gamma1, z_vector)\n",
    "    g2_z = np.power(gamma2, z_vector)\n",
    "    likelihood = np.outer(g1_z, g2_z)\n",
    "    numerator = initial_prior_joint * likelihood\n",
    "    norm = np.sum(numerator)\n",
    "    return numerator / norm if norm > 0 else numerator\n",
    "\n",
    "def calculate_conditional_posterior(z_vector: np.ndarray, o2_loc: int,\n",
    "                                    precomputed_conditionals: List[np.ndarray], gamma1: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculates the conditional posterior belief p(O1=i | z, O2=k) based on precomputed conditionals and current z_vector.\"\"\"\n",
    "    p0_conditional = precomputed_conditionals[o2_loc]\n",
    "    g1_z = np.power(gamma1, z_vector)\n",
    "    numerator = g1_z * p0_conditional\n",
    "    norm = np.sum(numerator)\n",
    "    return numerator / norm if norm > 0 else numerator\n",
    "\n",
    "def precompute_p0_conditionals(n: int, p0_joint: np.ndarray) -> list:\n",
    "    \"\"\"Pre-calculates P(O1=i | O2=k) for all k from a joint prior matrix.\"\"\"\n",
    "    if p0_joint.ndim != 2: # Should be (n,n) for joint prior\n",
    "        raise ValueError(\"p0 must be a 2D joint probability matrix for precomputing conditionals.\")\n",
    "    conditionals = []\n",
    "    for k in range(n):\n",
    "        marginal_o2 = np.sum(p0_joint[:, k])\n",
    "        conditionals.append(p0_joint[:, k] / marginal_o2 if marginal_o2 > 0 else np.zeros(n))\n",
    "    return conditionals\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Adaptive Re-planning Episode Simulation\n",
    "# =============================================================================\n",
    "\n",
    "def run_k_step_episode(n: int, T: int, p0_initial_joint: np.ndarray,\n",
    "                                    gamma1: np.ndarray, gamma2: np.ndarray, c: np.ndarray, k: int, ROLLOUT_SIMULATIONS: int, episode_seed: int) -> Tuple[bool, int, float, int]:\n",
    "    \"\"\"\n",
    "    Simulates a single episode using k-step lookahead with greedy rollout and then full DP.\n",
    "    Returns success status, time of detection (or -1 if failed), accumulated reward, and episode length.\n",
    "    \"\"\"\n",
    "    # 1. Secretly determine the true locations for the entire episode\n",
    "    flat_p0 = p0_initial_joint.flatten()\n",
    "    if np.sum(flat_p0) == 0: # Handle case where prior is all zeros\n",
    "         true_pos_o1, true_pos_o2 = -1, -1 # Indicate no target\n",
    "    else:\n",
    "        # Use np.random directly as its state is set by np.random.seed(episode_seed)\n",
    "        choice_index = np.random.choice(n * n, p=flat_p0 / np.sum(flat_p0)) # Normalize\n",
    "        true_pos_o1, true_pos_o2 = np.unravel_index(choice_index, (n, n))\n",
    "\n",
    "    # 2. Initialize global state and reward variables\n",
    "    z_vector_global = np.zeros(n, dtype=int) # Accumulated search history for the entire episode\n",
    "    theta2_global = 0 # O2 state: 0=hidden, >0=found at cell (theta2_global-1)\n",
    "    current_belief_global = np.copy(p0_initial_joint) # Evolving belief state (joint or marginal)\n",
    "\n",
    "    # Precompute global conditionals from the initial joint prior once\n",
    "    global_p0_conditionals = precompute_p0_conditionals(n, p0_initial_joint)\n",
    "\n",
    "    accumulated_reward = 0.0\n",
    "    detection_time = -1\n",
    "    episode_length = 0\n",
    "\n",
    "    # Phase 1: K-step lookahead for (T-k) time steps\n",
    "    for t_global in range(T - k): # t_global is the current time step in the entire episode\n",
    "        episode_length += 1\n",
    "\n",
    "        # Create a unique, reproducible seed for this KStepLookaheadSolver instance\n",
    "        solver_instance_seed = episode_seed * T + t_global\n",
    "\n",
    "        # Instantiate K-step lookahead solver with the current global state (belief, z_vector, theta2)\n",
    "        # The KStepLookaheadSolver will calculate policies for `k` steps starting from a relative z=0.\n",
    "        solver_instance = KStepLookaheadSolver(n=n, T=T-t_global,\n",
    "                                                initial_episode_belief=current_belief_global,\n",
    "                                                gamma1=gamma1, gamma2=gamma2,\n",
    "                                                theta2_at_init=theta2_global,\n",
    "                                                k=k, c=c, rollout_sims=ROLLOUT_SIMULATIONS,\n",
    "                                                solver_seed=solver_instance_seed)\n",
    "\n",
    "        if theta2_global == 0: # If O2 is currently hidden, use the general solve method\n",
    "            solver_instance.solve()\n",
    "        else: # If O2 is currently found, use the solve method specialized for a fixed theta2\n",
    "            solver_instance.solve_with_theta()\n",
    "\n",
    "        # Get the optimal action for the current global step (t_global) from the k-step solver's policy\n",
    "        # The k-step solver's policy is for its *relative* time t=0 and *relative* z=0.\n",
    "        action = solver_instance.Policy[0][(tuple([0]*n), theta2_global)]\n",
    "\n",
    "        # Deduct cost for the action\n",
    "        accumulated_reward -= c[action]\n",
    "\n",
    "        # Simulate outcome of the action\n",
    "        found_o1 = (action == true_pos_o1) and (np.random.random() > gamma1[action])\n",
    "        if found_o1:\n",
    "            accumulated_reward += 1.0 # Reward for finding O1\n",
    "            detection_time = t_global + 1 # Absolute time of detection\n",
    "            return True, detection_time, accumulated_reward, episode_length # Mission Success!\n",
    "\n",
    "        # Check if O2 is found *at this current global step* (only if it was previously hidden)\n",
    "        if theta2_global == 0 and action == true_pos_o2 and (np.random.random() > gamma2[action]):\n",
    "            theta2_global = action + 1 # O2 found at cell `action`, update state\n",
    "\n",
    "        # Update global z_vector based on the action taken\n",
    "        z_vector_global[action] += 1\n",
    "\n",
    "        # Update global belief state based on the action and observed outcome\n",
    "        if theta2_global == 0: # If O2 is still hidden globally\n",
    "            current_belief_global = calculate_joint_posterior(z_vector_global, p0_initial_joint, gamma1, gamma2)\n",
    "        else: # If O2 has been found globally\n",
    "            current_belief_global = calculate_conditional_posterior(z_vector_global, theta2_global - 1, global_p0_conditionals, gamma1)\n",
    "\n",
    "    # Phase 2: After T-k steps, switch to full DP for the remaining k steps\n",
    "    # The remaining time horizon for this DP problem is `k`\n",
    "    # The DP solvers operate on their own relative z_vector, taking `current_belief_global` as their starting prior.\n",
    "    final_z_for_dp = np.zeros(n, dtype=int) # z_vector for the DP solver starts from 0 relative to its own horizon\n",
    "    final_theta2_for_dp = theta2_global # O2 state is carried over to the DP phase\n",
    "    final_policy = None\n",
    "\n",
    "    if final_theta2_for_dp == 0: # O2 is still hidden, use TwoObjectDPSolver\n",
    "        dp_solver = TwoObjectDPSolver(n=n, T=k, p0=current_belief_global, gamma1=gamma1, gamma2=gamma2, c=c)\n",
    "        dp_solver.solve()\n",
    "        final_policy = dp_solver.Policy\n",
    "    else: # O2 has been found, use SingleObjectDPSolver\n",
    "        dp_solver = SingleObjectDPSolver(n=n, T=k, p0_marginal=current_belief_global, gamma1=gamma1, c=c)\n",
    "        dp_solver.solve()\n",
    "        final_policy = dp_solver.Policy\n",
    "\n",
    "    for t_dp in range(k): # Iterate for the remaining `k` time steps using the final DP policy\n",
    "        episode_length += 1\n",
    "\n",
    "        # Determine the state key for the DP policy based on theta2\n",
    "        state_for_dp = (tuple(final_z_for_dp), final_theta2_for_dp) if final_theta2_for_dp == 0 else tuple(final_z_for_dp)\n",
    "\n",
    "        if t_dp not in final_policy or state_for_dp not in final_policy[t_dp]:\n",
    "            # This case should ideally not happen if the DP policy is complete for the horizon `k`\n",
    "            # print(f\"Warning: DP Policy not found for state {state_for_dp} at time {t_dp}. Ending episode.\")\n",
    "            return False, detection_time, accumulated_reward, episode_length\n",
    "\n",
    "        action = final_policy[t_dp][state_for_dp]\n",
    "        accumulated_reward -= c[action]\n",
    "\n",
    "        # Simulate outcome of the action in the DP phase\n",
    "        if action == true_pos_o1 and (np.random.random() > gamma1[action]):\n",
    "            accumulated_reward += 1.0 # Reward for finding O1\n",
    "            detection_time = (T - k) + (t_dp + 1) # Absolute time of detection\n",
    "            return True, detection_time, accumulated_reward, episode_length # Mission Success!\n",
    "\n",
    "        # Check for O2 discovery during DP phase, if O2 was still hidden\n",
    "        if final_theta2_for_dp == 0 and action == true_pos_o2 and (np.random.random() > gamma2[action]):\n",
    "            # If O2 is found here, the state `final_theta2_for_dp` is updated, but the DP policy\n",
    "            # was computed assuming `final_theta2_for_dp` was `0` for the entire `k` steps.\n",
    "            # A more advanced adaptive approach would re-plan here with a new SingleObjectDPSolver.\n",
    "            # For this simplified model, we just update the state and continue with the same policy.\n",
    "            final_theta2_for_dp = action + 1\n",
    "\n",
    "        # Update z_vector for the DP solver's internal state\n",
    "        final_z_for_dp[action] += 1\n",
    "\n",
    "    return False, detection_time, accumulated_reward, episode_length # Mission Failed\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. MAIN EXPERIMENT SCRIPT (Modified for multiple seeds)\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Problem Definition ---\n",
    "    # Example usage for testing the greedy policy\n",
    "    NUM_CELLS = 25\n",
    "    TIME_HORIZON = 20\n",
    "    NUM_EPISODES_PER_SEED = 10 # Number of episodes to run for this benchmark\n",
    "    LOOKAHEAD_K = 1\n",
    "    ROLLOUT_SIMULATIONS = 10\n",
    "\n",
    "\n",
    "    print(\"--- Running Empirical Experiment for Greedy Policy ---\")\n",
    "    print(f\"Number of episodes: {NUM_EPISODES}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "\n",
    "    SEED = 42\n",
    "\n",
    "    prior, gammas1, gammas2, c = generate_problem_parameters(NUM_CELLS, SEED)\n",
    "\n",
    "    ## Ensure reproducibility \n",
    "    assert (prior[0][0], prior[24][24]) == (0.020334334191265974, 0.020140323245187997)\n",
    "    assert (gammas1[0], gammas1[1]) == (0.6936350297118405, 0.837678576602479)\n",
    "    assert (gammas2[0], gammas2[1]) == (0.22851759613930137, 0.16996737821583596)\n",
    "    assert (c[0], c[1]) ==( 0.2939169255529117, 0.2550265646722229)\n",
    "\n",
    "    all_success_rates = []\n",
    "    all_detection_times = []\n",
    "    all_rewards = []\n",
    "    all_episode_lengths = []\n",
    "    total_experiment_start_time = time.time()\n",
    "\n",
    "    print(\"--- Running Empirical Experiment for k-step lookahead Policy over Multiple Seeds ---\")\n",
    "    print(f\"Number of seeds: {NUM_SEEDS}\")\n",
    "    print(f\"Number of evaluation episodes per seed: {NUM_EPISODES_PER_SEED}\")\n",
    "    print(f\"Lookahead depth (k): {LOOKAHEAD_K}\")\n",
    "    print(f\"Rollout simulations: {ROLLOUT_SIMULATIONS}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "\n",
    "    for seed in range(NUM_SEEDS):\n",
    "        print(f\"\\n--- Running with Seed: {seed} ---\")\n",
    "        np.random.seed(seed) # Set seed for main episode simulation\n",
    "\n",
    "        num_successes = 0\n",
    "        detection_times_this_seed = []\n",
    "        rewards_this_seed = []\n",
    "        episode_lengths_this_seed = []\n",
    "        start_time_this_seed = time.time()\n",
    "\n",
    "        for i in range(NUM_EPISODES_PER_SEED):\n",
    "            # print(f\"Episode : {i}\") # Uncomment for detailed per-episode output\n",
    "            success, detection_time, reward, episode_length = run_k_step_episode(NUM_CELLS, TIME_HORIZON, prior, gammas1, gammas2, c, LOOKAHEAD_K, ROLLOUT_SIMULATIONS, seed)\n",
    "            rewards_this_seed.append(reward)\n",
    "            episode_lengths_this_seed.append(episode_length)\n",
    "\n",
    "            if success:\n",
    "                num_successes += 1\n",
    "                detection_times_this_seed.append(detection_time)\n",
    "                # print(f\"    (Episode {i+1} Result) SUCCESS at time {detection_time} with reward {reward:.4f}\")\n",
    "            # else:\n",
    "                # print(f\"    (Episode {i+1} Result) FAILURE with reward {reward:.4f}\")\n",
    "\n",
    "\n",
    "        end_time_this_seed = time.time()\n",
    "        success_rate_this_seed = num_successes / NUM_EPISODES_PER_SEED\n",
    "        all_success_rates.append(success_rate_this_seed)\n",
    "        all_detection_times.extend(detection_times_this_seed)\n",
    "        all_rewards.extend(rewards_this_seed)\n",
    "        all_episode_lengths.extend(episode_lengths_this_seed)\n",
    "\n",
    "\n",
    "        print(f\"--- Seed {seed} Results ---\")\n",
    "        print(f\"Success Rate: {success_rate_this_seed:.4f} ({success_rate_this_seed*100:.2f}%)\")\n",
    "        if detection_times_this_seed:\n",
    "            print(f\"Average Detection Time (Successful Episodes): {np.mean(detection_times_this_seed):.2f}\")\n",
    "        else:\n",
    "            print(\"Average Detection Time (Successful Episodes): N/A (no successes)\")\n",
    "        print(f\"Average Episode Reward: {np.mean(rewards_this_seed):.4f}\")\n",
    "        print(f\"Average Episode Length: {np.mean(episode_lengths_this_seed):.2f}\")\n",
    "        print(f\"Time taken for this seed: {end_time_this_seed - start_time_this_seed:.2f} seconds\")\n",
    "\n",
    "\n",
    "    total_experiment_end_time = time.time()\n",
    "\n",
    "    # --- 3.4. Display Final Results ---\n",
    "    mean_success_rate = np.mean(all_success_rates)\n",
    "    std_success_rate = np.std(all_success_rates)\n",
    "    mean_detection_time = np.mean(all_detection_times) if all_detection_times else -1\n",
    "    std_detection_time = np.std(all_detection_times) if all_detection_times else -1\n",
    "    mean_reward = np.mean(all_rewards)\n",
    "    std_reward = np.std(all_rewards)\n",
    "    mean_episode_length = np.mean(all_episode_lengths)\n",
    "    std_episode_length = np.std(all_episode_lengths)\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Overall {LOOKAHEAD_K}-step Lookahead Policy Results ---\")\n",
    "    print(f\"Total experiment time across {NUM_SEEDS} seeds: {total_experiment_end_time - total_experiment_start_time:.2f} seconds\")\n",
    "    print(f\"Average Success Rate over {NUM_SEEDS} seeds and {NUM_EPISODES_PER_SEED} episodes each: {mean_success_rate:.4f} ({mean_success_rate*100:.2f}%)\")\n",
    "    print(f\"Standard Deviation of Success Rate: {std_success_rate:.4f}\")\n",
    "    if all_detection_times:\n",
    "        print(f\"Average Detection Time (Successful Episodes) across all seeds: {mean_detection_time:.2f}\")\n",
    "        print(f\"Standard Deviation of Detection Time: {std_detection_time:.2f}\")\n",
    "    else:\n",
    "        print(\"Average Detection Time (Successful Episodes) across all seeds: N/A (no successes)\")\n",
    "\n",
    "    print(f\"Average Episode Reward across all seeds: {mean_reward:.4f}\")\n",
    "    print(f\"Standard Deviation of Reward: {std_reward:.4f}\")\n",
    "    print(f\"Average Episode Length across all seeds: {mean_episode_length:.2f}\")\n",
    "    print(f\"Standard Deviation of Episode Length: {std_episode_length:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
